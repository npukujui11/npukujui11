\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}文献阅读}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Pre-knowledge}{2}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}(综述2021.04)Lighting the darkness in the deep learning era}{3}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}背景}{3}{subsubsection.1.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2}介绍}{3}{subsubsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.3}细节}{3}{subsubsection.1.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Network Structure}{3}{figure.caption.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces  Summary of essential characteristics of representative deep learning-based methods, including learning strategies, network structures, loss functions, training datasets, testing datasets, evaluation metrics, data formats of input, and whether the models are Retinex-based or not. "simulated" means the testing data are simulated by the same approach as the synthetic training data. "self-selected" stands for the real-world images selected by the authors. "\#P" represents the number of trainable parameters. "-" means this item is not available or not indicated in the paper.\relax }}{4}{table.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab: Summary}{{1}{4}{Summary of essential characteristics of representative deep learning-based methods, including learning strategies, network structures, loss functions, training datasets, testing datasets, evaluation metrics, data formats of input, and whether the models are Retinex-based or not. "simulated" means the testing data are simulated by the same approach as the synthetic training data. "self-selected" stands for the real-world images selected by the authors. "\#P" represents the number of trainable parameters. "-" means this item is not available or not indicated in the paper.\relax }{table.caption.1}{}}
\newlabel{fig:subfig_a}{{1a}{5}{learning strategy\relax }{figure.caption.2}{}}
\newlabel{sub@fig:subfig_a}{{a}{5}{learning strategy\relax }{figure.caption.2}{}}
\newlabel{fig:subfig_b}{{1b}{5}{network structure\relax }{figure.caption.2}{}}
\newlabel{sub@fig:subfig_b}{{b}{5}{network structure\relax }{figure.caption.2}{}}
\newlabel{fig:subfig_c}{{1c}{5}{Retinex model\relax }{figure.caption.2}{}}
\newlabel{sub@fig:subfig_c}{{c}{5}{Retinex model\relax }{figure.caption.2}{}}
\newlabel{fig:subfig_d}{{1d}{5}{data format\relax }{figure.caption.2}{}}
\newlabel{sub@fig:subfig_d}{{d}{5}{data format\relax }{figure.caption.2}{}}
\newlabel{fig:subfig_e}{{1e}{5}{loss function\relax }{figure.caption.2}{}}
\newlabel{sub@fig:subfig_e}{{e}{5}{loss function\relax }{figure.caption.2}{}}
\newlabel{fig:subfig_f}{{1f}{5}{training dataset\relax }{figure.caption.2}{}}
\newlabel{sub@fig:subfig_f}{{f}{5}{training dataset\relax }{figure.caption.2}{}}
\newlabel{fig:subfig_g}{{1g}{5}{testing dataset\relax }{figure.caption.2}{}}
\newlabel{sub@fig:subfig_g}{{g}{5}{testing dataset\relax }{figure.caption.2}{}}
\newlabel{fig:subfig_h}{{1h}{5}{evaluation metric\relax }{figure.caption.2}{}}
\newlabel{sub@fig:subfig_h}{{h}{5}{evaluation metric\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces   A statictic analysis of deep learning-based LLIE methods, including learning strategy, network characteristic, Retinex model, data format, loss function, training dataset, testing dataset, and evaluation metric. Better to see with zoom. \relax }}{5}{figure.caption.2}\protected@file@percent }
\newlabel{fig: Statictic Analysis}{{1}{5}{A statictic analysis of deep learning-based LLIE methods, including learning strategy, network characteristic, Retinex model, data format, loss function, training dataset, testing dataset, and evaluation metric. Better to see with zoom. \relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Combination of Deep Model and Retinex Theory}{5}{figure.caption.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Data Format}{5}{figure.caption.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Loss Function}{5}{figure.caption.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Training Datasets}{5}{figure.caption.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Testing Dataset}{6}{figure.caption.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces  Summary of paired training datasets. 'Syn' represents Synthetic.\relax }}{6}{table.caption.3}\protected@file@percent }
\newlabel{tab: Paired_training_datases}{{2}{6}{Summary of paired training datasets. 'Syn' represents Synthetic.\relax }{table.caption.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces  Summary of testing datasets.\relax }}{6}{table.caption.4}\protected@file@percent }
\newlabel{tab: Testing datasets}{{3}{6}{Summary of testing datasets.\relax }{table.caption.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Benchmarking and Empirical Analysis}{6}{table.caption.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{A New Low-light Image and Video Dataset}{6}{table.caption.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces  Summary of LoLi-Phone dataset. LoLi-Phone dataset contains 120 videos (55,148 images) taken by 18 different mobile phones' cameras. "\#Video" and "\#Image" represent the number of videos and images, respectively.\relax }}{7}{table.caption.5}\protected@file@percent }
\newlabel{tab: LoLi-Phone dataset}{{4}{7}{Summary of LoLi-Phone dataset. LoLi-Phone dataset contains 120 videos (55,148 images) taken by 18 different mobile phones' cameras. "\#Video" and "\#Image" represent the number of videos and images, respectively.\relax }{table.caption.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Online Evaluation Platform}{7}{figure.caption.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Benchmark Results}{7}{figure.caption.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces   Several images sampled from the proposed LoLiPhone dataset. The images and videos are taken by different devices under diverse lighting conditions and scenes. \relax }}{8}{figure.caption.6}\protected@file@percent }
\newlabel{fig:Sample_Loli}{{2}{8}{Several images sampled from the proposed LoLiPhone dataset. The images and videos are taken by different devices under diverse lighting conditions and scenes. \relax }{figure.caption.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces  Quantitative comparisons on LOL-test and MIT-Adobe FiveK-test testing datasets in terms of MSE (×103), PSNR (in dB), SSIM, and LPIPS. The best result is in red whereas the second and third best results are in blue and purple under each case, respectively.\relax }}{8}{table.caption.11}\protected@file@percent }
\newlabel{tab: Quantitative Comparisons on LOL-test and MIT-Adobe FiveK-test testing datasets}{{5}{8}{Quantitative comparisons on LOL-test and MIT-Adobe FiveK-test testing datasets in terms of MSE (×103), PSNR (in dB), SSIM, and LPIPS. The best result is in red whereas the second and third best results are in blue and purple under each case, respectively.\relax }{table.caption.11}{}}
\newlabel{fig: LOL-test_a}{{3a}{9}{input\relax }{figure.caption.7}{}}
\newlabel{sub@fig: LOL-test_a}{{a}{9}{input\relax }{figure.caption.7}{}}
\newlabel{fig: LOL-test_b}{{3b}{9}{LLNet\relax }{figure.caption.7}{}}
\newlabel{sub@fig: LOL-test_b}{{b}{9}{LLNet\relax }{figure.caption.7}{}}
\newlabel{fig: LOL-test_c}{{3c}{9}{LightenNet\relax }{figure.caption.7}{}}
\newlabel{sub@fig: LOL-test_c}{{c}{9}{LightenNet\relax }{figure.caption.7}{}}
\newlabel{fig: LOL-test_d}{{3d}{9}{Retinex-Net\relax }{figure.caption.7}{}}
\newlabel{sub@fig: LOL-test_d}{{d}{9}{Retinex-Net\relax }{figure.caption.7}{}}
\newlabel{fig: LOL-test_e}{{3e}{9}{MBLLEN\relax }{figure.caption.7}{}}
\newlabel{sub@fig: LOL-test_e}{{e}{9}{MBLLEN\relax }{figure.caption.7}{}}
\newlabel{fig: LOL-test_f}{{3f}{9}{KinD\relax }{figure.caption.7}{}}
\newlabel{sub@fig: LOL-test_f}{{f}{9}{KinD\relax }{figure.caption.7}{}}
\newlabel{fig: LOL-test_g}{{3g}{9}{KinD++\relax }{figure.caption.7}{}}
\newlabel{sub@fig: LOL-test_g}{{g}{9}{KinD++\relax }{figure.caption.7}{}}
\newlabel{fig: LOL-test_h}{{3h}{9}{TBEFN\relax }{figure.caption.7}{}}
\newlabel{sub@fig: LOL-test_h}{{h}{9}{TBEFN\relax }{figure.caption.7}{}}
\newlabel{fig: LOL-test_i}{{3i}{9}{DSLR\relax }{figure.caption.7}{}}
\newlabel{sub@fig: LOL-test_i}{{i}{9}{DSLR\relax }{figure.caption.7}{}}
\newlabel{fig: LOL-test_j}{{3j}{9}{EnlightenGAN\relax }{figure.caption.7}{}}
\newlabel{sub@fig: LOL-test_j}{{j}{9}{EnlightenGAN\relax }{figure.caption.7}{}}
\newlabel{fig: LOL-test_k}{{3k}{9}{DRBN\relax }{figure.caption.7}{}}
\newlabel{sub@fig: LOL-test_k}{{k}{9}{DRBN\relax }{figure.caption.7}{}}
\newlabel{fig: LOL-test_l}{{3l}{9}{ExCNet\relax }{figure.caption.7}{}}
\newlabel{sub@fig: LOL-test_l}{{l}{9}{ExCNet\relax }{figure.caption.7}{}}
\newlabel{fig: LOL-test_m}{{3m}{9}{Zero-DCE\relax }{figure.caption.7}{}}
\newlabel{sub@fig: LOL-test_m}{{m}{9}{Zero-DCE\relax }{figure.caption.7}{}}
\newlabel{fig: LOL-test_n}{{3n}{9}{RRDNet\relax }{figure.caption.7}{}}
\newlabel{sub@fig: LOL-test_n}{{n}{9}{RRDNet\relax }{figure.caption.7}{}}
\newlabel{fig: LOL-test_o}{{3o}{9}{GT\relax }{figure.caption.7}{}}
\newlabel{sub@fig: LOL-test_o}{{o}{9}{GT\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces   Visual results of different methods on a low-light image sampled from LOL-test dataset. \relax }}{9}{figure.caption.7}\protected@file@percent }
\newlabel{fig: Visual Result from LOL-test dataset}{{3}{9}{Visual results of different methods on a low-light image sampled from LOL-test dataset. \relax }{figure.caption.7}{}}
\newlabel{fig: MIT-Adobe_FiveK_a}{{4a}{9}{input\relax }{figure.caption.8}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_a}{{a}{9}{input\relax }{figure.caption.8}{}}
\newlabel{fig: MIT-Adobe_FiveK_b}{{4b}{9}{LLNet\relax }{figure.caption.8}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_b}{{b}{9}{LLNet\relax }{figure.caption.8}{}}
\newlabel{fig: MIT-Adobe_FiveK_c}{{4c}{9}{LightenNet\relax }{figure.caption.8}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_c}{{c}{9}{LightenNet\relax }{figure.caption.8}{}}
\newlabel{fig: MIT-Adobe_FiveK_d}{{4d}{9}{Retinex-Net\relax }{figure.caption.8}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_d}{{d}{9}{Retinex-Net\relax }{figure.caption.8}{}}
\newlabel{fig: MIT-Adobe_FiveK_e}{{4e}{9}{MBLLEN\relax }{figure.caption.8}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_e}{{e}{9}{MBLLEN\relax }{figure.caption.8}{}}
\newlabel{fig: MIT-Adobe_FiveK_f}{{4f}{9}{KinD\relax }{figure.caption.8}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_f}{{f}{9}{KinD\relax }{figure.caption.8}{}}
\newlabel{fig: MIT-Adobe_FiveK_g}{{4g}{9}{KinD++\relax }{figure.caption.8}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_g}{{g}{9}{KinD++\relax }{figure.caption.8}{}}
\newlabel{fig: MIT-Adobe_FiveK_h}{{4h}{9}{TBEFN\relax }{figure.caption.8}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_h}{{h}{9}{TBEFN\relax }{figure.caption.8}{}}
\newlabel{fig: MIT-Adobe_FiveK_i}{{4i}{9}{DSLR\relax }{figure.caption.8}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_i}{{i}{9}{DSLR\relax }{figure.caption.8}{}}
\newlabel{fig: MIT-Adobe_FiveK_j}{{4j}{9}{EnlightenGAN\relax }{figure.caption.8}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_j}{{j}{9}{EnlightenGAN\relax }{figure.caption.8}{}}
\newlabel{fig: MIT-Adobe_FiveK_k}{{4k}{9}{DRBN\relax }{figure.caption.8}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_k}{{k}{9}{DRBN\relax }{figure.caption.8}{}}
\newlabel{fig: MIT-Adobe_FiveK_l}{{4l}{9}{ExCNet\relax }{figure.caption.8}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_l}{{l}{9}{ExCNet\relax }{figure.caption.8}{}}
\newlabel{fig: MIT-Adobe_FiveK_m}{{4m}{9}{Zero-DCE\relax }{figure.caption.8}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_m}{{m}{9}{Zero-DCE\relax }{figure.caption.8}{}}
\newlabel{fig: MIT-Adobe_FiveK_n}{{4n}{9}{RRDNet\relax }{figure.caption.8}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_n}{{n}{9}{RRDNet\relax }{figure.caption.8}{}}
\newlabel{fig: MIT-Adobe_FiveK_o}{{4o}{9}{GT\relax }{figure.caption.8}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_o}{{o}{9}{GT\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces   Visual results of different methods on a low-light image sampled from MIT-Adobe FiveK-test dataset. \relax }}{9}{figure.caption.8}\protected@file@percent }
\newlabel{fig: Visual Result from MIT-Adobe FiveK dataset}{{4}{9}{Visual results of different methods on a low-light image sampled from MIT-Adobe FiveK-test dataset. \relax }{figure.caption.8}{}}
\newlabel{fig: LoLi-Phone-imgT_a}{{5a}{10}{\relax }{figure.caption.9}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_a}{{a}{10}{\relax }{figure.caption.9}{}}
\newlabel{fig: LoLi-Phone-imgT_b}{{5b}{10}{\relax }{figure.caption.9}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_b}{{b}{10}{\relax }{figure.caption.9}{}}
\newlabel{fig: LoLi-Phone-imgT_c}{{5c}{10}{\relax }{figure.caption.9}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_c}{{c}{10}{\relax }{figure.caption.9}{}}
\newlabel{fig: LoLi-Phone-imgT_d}{{5d}{10}{\relax }{figure.caption.9}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_d}{{d}{10}{\relax }{figure.caption.9}{}}
\newlabel{fig: LoLi-Phone-imgT_e}{{5e}{10}{\relax }{figure.caption.9}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_e}{{e}{10}{\relax }{figure.caption.9}{}}
\newlabel{fig: LoLi-Phone-imgT_f}{{5f}{10}{\relax }{figure.caption.9}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_f}{{f}{10}{\relax }{figure.caption.9}{}}
\newlabel{fig: LoLi-Phone-imgT_g}{{5g}{10}{\relax }{figure.caption.9}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_g}{{g}{10}{\relax }{figure.caption.9}{}}
\newlabel{fig: LoLi-Phone-imgT_h}{{5h}{10}{\relax }{figure.caption.9}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_h}{{h}{10}{\relax }{figure.caption.9}{}}
\newlabel{fig: LoLi-Phone-imgT_i}{{5i}{10}{\relax }{figure.caption.9}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_i}{{i}{10}{\relax }{figure.caption.9}{}}
\newlabel{fig: LoLi-Phone-imgT_j}{{5j}{10}{\relax }{figure.caption.9}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_j}{{j}{10}{\relax }{figure.caption.9}{}}
\newlabel{fig: LoLi-Phone-imgT_k}{{5k}{10}{\relax }{figure.caption.9}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_k}{{k}{10}{\relax }{figure.caption.9}{}}
\newlabel{fig: LoLi-Phone-imgT_l}{{5l}{10}{\relax }{figure.caption.9}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_l}{{l}{10}{\relax }{figure.caption.9}{}}
\newlabel{fig: LoLi-Phone-imgT_m}{{5m}{10}{\relax }{figure.caption.9}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_m}{{m}{10}{\relax }{figure.caption.9}{}}
\newlabel{fig: LoLi-Phone-imgT_n}{{5n}{10}{\relax }{figure.caption.9}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_n}{{n}{10}{\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces   Visual results of different methods on a low-light image sampled from LoLi-Phone-imgT dataset. (a) input (b) LLNet (c) LightenNet (d) Retinex-Net (e) MBLLEN (f) KinD (g) KinD++ (h) TBEFN (i) DSLR (j) EnlightenGAN (k) DRBN (l) ExCNet (m) Zero-DCE (n) RRDNet \relax }}{10}{figure.caption.9}\protected@file@percent }
\newlabel{fig: Visual Result from LoLi-Phone-imgT dataset}{{5}{10}{Visual results of different methods on a low-light image sampled from LoLi-Phone-imgT dataset. (a) input (b) LLNet (c) LightenNet (d) Retinex-Net (e) MBLLEN (f) KinD (g) KinD++ (h) TBEFN (i) DSLR (j) EnlightenGAN (k) DRBN (l) ExCNet (m) Zero-DCE (n) RRDNet \relax }{figure.caption.9}{}}
\newlabel{fig: LoLi-Phone-imgT_1_a}{{6a}{10}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_a}{{a}{10}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_1_b}{{6b}{10}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_b}{{b}{10}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_1_c}{{6c}{10}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_c}{{c}{10}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_1_d}{{6d}{10}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_d}{{d}{10}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_1_e}{{6e}{10}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_e}{{e}{10}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_1_f}{{6f}{10}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_f}{{f}{10}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_1_g}{{6g}{10}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_g}{{g}{10}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_1_h}{{6h}{10}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_h}{{h}{10}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_1_i}{{6i}{10}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_i}{{i}{10}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_1_j}{{6j}{10}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_j}{{j}{10}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_1_k}{{6k}{10}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_k}{{k}{10}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_1_l}{{6l}{10}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_l}{{l}{10}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_1_m}{{6m}{10}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_m}{{m}{10}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_1_n}{{6n}{10}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_n}{{n}{10}{\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces   Visual results of different methods on a low-light image sampled from LoLi-Phone-imgT dataset. (a) input (b) LLNet (c) LightenNet (d) Retinex-Net (e) MBLLEN (f) KinD (g) KinD++ (h) TBEFN (i) DSLR (j) EnlightenGAN (k) DRBN (l) ExCNet (m) Zero-DCE (n) RRDNet \relax }}{10}{figure.caption.10}\protected@file@percent }
\newlabel{fig: Visual Result from LoLi-Phone-imgT_1 dataset}{{6}{10}{Visual results of different methods on a low-light image sampled from LoLi-Phone-imgT dataset. (a) input (b) LLNet (c) LightenNet (d) Retinex-Net (e) MBLLEN (f) KinD (g) KinD++ (h) TBEFN (i) DSLR (j) EnlightenGAN (k) DRBN (l) ExCNet (m) Zero-DCE (n) RRDNet \relax }{figure.caption.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces  Quantitative comparisons on LoLi-Phone-imgT dataset in terms of NIQE, LOE, PI, and SPAQ. The best result is in red whereas the second and third best results are in blue and purple under each case, respectively.\relax }}{11}{table.caption.12}\protected@file@percent }
\newlabel{tab: Quantitative comparisons on LoLi-Phone-imgT dataset}{{6}{11}{Quantitative comparisons on LoLi-Phone-imgT dataset in terms of NIQE, LOE, PI, and SPAQ. The best result is in red whereas the second and third best results are in blue and purple under each case, respectively.\relax }{table.caption.12}{}}
\@writefile{toc}{\contentsline {paragraph}{Computational Complexity}{11}{table.caption.12}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces  Quantitative comparisons of computational complexity in terms of runtime (in second), number of trainable parameters (\#Parameters) (in M), and FLOPs (in G). The best result is in \textcolor {red}{red} whereas the second and third best results are in \textcolor {blue}{blue} and \textcolor {purple}{purple} under each case, respectively. ‘-’ indicates the result is not available.\relax }}{11}{table.caption.13}\protected@file@percent }
\newlabel{tab: Quantitative comparisons of computational complexity}{{7}{11}{Quantitative comparisons of computational complexity in terms of runtime (in second), number of trainable parameters (\#Parameters) (in M), and FLOPs (in G). The best result is in \textcolor {red}{red} whereas the second and third best results are in \textcolor {blue}{blue} and \textcolor {purple}{purple} under each case, respectively. ‘-’ indicates the result is not available.\relax }{table.caption.13}{}}
\@writefile{toc}{\contentsline {paragraph}{Application-based Evaluation}{12}{table.caption.13}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces   The P-R curves of face detection in the dark. \relax }}{12}{figure.caption.14}\protected@file@percent }
\newlabel{fig: P-R_curves}{{7}{12}{The P-R curves of face detection in the dark. \relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {paragraph}{Discussion}{12}{figure.caption.15}\protected@file@percent }
\newlabel{fig: DARK_FACE_a}{{8a}{13}{input\relax }{figure.caption.15}{}}
\newlabel{sub@fig: DARK_FACE_a}{{a}{13}{input\relax }{figure.caption.15}{}}
\newlabel{fig: DARK_FACE_b}{{8b}{13}{LightenNet\relax }{figure.caption.15}{}}
\newlabel{sub@fig: DARK_FACE_b}{{b}{13}{LightenNet\relax }{figure.caption.15}{}}
\newlabel{fig: DARK_FACE_c}{{8c}{13}{Retinex-Net\relax }{figure.caption.15}{}}
\newlabel{sub@fig: DARK_FACE_c}{{c}{13}{Retinex-Net\relax }{figure.caption.15}{}}
\newlabel{fig: DARK_FACE_d}{{8d}{13}{MBLLEN\relax }{figure.caption.15}{}}
\newlabel{sub@fig: DARK_FACE_d}{{d}{13}{MBLLEN\relax }{figure.caption.15}{}}
\newlabel{fig: DARK_FACE_e}{{8e}{13}{KinD++\relax }{figure.caption.15}{}}
\newlabel{sub@fig: DARK_FACE_e}{{e}{13}{KinD++\relax }{figure.caption.15}{}}
\newlabel{fig: DARK_FACE_f}{{8f}{13}{TBEFN\relax }{figure.caption.15}{}}
\newlabel{sub@fig: DARK_FACE_f}{{f}{13}{TBEFN\relax }{figure.caption.15}{}}
\newlabel{fig: DARK_FACE_g}{{8g}{13}{DSLR\relax }{figure.caption.15}{}}
\newlabel{sub@fig: DARK_FACE_g}{{g}{13}{DSLR\relax }{figure.caption.15}{}}
\newlabel{fig: DARK_FACE_h}{{8h}{13}{EnlightenGAN\relax }{figure.caption.15}{}}
\newlabel{sub@fig: DARK_FACE_h}{{h}{13}{EnlightenGAN\relax }{figure.caption.15}{}}
\newlabel{fig:DARK_FACE_i}{{8i}{13}{DRBN\relax }{figure.caption.15}{}}
\newlabel{sub@fig:DARK_FACE_i}{{i}{13}{DRBN\relax }{figure.caption.15}{}}
\newlabel{fig: DARK_FACE_j}{{8j}{13}{ExCNet\relax }{figure.caption.15}{}}
\newlabel{sub@fig: DARK_FACE_j}{{j}{13}{ExCNet\relax }{figure.caption.15}{}}
\newlabel{fig: DARK_FACE_k}{{8k}{13}{Zero-DCE\relax }{figure.caption.15}{}}
\newlabel{sub@fig: DARK_FACE_k}{{k}{13}{Zero-DCE\relax }{figure.caption.15}{}}
\newlabel{fig: DARK_FACE_l}{{8l}{13}{RRDNet\relax }{figure.caption.15}{}}
\newlabel{sub@fig: DARK_FACE_l}{{l}{13}{RRDNet\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces   Visual results of different methods on a low-light image sampled from DARK FACE dataset. Better see with zoom in for the bounding boxes of faces. \relax }}{13}{figure.caption.15}\protected@file@percent }
\newlabel{fig: Visual Result from DARK_FACE dataset}{{8}{13}{Visual results of different methods on a low-light image sampled from DARK FACE dataset. Better see with zoom in for the bounding boxes of faces. \relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {paragraph}{Future Research Directions}{13}{figure.caption.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Effective Learning Strategies}{13}{figure.caption.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Specialized Network Structures}{13}{figure.caption.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Loss Function}{13}{figure.caption.15}\protected@file@percent }
\citation{GLADNet}
\@writefile{toc}{\contentsline {subparagraph}{Realistic Training Data}{14}{figure.caption.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Standard Testing Data}{14}{figure.caption.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Task-Specific Evaluation Metrics}{14}{figure.caption.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Robust Generalization Capability}{14}{figure.caption.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Extension to Low-light Video Enhancement}{14}{figure.caption.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Integrating Semantic Information}{14}{figure.caption.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.4}项目链接}{14}{subsubsection.1.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}GLADNet: Low Light Enhancement Network with Global Awareness}{15}{subsection.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces   The architecture of GLADNet. The architecture consists of two steps, global illumination estimation step and detail reconstruction step. In the first step, the encoder-decoder network produces an illumination estimation of a fixed size (96 × 96 here). In the second step, a convolutional network utilizes the input image and the outputs from the previous step to compensate the details. \relax }}{15}{figure.caption.16}\protected@file@percent }
\newlabel{fig:GLADNet}{{9}{15}{The architecture of GLADNet. The architecture consists of two steps, global illumination estimation step and detail reconstruction step. In the first step, the encoder-decoder network produces an illumination estimation of a fixed size (96 × 96 here). In the second step, a convolutional network utilizes the input image and the outputs from the previous step to compensate the details. \relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}全局亮度先验估计}{15}{subsubsection.1.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.2}细节还原}{15}{subsubsection.1.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.3}损失函数}{16}{subsubsection.1.3.3}\protected@file@percent }
\newlabel{eq:L1}{{1}{16}{损失函数}{equation.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Deep Retinex Decomposition for Low Light Enhancement}{16}{subsection.1.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces   The proposed framework for Retinex-Net. The enhancement process is divided into three steps: decomposition, adjustment and reconstruction. In the decomposition step, a subnetwork Decom-Net decomposes the input image into reflectance and illumination. In the following adjustment step, an encoder-decoder based Enhance-Net brightens up the illumination. Multi-scale concatenation is introduced to adjust the illumination from multi-scale perspectives. Noise on the reflectance is also removed at this step. Finally, we reconstruct the adjusted illumination and reflectance to get the enhanced result. \relax }}{17}{figure.caption.17}\protected@file@percent }
\newlabel{fig:RetinexNet}{{10}{17}{The proposed framework for Retinex-Net. The enhancement process is divided into three steps: decomposition, adjustment and reconstruction. In the decomposition step, a subnetwork Decom-Net decomposes the input image into reflectance and illumination. In the following adjustment step, an encoder-decoder based Enhance-Net brightens up the illumination. Multi-scale concatenation is introduced to adjust the illumination from multi-scale perspectives. Noise on the reflectance is also removed at this step. Finally, we reconstruct the adjusted illumination and reflectance to get the enhanced result. \relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.1}损失函数}{17}{subsubsection.1.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Kindling the Darkness: A Practical Low Light Image Enhancer}{18}{subsection.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces   The architecture of our KinD network. Two branches correspond to the reflectance and illumination, respectively. From the perspective of functionality, it also can be divided into three modules, including layer decomposition, reflectance restoration, and illumination adjustment. \relax }}{18}{figure.caption.18}\protected@file@percent }
\newlabel{fig:KinD}{{11}{18}{The architecture of our KinD network. Two branches correspond to the reflectance and illumination, respectively. From the perspective of functionality, it also can be divided into three modules, including layer decomposition, reflectance restoration, and illumination adjustment. \relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.1}损失函数}{19}{subsubsection.1.5.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Quantitative comparison on LOL dataset in terms of PSNR, SSIM, LOE, $\text  {LOE}_{ref}$, and NIQE. The best results are highlighted in bold\relax }}{19}{table.caption.19}\protected@file@percent }
\newlabel{tab:Quantitative}{{8}{19}{Quantitative comparison on LOL dataset in terms of PSNR, SSIM, LOE, $\text {LOE}_{ref}$, and NIQE. The best results are highlighted in bold\relax }{table.caption.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6}MSRNet Low Light Image Enhancement using Deep Convolutional Networks}{20}{subsection.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7}A Pipeline Neural Network for Low Light Image Enhancemen}{20}{subsection.1.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8}LLCNN A Convolutional Neural Network for Low Light Image Enhancement}{20}{subsection.1.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.9}DSLR Quality Photos on Mobile Devices with Deep Convolutional Networks}{20}{subsection.1.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.10}Learning to see in the dark}{20}{subsection.1.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.11}Learning Digital Camera Pipeline for Extreme Low Light Imaging}{20}{subsection.1.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.11.1}损失函数}{20}{subsubsection.1.11.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.12}End to End Denoising of Dark Burst Images using Recurrent Fully Convolutionaly Networks}{21}{subsection.1.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.12.1}损失函数}{21}{subsubsection.1.12.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.13}Deep Burst Noising}{21}{subsection.1.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.14}DeepISP Toward Learning an End to End Image Processing Pipeline}{21}{subsection.1.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.14.1}网络架构}{21}{subsubsection.1.14.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Low Level Stage}{21}{figure.caption.20}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces   Proposed network architecture. The network consists of two stages: Lower level and higher level. Layers that output features are colored dark blue and layers that output an image (or residual image) are colored bright blue.\relax }}{22}{figure.caption.20}\protected@file@percent }
\newlabel{fig: network architecture DeepISP}{{12}{22}{Proposed network architecture. The network consists of two stages: Lower level and higher level. Layers that output features are colored dark blue and layers that output an image (or residual image) are colored bright blue.\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {paragraph}{High Level Stage}{22}{figure.caption.20}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Output}{22}{figure.caption.20}\protected@file@percent }
\newlabel{eq:Transform}{{10}{22}{Output}{equation.1.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.14.2}损失函数}{23}{subsubsection.1.14.2}\protected@file@percent }
\newlabel{eq:DeepISP loss function}{{11}{23}{损失函数}{equation.1.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces   Results for joint denoising and demosaicing. Trained and tested on images from the MSR Dataset [29]. The input to the network is demosaiced by bilinear interpolation. The artifacts caused by the interpolation are visible in the middle column images and the model learns to remove them quite well.\relax }}{23}{figure.caption.21}\protected@file@percent }
\newlabel{fig: Results for joint denoising and demosaicinge DeepISP}{{13}{23}{Results for joint denoising and demosaicing. Trained and tested on images from the MSR Dataset [29]. The input to the network is demosaiced by bilinear interpolation. The artifacts caused by the interpolation are visible in the middle column images and the model learns to remove them quite well.\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.15}Underexposed Photo Enhancement using Deep Illumination Estimation}{23}{subsection.1.15}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces   Overview of our network. First, we downsample and encode the input into a feature map, extract local and global features, and concatenate them to predict the low-res illumination via a convolution layer. Then we upsample the result to produce the full-res multi-channel illumination $S$ (hot color map), and take it to recover the full-res enhanced image. We train the end-to-end network to learn $S$ from image pairs {$I_i,\tilde  {I}_i$} with three loss components {$L_{r}^i,L_{s}^i ,L_{c}^i$}.\relax }}{24}{figure.caption.22}\protected@file@percent }
\newlabel{fig:network overview}{{14}{24}{Overview of our network. First, we downsample and encode the input into a feature map, extract local and global features, and concatenate them to predict the low-res illumination via a convolution layer. Then we upsample the result to produce the full-res multi-channel illumination $S$ (hot color map), and take it to recover the full-res enhanced image. We train the end-to-end network to learn $S$ from image pairs {$I_i,\tilde {I}_i$} with three loss components {$L_{r}^i,L_{s}^i ,L_{c}^i$}.\relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.15.1}网络架构}{24}{subsubsection.1.15.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{有效学习}{24}{subsubsection.1.15.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{高效计算}{24}{subsubsection.1.15.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.15.2}损失函数}{24}{subsubsection.1.15.2}\protected@file@percent }
\newlabel{eq:Loss function 2019 }{{12}{24}{损失函数}{equation.1.12}{}}
\@writefile{toc}{\contentsline {paragraph}{重建损失}{25}{equation.1.12}\protected@file@percent }
\newlabel{eq:Rebuilding loss}{{13}{25}{重建损失}{equation.1.13}{}}
\@writefile{toc}{\contentsline {paragraph}{平滑损失}{25}{equation.1.13}\protected@file@percent }
\newlabel{eq:Smooth loss}{{14}{25}{平滑损失}{equation.1.14}{}}
\@writefile{toc}{\contentsline {paragraph}{颜色损失}{25}{equation.1.14}\protected@file@percent }
\newlabel{eq:color loss}{{15}{25}{颜色损失}{equation.1.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.16}Deep Bilateral Learning for Real Time Image Enhancement}{25}{subsection.1.16}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces   Our new network architecture seeks to perform as much computation as possible at a low resolution, while still capturing high-frequency effects at full image resolution. It consists of two distinct streams operating at different resolutions. The \textit  {low-resolution} stream (top) processes a downsampled version $\tilde  {I}$ of the input I through several convolutional layers so as to estimate a bilateral grid of affine coefficients $A$. This low-resolution stream is further split in two paths to learn both local features $L^i$ and global features $G^i$ , which are fused ($F$) before making the final prediction. The global and local paths share a common set of low-level features $S^i$ . In turn, the \textit  {high-resolution} stream (bottom) performs a minimal yet critical amount of work: it learns a grayscale guidance map $g$ used by our new slicing node to upsample the grid of affine coefficients back to full-resolution $\bar  {A}$. These per-pixel local affine transformations are then applied to the full-resolution input, which yields the final output $O$.\relax }}{26}{figure.caption.23}\protected@file@percent }
\newlabel{fig:network architecture 2017}{{15}{26}{Our new network architecture seeks to perform as much computation as possible at a low resolution, while still capturing high-frequency effects at full image resolution. It consists of two distinct streams operating at different resolutions. The \textit {low-resolution} stream (top) processes a downsampled version $\tilde {I}$ of the input I through several convolutional layers so as to estimate a bilateral grid of affine coefficients $A$. This low-resolution stream is further split in two paths to learn both local features $L^i$ and global features $G^i$ , which are fused ($F$) before making the final prediction. The global and local paths share a common set of low-level features $S^i$ . In turn, the \textit {high-resolution} stream (bottom) performs a minimal yet critical amount of work: it learns a grayscale guidance map $g$ used by our new slicing node to upsample the grid of affine coefficients back to full-resolution $\bar {A}$. These per-pixel local affine transformations are then applied to the full-resolution input, which yields the final output $O$.\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.16.1}网络架构}{26}{subsubsection.1.16.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{低分辨率分支}{26}{subsubsection.1.16.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{高分辨率分支}{27}{subsubsection.1.16.1}\protected@file@percent }
\newlabel{eq:Linear affine transformation }{{16}{27}{高分辨率分支}{equation.1.16}{}}
\@writefile{toc}{\contentsline {paragraph}{重建分支}{27}{equation.1.16}\protected@file@percent }
\newlabel{eq:Computation of affine parameters }{{17}{27}{重建分支}{equation.1.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces   Our method (d) can learn to replicate the correct effect (b) for operations that are not scale invariant, such as the Local Laplacian filter shown here (a–b). Methods like Bilateral Guided Upsampling that only apply the operation at low-resolution (insets (a–b)) produce a different-looking output (c). The difference is most noticeable in the areas pointed by the arrows.\relax }}{27}{figure.caption.24}\protected@file@percent }
\newlabel{fig:method 2017}{{16}{27}{Our method (d) can learn to replicate the correct effect (b) for operations that are not scale invariant, such as the Local Laplacian filter shown here (a–b). Methods like Bilateral Guided Upsampling that only apply the operation at low-resolution (insets (a–b)) produce a different-looking output (c). The difference is most noticeable in the areas pointed by the arrows.\relax }{figure.caption.24}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}个人工作进展}{28}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}疾病的诊断与AI结合的初步调研}{28}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}甲状腺功能亢进}{28}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{诊断}{28}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{病史和体格检查}{28}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{血液检测}{28}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{更进一步的检查}{29}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{治疗}{29}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{抗甲状腺药物}{29}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{$\beta $-受体阻滞剂}{30}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{放射性碘疗法}{30}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{甲状腺切除术}{30}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{发展}{30}{subsubsection.2.1.1}\protected@file@percent }
\citation{ConVNet}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}癫痫前期诊断---望闻问切}{31}{subsubsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{采用人工智能驱动的ConvNet检测癫痫发作}{31}{subsubsection.2.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces   Illustration of CNN architecture of the proposed model. \relax }}{32}{figure.caption.25}\protected@file@percent }
\newlabel{fig:ConvNet}{{17}{32}{Illustration of CNN architecture of the proposed model. \relax }{figure.caption.25}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}按照paper脉络梳理暗弱光技术}{32}{subsection.2.2}\protected@file@percent }
\bibstyle{unsrt}
\bibdata{reference}
\bibcite{GLADNet}{{1}{}{{}}{{}}}
\bibcite{ConVNet}{{2}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\@writefile{toc}{\contentsline {section}{\numberline {3}下周工作计划}{33}{section.3}\protected@file@percent }
\gdef \@abspage@last{33}
