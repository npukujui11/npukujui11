\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{rana2021edge}
\citation{zhu2020eemefn}
\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Pre-Knowledge}{2}{part.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1}方向一}{2}{section.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces   The proposed network with feature extraction module (FEM), enhancement module (EM) and fusion module (FM). The output image is produced via feature fusion. \relax }}{2}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig: MBLLEN Architecture}{{1}{2}{The proposed network with feature extraction module (FEM), enhancement module (EM) and fusion module (FM). The output image is produced via feature fusion. \relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}方向二}{2}{section.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces   The overview of our framework with explicit appearance modeling $\mathcal  {A}$, structure modeling $\mathcal  {S}$, and SGEM $\mathcal  {E}$. The supervision for $I_a$ and $\setbox \z@ \hbox {\mathsurround \z@ $\textstyle I$}\mathaccent "0362{I}$ is the normal-light image $\bar  {I}$, and for $I_s$ is the edge $\bar  {I}_s$ extracted from $\bar  {I}$. The overall framework can be trained end-to-end. \relax }}{3}{figure.caption.2}\protected@file@percent }
\newlabel{fig: Overview}{{2}{3}{The overview of our framework with explicit appearance modeling $\mathcal {A}$, structure modeling $\mathcal {S}$, and SGEM $\mathcal {E}$. The supervision for $I_a$ and $\widehat {I}$ is the normal-light image $\bar {I}$, and for $I_s$ is the edge $\bar {I}_s$ extracted from $\bar {I}$. The overall framework can be trained end-to-end. \relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {part}{II\hspace  {1em}Paper Reading}{3}{part.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}LLIE}{4}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}(2023.1)Illumination estimation for nature preserving low-light image enhancement}{4}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{低光图像增强中保留自然光的照度估计}{4}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(The Visual Computer 3区) doi: 10.1007/s00371-023-02770-9}{4}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Research Background}{4}{subsubsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Contribution}{4}{subsubsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Approach}{4}{subsubsection.3.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}Future}{5}{subsubsection.3.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}(2023.7)Division Gets Better: Learning Brightness-Aware and Detail-Sensitive Representations for Low-Light Image Enhancement}{5}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{分割可以变得更好：学习亮度感知和细节敏感的表示为低光图像增强}{5}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(CVPR 2023) doi: 10.48550/arXiv.2307.09104}{5}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Research Background}{5}{subsubsection.3.2.1}\protected@file@percent }
\newlabel{fig: RGB-low light1}{{\caption@xref {fig: RGB-low light1}{ on input line 215}}{5}{Research Background}{figure.caption.3}{}}
\newlabel{sub@fig: RGB-low light1}{{}{5}{Research Background}{figure.caption.3}{}}
\newlabel{fig: R1}{{\caption@xref {fig: R1}{ on input line 220}}{5}{Research Background}{figure.caption.3}{}}
\newlabel{sub@fig: R1}{{}{5}{Research Background}{figure.caption.3}{}}
\newlabel{fig: G1}{{\caption@xref {fig: G1}{ on input line 225}}{5}{Research Background}{figure.caption.3}{}}
\newlabel{sub@fig: G1}{{}{5}{Research Background}{figure.caption.3}{}}
\newlabel{fig: B1}{{\caption@xref {fig: B1}{ on input line 230}}{5}{Research Background}{figure.caption.3}{}}
\newlabel{sub@fig: B1}{{}{5}{Research Background}{figure.caption.3}{}}
\newlabel{fig: Y1}{{\caption@xref {fig: Y1}{ on input line 235}}{5}{Research Background}{figure.caption.3}{}}
\newlabel{sub@fig: Y1}{{}{5}{Research Background}{figure.caption.3}{}}
\newlabel{fig: Cb1}{{\caption@xref {fig: Cb1}{ on input line 240}}{5}{Research Background}{figure.caption.3}{}}
\newlabel{sub@fig: Cb1}{{}{5}{Research Background}{figure.caption.3}{}}
\newlabel{fig: Cr1}{{\caption@xref {fig: Cr1}{ on input line 245}}{5}{Research Background}{figure.caption.3}{}}
\newlabel{sub@fig: Cr1}{{}{5}{Research Background}{figure.caption.3}{}}
\newlabel{fig: Ours1}{{\caption@xref {fig: Ours1}{ on input line 250}}{5}{Research Background}{figure.caption.3}{}}
\newlabel{sub@fig: Ours1}{{}{5}{Research Background}{figure.caption.3}{}}
\newlabel{fig: RGB-low light2}{{\caption@xref {fig: RGB-low light2}{ on input line 256}}{5}{Research Background}{figure.caption.3}{}}
\newlabel{sub@fig: RGB-low light2}{{}{5}{Research Background}{figure.caption.3}{}}
\newlabel{fig: R2}{{\caption@xref {fig: R2}{ on input line 261}}{5}{Research Background}{figure.caption.3}{}}
\newlabel{sub@fig: R2}{{}{5}{Research Background}{figure.caption.3}{}}
\newlabel{fig: G2}{{\caption@xref {fig: G2}{ on input line 266}}{5}{Research Background}{figure.caption.3}{}}
\newlabel{sub@fig: G2}{{}{5}{Research Background}{figure.caption.3}{}}
\newlabel{fig: B2}{{\caption@xref {fig: B2}{ on input line 271}}{5}{Research Background}{figure.caption.3}{}}
\newlabel{sub@fig: B2}{{}{5}{Research Background}{figure.caption.3}{}}
\newlabel{fig: Y2}{{\caption@xref {fig: Y2}{ on input line 276}}{5}{Research Background}{figure.caption.3}{}}
\newlabel{sub@fig: Y2}{{}{5}{Research Background}{figure.caption.3}{}}
\newlabel{fig: Cb2}{{\caption@xref {fig: Cb2}{ on input line 281}}{5}{Research Background}{figure.caption.3}{}}
\newlabel{sub@fig: Cb2}{{}{5}{Research Background}{figure.caption.3}{}}
\newlabel{fig: Cr2}{{\caption@xref {fig: Cr2}{ on input line 286}}{5}{Research Background}{figure.caption.3}{}}
\newlabel{sub@fig: Cr2}{{}{5}{Research Background}{figure.caption.3}{}}
\newlabel{fig: Ours2}{{\caption@xref {fig: Ours2}{ on input line 291}}{5}{Research Background}{figure.caption.3}{}}
\newlabel{sub@fig: Ours2}{{}{5}{Research Background}{figure.caption.3}{}}
\newlabel{fig: RGB-low light3}{{\caption@xref {fig: RGB-low light3}{ on input line 298}}{5}{Research Background}{figure.caption.3}{}}
\newlabel{sub@fig: RGB-low light3}{{}{5}{Research Background}{figure.caption.3}{}}
\newlabel{fig: R3}{{\caption@xref {fig: R3}{ on input line 304}}{5}{Research Background}{figure.caption.3}{}}
\newlabel{sub@fig: R3}{{}{5}{Research Background}{figure.caption.3}{}}
\newlabel{fig: G3}{{\caption@xref {fig: G3}{ on input line 310}}{5}{Research Background}{figure.caption.3}{}}
\newlabel{sub@fig: G3}{{}{5}{Research Background}{figure.caption.3}{}}
\newlabel{fig: B3}{{\caption@xref {fig: B3}{ on input line 316}}{5}{Research Background}{figure.caption.3}{}}
\newlabel{sub@fig: B3}{{}{5}{Research Background}{figure.caption.3}{}}
\newlabel{fig: Y3}{{\caption@xref {fig: Y3}{ on input line 322}}{5}{Research Background}{figure.caption.3}{}}
\newlabel{sub@fig: Y3}{{}{5}{Research Background}{figure.caption.3}{}}
\newlabel{fig: Cb3}{{\caption@xref {fig: Cb3}{ on input line 328}}{5}{Research Background}{figure.caption.3}{}}
\newlabel{sub@fig: Cb3}{{}{5}{Research Background}{figure.caption.3}{}}
\newlabel{fig: Cr3}{{\caption@xref {fig: Cr3}{ on input line 334}}{5}{Research Background}{figure.caption.3}{}}
\newlabel{sub@fig: Cr3}{{}{5}{Research Background}{figure.caption.3}{}}
\newlabel{fig: Our3}{{\caption@xref {fig: Our3}{ on input line 340}}{5}{Research Background}{figure.caption.3}{}}
\newlabel{sub@fig: Our3}{{}{5}{Research Background}{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces   Low-light images and their luminance and chrominance decomposition. The first four columns show original low-light images in RGB space and their RGB channel decomposition, the fifth column shows luminance (Y) channels of low-light images, the sixth and seventh columns are chrominance (CbCr) components of low-light images, and the last column exhibits the enhanced images by our method. We can obviously observe that three channels of RGB space indicate indistinguishable distortion patterns, and Y and CbCr components reveal distinctly different distortion patterns. \relax }}{5}{figure.caption.3}\protected@file@percent }
\newlabel{fig: Low-light images and their luminance and chrominance decomposition}{{3}{5}{Low-light images and their luminance and chrominance decomposition. The first four columns show original low-light images in RGB space and their RGB channel decomposition, the fifth column shows luminance (Y) channels of low-light images, the sixth and seventh columns are chrominance (CbCr) components of low-light images, and the last column exhibits the enhanced images by our method. We can obviously observe that three channels of RGB space indicate indistinguishable distortion patterns, and Y and CbCr components reveal distinctly different distortion patterns. \relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Contribution}{5}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Approach}{6}{subsubsection.3.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces   The overview of the proposed LCDBNet. The input images are transformed from RGB space to YCbCr space. Luminance and chrominance components are fed into LAN and CRN, respectively. Then, their outputs are fused in FN to derive the enhanced results. Finally, the enhanced images are converted back to RGB space. \relax }}{6}{figure.caption.4}\protected@file@percent }
\newlabel{fig: LCDBNet}{{4}{6}{The overview of the proposed LCDBNet. The input images are transformed from RGB space to YCbCr space. Luminance and chrominance components are fed into LAN and CRN, respectively. Then, their outputs are fused in FN to derive the enhanced results. Finally, the enhanced images are converted back to RGB space. \relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4}Future}{6}{subsubsection.3.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}(2023.3)Advanced RetinexNet: A fully convolutional network for low-light image enhancement}{7}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Advanced RetinexNet：用于弱光图像增强的全卷积网络}{7}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(Signal Processing: Image Communication 2区) doi: 10.1016/j.image.2022.116916}{7}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Research Background}{7}{subsubsection.3.3.1}\protected@file@percent }
\newlabel{fig: Input1}{{\caption@xref {fig: Input1}{ on input line 414}}{7}{Research Background}{figure.caption.5}{}}
\newlabel{sub@fig: Input1}{{}{7}{Research Background}{figure.caption.5}{}}
\newlabel{fig: LLME1}{{\caption@xref {fig: LLME1}{ on input line 419}}{7}{Research Background}{figure.caption.5}{}}
\newlabel{sub@fig: LLME1}{{}{7}{Research Background}{figure.caption.5}{}}
\newlabel{fig: SRIE1}{{\caption@xref {fig: SRIE1}{ on input line 424}}{7}{Research Background}{figure.caption.5}{}}
\newlabel{sub@fig: SRIE1}{{}{7}{Research Background}{figure.caption.5}{}}
\newlabel{fig: Our1}{{\caption@xref {fig: Our1}{ on input line 429}}{7}{Research Background}{figure.caption.5}{}}
\newlabel{sub@fig: Our1}{{}{7}{Research Background}{figure.caption.5}{}}
\newlabel{fig: Input2}{{5a}{7}{Input\relax }{figure.caption.5}{}}
\newlabel{sub@fig: Input2}{{a}{7}{Input\relax }{figure.caption.5}{}}
\newlabel{fig: LLME2}{{5b}{7}{LLME\relax }{figure.caption.5}{}}
\newlabel{sub@fig: LLME2}{{b}{7}{LLME\relax }{figure.caption.5}{}}
\newlabel{fig: SRIE2}{{5c}{7}{SRIE\relax }{figure.caption.5}{}}
\newlabel{sub@fig: SRIE2}{{c}{7}{SRIE\relax }{figure.caption.5}{}}
\newlabel{fig: Our2}{{5d}{7}{Our\relax }{figure.caption.5}{}}
\newlabel{sub@fig: Our2}{{d}{7}{Our\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces   Examples of low-light image enhanced results. The proposed method can not only improve the contrast of the image but also suppress the noise and artifacts in the dark regions. LIME generates color distorted results and causes noise amplification. SRIE generates under-enhancement results. \relax }}{7}{figure.caption.5}\protected@file@percent }
\newlabel{fig: Examples of LLIE}{{5}{7}{Examples of low-light image enhanced results. The proposed method can not only improve the contrast of the image but also suppress the noise and artifacts in the dark regions. LIME generates color distorted results and causes noise amplification. SRIE generates under-enhancement results. \relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Contribution}{7}{subsubsection.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Approach}{7}{subsubsection.3.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces   The overall network architecture of our proposed method. \relax }}{8}{figure.caption.6}\protected@file@percent }
\newlabel{fig: Advanced RetinexNet}{{6}{8}{The overall network architecture of our proposed method. \relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces   The proposed Decom-Net architecture. The Decom-Net decomposes the input image into reflectance and illumination and can suppress the noise in the reflectance map. Note that the decompositions of normal-light images do not participate in the Enhance-Net training stage. \relax }}{8}{figure.caption.7}\protected@file@percent }
\newlabel{fig: Decom-Net}{{7}{8}{The proposed Decom-Net architecture. The Decom-Net decomposes the input image into reflectance and illumination and can suppress the noise in the reflectance map. Note that the decompositions of normal-light images do not participate in the Enhance-Net training stage. \relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces   The proposed Enhance-Net architecture. The Enhance-Net takes the output of Decom-Net as the input to enhance the contrast and brightness of the illumination. \relax }}{8}{figure.caption.8}\protected@file@percent }
\newlabel{fig: Enhance-Net}{{8}{8}{The proposed Enhance-Net architecture. The Enhance-Net takes the output of Decom-Net as the input to enhance the contrast and brightness of the illumination. \relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.4}Future}{9}{subsubsection.3.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}(2023.1)LightingNet: An Integrated Learning Method for Low-Light Image Enhancement}{9}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{LightingNet：一种用于弱光图像增强的集成学习方法}{9}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(IEEE Transactions on Computational Imaging 2区) doi: 10.1109/TCI.2023.3240087}{9}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Research Background}{9}{subsubsection.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Contribution}{9}{subsubsection.3.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.3}Approach}{9}{subsubsection.3.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces   Architecture of the proposed LightingNet. \relax }}{10}{figure.caption.9}\protected@file@percent }
\newlabel{fig: LightingNet}{{9}{10}{Architecture of the proposed LightingNet. \relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.4}Future}{10}{subsubsection.3.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}(2022.10)Low-light Image Enhancement via Breaking Down the Darkness}{10}{subsection.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{通过破坏黑暗来进行低光图像增强}{10}{subsection.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(International Journal of Computer Vision 2区) doi: 10.1007/s11263-022-01667-9}{10}{subsection.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.1}Research Background}{10}{subsubsection.3.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.2}Contribution}{10}{subsubsection.3.5.2}\protected@file@percent }
\bibstyle{unsrt}
\bibdata{reference}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces   The overall network architecture of our proposed Bread framework \relax }}{11}{figure.caption.10}\protected@file@percent }
\newlabel{fig: Bread}{{10}{11}{The overall network architecture of our proposed Bread framework \relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces   The U-shaped architecture of our sub-networks, including IAN, ANSN, NFM, and CAN \relax }}{11}{figure.caption.11}\protected@file@percent }
\newlabel{fig: U-shaped}{{11}{11}{The U-shaped architecture of our sub-networks, including IAN, ANSN, NFM, and CAN \relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.3}Approach}{11}{subsubsection.3.5.3}\protected@file@percent }
\bibcite{rana2021edge}{1}
\bibcite{zhu2020eemefn}{2}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.4}Future}{12}{subsubsection.3.5.4}\protected@file@percent }
\newlabel{LastPage}{{}{12}{}{page.12}{}}
\xdef\lastpage@lastpage{12}
\xdef\lastpage@lastpageHy{12}
\gdef \@abspage@last{12}
