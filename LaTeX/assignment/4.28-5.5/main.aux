\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}文献阅读}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Pre-knowledge}{1}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}(综述2021.04)Lighting the darkness in the deep learning era}{2}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}背景}{2}{subsubsection.1.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2}介绍}{2}{subsubsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.3}细节}{2}{subsubsection.1.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Network Structure}{2}{figure.caption.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces  Summary of essential characteristics of representative deep learning-based methods, including learning strategies, network structures, loss functions, training datasets, testing datasets, evaluation metrics, data formats of input, and whether the models are Retinex-based or not. "simulated" means the testing data are simulated by the same approach as the synthetic training data. "self-selected" stands for the real-world images selected by the authors. "\#P" represents the number of trainable parameters. "-" means this item is not available or not indicated in the paper.\relax }}{3}{table.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab: Summary}{{1}{3}{Summary of essential characteristics of representative deep learning-based methods, including learning strategies, network structures, loss functions, training datasets, testing datasets, evaluation metrics, data formats of input, and whether the models are Retinex-based or not. "simulated" means the testing data are simulated by the same approach as the synthetic training data. "self-selected" stands for the real-world images selected by the authors. "\#P" represents the number of trainable parameters. "-" means this item is not available or not indicated in the paper.\relax }{table.caption.1}{}}
\newlabel{fig:subfig_a}{{1a}{4}{learning strategy\relax }{figure.caption.2}{}}
\newlabel{sub@fig:subfig_a}{{a}{4}{learning strategy\relax }{figure.caption.2}{}}
\newlabel{fig:subfig_b}{{1b}{4}{network structure\relax }{figure.caption.2}{}}
\newlabel{sub@fig:subfig_b}{{b}{4}{network structure\relax }{figure.caption.2}{}}
\newlabel{fig:subfig_c}{{1c}{4}{Retinex model\relax }{figure.caption.2}{}}
\newlabel{sub@fig:subfig_c}{{c}{4}{Retinex model\relax }{figure.caption.2}{}}
\newlabel{fig:subfig_d}{{1d}{4}{data format\relax }{figure.caption.2}{}}
\newlabel{sub@fig:subfig_d}{{d}{4}{data format\relax }{figure.caption.2}{}}
\newlabel{fig:subfig_e}{{1e}{4}{loss function\relax }{figure.caption.2}{}}
\newlabel{sub@fig:subfig_e}{{e}{4}{loss function\relax }{figure.caption.2}{}}
\newlabel{fig:subfig_f}{{1f}{4}{training dataset\relax }{figure.caption.2}{}}
\newlabel{sub@fig:subfig_f}{{f}{4}{training dataset\relax }{figure.caption.2}{}}
\newlabel{fig:subfig_g}{{1g}{4}{testing dataset\relax }{figure.caption.2}{}}
\newlabel{sub@fig:subfig_g}{{g}{4}{testing dataset\relax }{figure.caption.2}{}}
\newlabel{fig:subfig_h}{{1h}{4}{evaluation metric\relax }{figure.caption.2}{}}
\newlabel{sub@fig:subfig_h}{{h}{4}{evaluation metric\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces   A statictic analysis of deep learning-based LLIE methods, including learning strategy, network characteristic, Retinex model, data format, loss function, training dataset, testing dataset, and evaluation metric. Better to see with zoom. \relax }}{4}{figure.caption.2}\protected@file@percent }
\newlabel{fig: Statictic Analysis}{{1}{4}{A statictic analysis of deep learning-based LLIE methods, including learning strategy, network characteristic, Retinex model, data format, loss function, training dataset, testing dataset, and evaluation metric. Better to see with zoom. \relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Combination of Deep Model and Retinex Theory}{4}{figure.caption.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Data Format}{4}{figure.caption.2}\protected@file@percent }
\citation{wei2018deep}
\citation{cai2018learning}
\citation{jiang2019learning}
\citation{bychkovsky2011learning}
\citation{wei2018deep}
\citation{chen2019seeing}
\citation{jiang2019learning}
\citation{guo2016lime}
\citation{wang2013naturalness}
\citation{lee2011power}
\citation{lee2013contrast}
\citation{yu2020bdd100k}
\citation{loh2019getting}
\citation{yuan2019ug}
\citation{jiang2019learning}
\@writefile{toc}{\contentsline {paragraph}{Loss Function}{5}{figure.caption.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Training Datasets}{5}{figure.caption.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces  Summary of paired training datasets. 'Syn' represents Synthetic.\relax }}{5}{table.caption.3}\protected@file@percent }
\newlabel{tab: Paired_training_datases}{{2}{5}{Summary of paired training datasets. 'Syn' represents Synthetic.\relax }{table.caption.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Testing Dataset}{5}{table.caption.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces  Summary of testing datasets.\relax }}{5}{table.caption.4}\protected@file@percent }
\newlabel{tab: Testing datasets}{{3}{5}{Summary of testing datasets.\relax }{table.caption.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Benchmarking and Empirical Analysis}{5}{table.caption.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{A New Low-light Image and Video Dataset}{6}{table.caption.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces  Summary of LoLi-Phone dataset. LoLi-Phone dataset contains 120 videos (55,148 images) taken by 18 different mobile phones' cameras. "\#Video" and "\#Image" represent the number of videos and images, respectively.\relax }}{6}{table.caption.5}\protected@file@percent }
\newlabel{tab: LoLi-Phone dataset}{{4}{6}{Summary of LoLi-Phone dataset. LoLi-Phone dataset contains 120 videos (55,148 images) taken by 18 different mobile phones' cameras. "\#Video" and "\#Image" represent the number of videos and images, respectively.\relax }{table.caption.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Online Evaluation Platform}{6}{figure.caption.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Benchmark Results}{6}{figure.caption.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces   Several images sampled from the proposed LoLiPhone dataset. The images and videos are taken by different devices under diverse lighting conditions and scenes. \relax }}{7}{figure.caption.6}\protected@file@percent }
\newlabel{fig:Sample_Loli}{{2}{7}{Several images sampled from the proposed LoLiPhone dataset. The images and videos are taken by different devices under diverse lighting conditions and scenes. \relax }{figure.caption.6}{}}
\newlabel{fig: LOL-test_a}{{3a}{8}{input\relax }{figure.caption.7}{}}
\newlabel{sub@fig: LOL-test_a}{{a}{8}{input\relax }{figure.caption.7}{}}
\newlabel{fig: LOL-test_b}{{3b}{8}{LLNet\relax }{figure.caption.7}{}}
\newlabel{sub@fig: LOL-test_b}{{b}{8}{LLNet\relax }{figure.caption.7}{}}
\newlabel{fig: LOL-test_c}{{3c}{8}{LightenNet\relax }{figure.caption.7}{}}
\newlabel{sub@fig: LOL-test_c}{{c}{8}{LightenNet\relax }{figure.caption.7}{}}
\newlabel{fig: LOL-test_d}{{3d}{8}{Retinex-Net\relax }{figure.caption.7}{}}
\newlabel{sub@fig: LOL-test_d}{{d}{8}{Retinex-Net\relax }{figure.caption.7}{}}
\newlabel{fig: LOL-test_e}{{3e}{8}{MBLLEN\relax }{figure.caption.7}{}}
\newlabel{sub@fig: LOL-test_e}{{e}{8}{MBLLEN\relax }{figure.caption.7}{}}
\newlabel{fig: LOL-test_f}{{3f}{8}{KinD\relax }{figure.caption.7}{}}
\newlabel{sub@fig: LOL-test_f}{{f}{8}{KinD\relax }{figure.caption.7}{}}
\newlabel{fig: LOL-test_g}{{3g}{8}{KinD++\relax }{figure.caption.7}{}}
\newlabel{sub@fig: LOL-test_g}{{g}{8}{KinD++\relax }{figure.caption.7}{}}
\newlabel{fig: LOL-test_h}{{3h}{8}{TBEFN\relax }{figure.caption.7}{}}
\newlabel{sub@fig: LOL-test_h}{{h}{8}{TBEFN\relax }{figure.caption.7}{}}
\newlabel{fig: LOL-test_i}{{3i}{8}{DSLR\relax }{figure.caption.7}{}}
\newlabel{sub@fig: LOL-test_i}{{i}{8}{DSLR\relax }{figure.caption.7}{}}
\newlabel{fig: LOL-test_j}{{3j}{8}{EnlightenGAN\relax }{figure.caption.7}{}}
\newlabel{sub@fig: LOL-test_j}{{j}{8}{EnlightenGAN\relax }{figure.caption.7}{}}
\newlabel{fig: LOL-test_k}{{3k}{8}{DRBN\relax }{figure.caption.7}{}}
\newlabel{sub@fig: LOL-test_k}{{k}{8}{DRBN\relax }{figure.caption.7}{}}
\newlabel{fig: LOL-test_l}{{3l}{8}{ExCNet\relax }{figure.caption.7}{}}
\newlabel{sub@fig: LOL-test_l}{{l}{8}{ExCNet\relax }{figure.caption.7}{}}
\newlabel{fig: LOL-test_m}{{3m}{8}{Zero-DCE\relax }{figure.caption.7}{}}
\newlabel{sub@fig: LOL-test_m}{{m}{8}{Zero-DCE\relax }{figure.caption.7}{}}
\newlabel{fig: LOL-test_n}{{3n}{8}{RRDNet\relax }{figure.caption.7}{}}
\newlabel{sub@fig: LOL-test_n}{{n}{8}{RRDNet\relax }{figure.caption.7}{}}
\newlabel{fig: LOL-test_o}{{3o}{8}{GT\relax }{figure.caption.7}{}}
\newlabel{sub@fig: LOL-test_o}{{o}{8}{GT\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces   Visual results of different methods on a low-light image sampled from LOL-test dataset. \relax }}{8}{figure.caption.7}\protected@file@percent }
\newlabel{fig: Visual Result from LOL-test dataset}{{3}{8}{Visual results of different methods on a low-light image sampled from LOL-test dataset. \relax }{figure.caption.7}{}}
\newlabel{fig: MIT-Adobe_FiveK_a}{{4a}{8}{input\relax }{figure.caption.8}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_a}{{a}{8}{input\relax }{figure.caption.8}{}}
\newlabel{fig: MIT-Adobe_FiveK_b}{{4b}{8}{LLNet\relax }{figure.caption.8}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_b}{{b}{8}{LLNet\relax }{figure.caption.8}{}}
\newlabel{fig: MIT-Adobe_FiveK_c}{{4c}{8}{LightenNet\relax }{figure.caption.8}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_c}{{c}{8}{LightenNet\relax }{figure.caption.8}{}}
\newlabel{fig: MIT-Adobe_FiveK_d}{{4d}{8}{Retinex-Net\relax }{figure.caption.8}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_d}{{d}{8}{Retinex-Net\relax }{figure.caption.8}{}}
\newlabel{fig: MIT-Adobe_FiveK_e}{{4e}{8}{MBLLEN\relax }{figure.caption.8}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_e}{{e}{8}{MBLLEN\relax }{figure.caption.8}{}}
\newlabel{fig: MIT-Adobe_FiveK_f}{{4f}{8}{KinD\relax }{figure.caption.8}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_f}{{f}{8}{KinD\relax }{figure.caption.8}{}}
\newlabel{fig: MIT-Adobe_FiveK_g}{{4g}{8}{KinD++\relax }{figure.caption.8}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_g}{{g}{8}{KinD++\relax }{figure.caption.8}{}}
\newlabel{fig: MIT-Adobe_FiveK_h}{{4h}{8}{TBEFN\relax }{figure.caption.8}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_h}{{h}{8}{TBEFN\relax }{figure.caption.8}{}}
\newlabel{fig: MIT-Adobe_FiveK_i}{{4i}{8}{DSLR\relax }{figure.caption.8}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_i}{{i}{8}{DSLR\relax }{figure.caption.8}{}}
\newlabel{fig: MIT-Adobe_FiveK_j}{{4j}{8}{EnlightenGAN\relax }{figure.caption.8}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_j}{{j}{8}{EnlightenGAN\relax }{figure.caption.8}{}}
\newlabel{fig: MIT-Adobe_FiveK_k}{{4k}{8}{DRBN\relax }{figure.caption.8}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_k}{{k}{8}{DRBN\relax }{figure.caption.8}{}}
\newlabel{fig: MIT-Adobe_FiveK_l}{{4l}{8}{ExCNet\relax }{figure.caption.8}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_l}{{l}{8}{ExCNet\relax }{figure.caption.8}{}}
\newlabel{fig: MIT-Adobe_FiveK_m}{{4m}{8}{Zero-DCE\relax }{figure.caption.8}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_m}{{m}{8}{Zero-DCE\relax }{figure.caption.8}{}}
\newlabel{fig: MIT-Adobe_FiveK_n}{{4n}{8}{RRDNet\relax }{figure.caption.8}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_n}{{n}{8}{RRDNet\relax }{figure.caption.8}{}}
\newlabel{fig: MIT-Adobe_FiveK_o}{{4o}{8}{GT\relax }{figure.caption.8}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_o}{{o}{8}{GT\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces   Visual results of different methods on a low-light image sampled from MIT-Adobe FiveK-test dataset. \relax }}{8}{figure.caption.8}\protected@file@percent }
\newlabel{fig: Visual Result from MIT-Adobe FiveK dataset}{{4}{8}{Visual results of different methods on a low-light image sampled from MIT-Adobe FiveK-test dataset. \relax }{figure.caption.8}{}}
\newlabel{fig: LoLi-Phone-imgT_a}{{5a}{9}{\relax }{figure.caption.9}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_a}{{a}{9}{\relax }{figure.caption.9}{}}
\newlabel{fig: LoLi-Phone-imgT_b}{{5b}{9}{\relax }{figure.caption.9}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_b}{{b}{9}{\relax }{figure.caption.9}{}}
\newlabel{fig: LoLi-Phone-imgT_c}{{5c}{9}{\relax }{figure.caption.9}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_c}{{c}{9}{\relax }{figure.caption.9}{}}
\newlabel{fig: LoLi-Phone-imgT_d}{{5d}{9}{\relax }{figure.caption.9}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_d}{{d}{9}{\relax }{figure.caption.9}{}}
\newlabel{fig: LoLi-Phone-imgT_e}{{5e}{9}{\relax }{figure.caption.9}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_e}{{e}{9}{\relax }{figure.caption.9}{}}
\newlabel{fig: LoLi-Phone-imgT_f}{{5f}{9}{\relax }{figure.caption.9}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_f}{{f}{9}{\relax }{figure.caption.9}{}}
\newlabel{fig: LoLi-Phone-imgT_g}{{5g}{9}{\relax }{figure.caption.9}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_g}{{g}{9}{\relax }{figure.caption.9}{}}
\newlabel{fig: LoLi-Phone-imgT_h}{{5h}{9}{\relax }{figure.caption.9}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_h}{{h}{9}{\relax }{figure.caption.9}{}}
\newlabel{fig: LoLi-Phone-imgT_i}{{5i}{9}{\relax }{figure.caption.9}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_i}{{i}{9}{\relax }{figure.caption.9}{}}
\newlabel{fig: LoLi-Phone-imgT_j}{{5j}{9}{\relax }{figure.caption.9}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_j}{{j}{9}{\relax }{figure.caption.9}{}}
\newlabel{fig: LoLi-Phone-imgT_k}{{5k}{9}{\relax }{figure.caption.9}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_k}{{k}{9}{\relax }{figure.caption.9}{}}
\newlabel{fig: LoLi-Phone-imgT_l}{{5l}{9}{\relax }{figure.caption.9}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_l}{{l}{9}{\relax }{figure.caption.9}{}}
\newlabel{fig: LoLi-Phone-imgT_m}{{5m}{9}{\relax }{figure.caption.9}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_m}{{m}{9}{\relax }{figure.caption.9}{}}
\newlabel{fig: LoLi-Phone-imgT_n}{{5n}{9}{\relax }{figure.caption.9}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_n}{{n}{9}{\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces   Visual results of different methods on a low-light image sampled from LoLi-Phone-imgT dataset. (a) input (b) LLNet (c) LightenNet (d) Retinex-Net (e) MBLLEN (f) KinD (g) KinD++ (h) TBEFN (i) DSLR (j) EnlightenGAN (k) DRBN (l) ExCNet (m) Zero-DCE (n) RRDNet \relax }}{9}{figure.caption.9}\protected@file@percent }
\newlabel{fig: Visual Result from LoLi-Phone-imgT dataset}{{5}{9}{Visual results of different methods on a low-light image sampled from LoLi-Phone-imgT dataset. (a) input (b) LLNet (c) LightenNet (d) Retinex-Net (e) MBLLEN (f) KinD (g) KinD++ (h) TBEFN (i) DSLR (j) EnlightenGAN (k) DRBN (l) ExCNet (m) Zero-DCE (n) RRDNet \relax }{figure.caption.9}{}}
\newlabel{fig: LoLi-Phone-imgT_1_a}{{6a}{9}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_a}{{a}{9}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_1_b}{{6b}{9}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_b}{{b}{9}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_1_c}{{6c}{9}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_c}{{c}{9}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_1_d}{{6d}{9}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_d}{{d}{9}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_1_e}{{6e}{9}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_e}{{e}{9}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_1_f}{{6f}{9}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_f}{{f}{9}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_1_g}{{6g}{9}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_g}{{g}{9}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_1_h}{{6h}{9}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_h}{{h}{9}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_1_i}{{6i}{9}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_i}{{i}{9}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_1_j}{{6j}{9}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_j}{{j}{9}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_1_k}{{6k}{9}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_k}{{k}{9}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_1_l}{{6l}{9}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_l}{{l}{9}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_1_m}{{6m}{9}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_m}{{m}{9}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_1_n}{{6n}{9}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_n}{{n}{9}{\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces   Visual results of different methods on a low-light image sampled from LoLi-Phone-imgT dataset. (a) input (b) LLNet (c) LightenNet (d) Retinex-Net (e) MBLLEN (f) KinD (g) KinD++ (h) TBEFN (i) DSLR (j) EnlightenGAN (k) DRBN (l) ExCNet (m) Zero-DCE (n) RRDNet \relax }}{9}{figure.caption.10}\protected@file@percent }
\newlabel{fig: Visual Result from LoLi-Phone-imgT_1 dataset}{{6}{9}{Visual results of different methods on a low-light image sampled from LoLi-Phone-imgT dataset. (a) input (b) LLNet (c) LightenNet (d) Retinex-Net (e) MBLLEN (f) KinD (g) KinD++ (h) TBEFN (i) DSLR (j) EnlightenGAN (k) DRBN (l) ExCNet (m) Zero-DCE (n) RRDNet \relax }{figure.caption.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces  Quantitative comparisons on LOL-test and MIT-Adobe FiveK-test testing datasets in terms of MSE (×103), PSNR (in dB), SSIM, and LPIPS. The best result is in red whereas the second and third best results are in blue and purple under each case, respectively.\relax }}{10}{table.caption.11}\protected@file@percent }
\newlabel{tab: Quantitative Comparisons on LOL-test and MIT-Adobe FiveK-test testing datasets}{{5}{10}{Quantitative comparisons on LOL-test and MIT-Adobe FiveK-test testing datasets in terms of MSE (×103), PSNR (in dB), SSIM, and LPIPS. The best result is in red whereas the second and third best results are in blue and purple under each case, respectively.\relax }{table.caption.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces  Quantitative comparisons on LoLi-Phone-imgT dataset in terms of NIQE, LOE, PI, and SPAQ. The best result is in red whereas the second and third best results are in blue and purple under each case, respectively.\relax }}{10}{table.caption.12}\protected@file@percent }
\newlabel{tab: Quantitative comparisons on LoLi-Phone-imgT dataset}{{6}{10}{Quantitative comparisons on LoLi-Phone-imgT dataset in terms of NIQE, LOE, PI, and SPAQ. The best result is in red whereas the second and third best results are in blue and purple under each case, respectively.\relax }{table.caption.12}{}}
\@writefile{toc}{\contentsline {paragraph}{Computational Complexity}{11}{table.caption.12}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces  Quantitative comparisons of computational complexity in terms of runtime (in second), number of trainable parameters (\#Parameters) (in M), and FLOPs (in G). The best result is in \textcolor {red}{red} whereas the second and third best results are in \textcolor {blue}{blue} and \textcolor {purple}{purple} under each case, respectively. ‘-’ indicates the result is not available.\relax }}{11}{table.caption.13}\protected@file@percent }
\newlabel{tab: Quantitative comparisons of computational complexity}{{7}{11}{Quantitative comparisons of computational complexity in terms of runtime (in second), number of trainable parameters (\#Parameters) (in M), and FLOPs (in G). The best result is in \textcolor {red}{red} whereas the second and third best results are in \textcolor {blue}{blue} and \textcolor {purple}{purple} under each case, respectively. ‘-’ indicates the result is not available.\relax }{table.caption.13}{}}
\@writefile{toc}{\contentsline {paragraph}{Application-based Evaluation}{11}{table.caption.13}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Discussion}{11}{figure.caption.15}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces   The P-R curves of face detection in the dark. \relax }}{12}{figure.caption.14}\protected@file@percent }
\newlabel{fig: P-R_curves}{{7}{12}{The P-R curves of face detection in the dark. \relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {paragraph}{Future Research Directions}{12}{figure.caption.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Effective Learning Strategies}{12}{figure.caption.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Specialized Network Structures}{12}{figure.caption.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Loss Function}{12}{figure.caption.15}\protected@file@percent }
\newlabel{fig: DARK_FACE_a}{{8a}{13}{input\relax }{figure.caption.15}{}}
\newlabel{sub@fig: DARK_FACE_a}{{a}{13}{input\relax }{figure.caption.15}{}}
\newlabel{fig: DARK_FACE_b}{{8b}{13}{LightenNet\relax }{figure.caption.15}{}}
\newlabel{sub@fig: DARK_FACE_b}{{b}{13}{LightenNet\relax }{figure.caption.15}{}}
\newlabel{fig: DARK_FACE_c}{{8c}{13}{Retinex-Net\relax }{figure.caption.15}{}}
\newlabel{sub@fig: DARK_FACE_c}{{c}{13}{Retinex-Net\relax }{figure.caption.15}{}}
\newlabel{fig: DARK_FACE_d}{{8d}{13}{MBLLEN\relax }{figure.caption.15}{}}
\newlabel{sub@fig: DARK_FACE_d}{{d}{13}{MBLLEN\relax }{figure.caption.15}{}}
\newlabel{fig: DARK_FACE_e}{{8e}{13}{KinD++\relax }{figure.caption.15}{}}
\newlabel{sub@fig: DARK_FACE_e}{{e}{13}{KinD++\relax }{figure.caption.15}{}}
\newlabel{fig: DARK_FACE_f}{{8f}{13}{TBEFN\relax }{figure.caption.15}{}}
\newlabel{sub@fig: DARK_FACE_f}{{f}{13}{TBEFN\relax }{figure.caption.15}{}}
\newlabel{fig: DARK_FACE_g}{{8g}{13}{DSLR\relax }{figure.caption.15}{}}
\newlabel{sub@fig: DARK_FACE_g}{{g}{13}{DSLR\relax }{figure.caption.15}{}}
\newlabel{fig: DARK_FACE_h}{{8h}{13}{EnlightenGAN\relax }{figure.caption.15}{}}
\newlabel{sub@fig: DARK_FACE_h}{{h}{13}{EnlightenGAN\relax }{figure.caption.15}{}}
\newlabel{fig:DARK_FACE_i}{{8i}{13}{DRBN\relax }{figure.caption.15}{}}
\newlabel{sub@fig:DARK_FACE_i}{{i}{13}{DRBN\relax }{figure.caption.15}{}}
\newlabel{fig: DARK_FACE_j}{{8j}{13}{ExCNet\relax }{figure.caption.15}{}}
\newlabel{sub@fig: DARK_FACE_j}{{j}{13}{ExCNet\relax }{figure.caption.15}{}}
\newlabel{fig: DARK_FACE_k}{{8k}{13}{Zero-DCE\relax }{figure.caption.15}{}}
\newlabel{sub@fig: DARK_FACE_k}{{k}{13}{Zero-DCE\relax }{figure.caption.15}{}}
\newlabel{fig: DARK_FACE_l}{{8l}{13}{RRDNet\relax }{figure.caption.15}{}}
\newlabel{sub@fig: DARK_FACE_l}{{l}{13}{RRDNet\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces   Visual results of different methods on a low-light image sampled from DARK FACE dataset. Better see with zoom in for the bounding boxes of faces. \relax }}{13}{figure.caption.15}\protected@file@percent }
\newlabel{fig: Visual Result from DARK_FACE dataset}{{8}{13}{Visual results of different methods on a low-light image sampled from DARK FACE dataset. Better see with zoom in for the bounding boxes of faces. \relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subparagraph}{Realistic Training Data}{13}{figure.caption.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Standard Testing Data}{13}{figure.caption.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Task-Specific Evaluation Metrics}{13}{figure.caption.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Robust Generalization Capability}{13}{figure.caption.15}\protected@file@percent }
\citation{howard2017mobilenets}
\citation{liu2020improving}
\citation{liu2018progressive}
\citation{liu2019auto}
\citation{dosovitskiy2020image}
\citation{chen2021pre}
\@writefile{toc}{\contentsline {subparagraph}{Extension to Low-light Video Enhancement}{14}{figure.caption.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Integrating Semantic Information}{14}{figure.caption.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.4}项目链接}{14}{subsubsection.1.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}个人工作进展}{14}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}按照paper脉络梳理暗弱光技术}{14}{subsection.2.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces  We have collected a list of resources related to low light image enhancement, including methods/codes/papers, etc. We hope this can help develop new methods and solutions for low light tasks.\relax }}{15}{table.caption.16}\protected@file@percent }
\newlabel{tab: Method/code/paper related to LLIE}{{8}{15}{We have collected a list of resources related to low light image enhancement, including methods/codes/papers, etc. We hope this can help develop new methods and solutions for low light tasks.\relax }{table.caption.16}{}}
\citation{fang2020perceptual}
\citation{talebi2018nima}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces  Datasets associated with the low-light image enhancement.\relax }}{17}{table.caption.17}\protected@file@percent }
\newlabel{tab: Datasets}{{9}{17}{Datasets associated with the low-light image enhancement.\relax }{table.caption.17}{}}
\citation{fang2020perceptual}
\citation{talebi2018nima}
\bibstyle{unsrt}
\bibdata{reference}
\bibcite{wei2018deep}{1}
\bibcite{cai2018learning}{2}
\@writefile{toc}{\contentsline {section}{\numberline {3}下周工作计划}{18}{section.3}\protected@file@percent }
\bibcite{jiang2019learning}{3}
\bibcite{bychkovsky2011learning}{4}
\bibcite{chen2019seeing}{5}
\bibcite{guo2016lime}{6}
\bibcite{wang2013naturalness}{7}
\bibcite{lee2011power}{8}
\bibcite{lee2013contrast}{9}
\bibcite{yu2020bdd100k}{10}
\bibcite{loh2019getting}{11}
\bibcite{yuan2019ug}{12}
\bibcite{howard2017mobilenets}{13}
\bibcite{liu2020improving}{14}
\bibcite{liu2018progressive}{15}
\bibcite{liu2019auto}{16}
\bibcite{dosovitskiy2020image}{17}
\bibcite{chen2021pre}{18}
\bibcite{fang2020perceptual}{19}
\bibcite{talebi2018nima}{20}
\gdef \@abspage@last{20}
