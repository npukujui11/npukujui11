\contentsline {section}{\numberline {1}Pre-Knowledge}{1}{section.1}%
\contentsline {subsection}{\numberline {1.1}Transformer}{1}{subsection.1.1}%
\contentsline {subsubsection}{\numberline {1.1.1}Idea}{2}{subsubsection.1.1.1}%
\contentsline {subsubsection}{\numberline {1.1.2}Architecture}{2}{subsubsection.1.1.2}%
\contentsline {paragraph}{Encoder}{2}{figure.caption.1}%
\contentsline {paragraph}{Decoder}{2}{figure.caption.1}%
\contentsline {subsubsection}{\numberline {1.1.3}Attention Mechanism}{4}{subsubsection.1.1.3}%
\contentsline {paragraph}{Scaled dot-product attention}{4}{subsubsection.1.1.3}%
\contentsline {paragraph}{Multi-head attention}{5}{equation.1.2}%
\contentsline {subsubsection}{\numberline {1.1.4}Layer normalization}{5}{subsubsection.1.1.4}%
\contentsline {paragraph}{Batch normalization}{5}{equation.1.5}%
\contentsline {paragraph}{Layer normalization}{7}{equation.1.8}%
\contentsline {section}{\numberline {2}Paper reading}{8}{section.2}%
\contentsline {subsection}{\numberline {2.1}Ultra-High-Definition Low-Light Image Enhancement}{8}{subsection.2.1}%
\contentsline {subsubsection}{\numberline {2.1.1}Introduce}{8}{subsubsection.2.1.1}%
\contentsline {subsubsection}{\numberline {2.1.2}Innovation}{8}{subsubsection.2.1.2}%
\contentsline {paragraph}{Axis-based Transformer Block}{9}{subsubsection.2.1.2}%
\contentsline {paragraph}{Dual Gated Feed-forward Network(DGFN)}{9}{subsubsection.2.1.2}%
\contentsline {paragraph}{Cross-layer Attention Fusion Block}{10}{equation.2.11}%
\contentsline {subsubsection}{\numberline {2.1.3}Result}{10}{subsubsection.2.1.3}%
\contentsline {section}{\numberline {3}个人工作进展}{10}{section.3}%
\contentsline {subsection}{\numberline {3.1}思考}{10}{subsection.3.1}%
\contentsline {section}{\numberline {4}下周工作计划}{12}{section.4}%
