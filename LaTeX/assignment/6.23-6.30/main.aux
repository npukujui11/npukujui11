\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Pre-Knowledge}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Transformer}{1}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}Idea}{2}{subsubsection.1.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}Architecture}{2}{subsubsection.1.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces   The Transformer - model architecture. \relax }}{3}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig: Transformer}{{1}{3}{The Transformer - model architecture. \relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Encoder}{4}{figure.caption.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Decoder}{4}{figure.caption.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.3}Attention Mechanism}{4}{subsubsection.1.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Scaled dot-product attention}{4}{subsubsection.1.1.3}\protected@file@percent }
\newlabel{eq: multiplicative attention}{{1}{4}{Scaled dot-product attention}{equation.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces   (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel. \relax }}{5}{figure.caption.2}\protected@file@percent }
\newlabel{fig: Transformer_attention}{{2}{5}{(left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel. \relax }{figure.caption.2}{}}
\newlabel{eq: Transformer_attention}{{2}{5}{Scaled dot-product attention}{equation.1.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Multi-head attention}{6}{equation.1.2}\protected@file@percent }
\newlabel{eq: Multi-head attention}{{3}{6}{Multi-head attention}{equation.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Paper reading}{6}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Ultra-High-Definition Low-Light Image Enhancement}{6}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Introduce}{6}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Innovation}{6}{subsubsection.2.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces   \textbf  {LLFormer architecture}. The core design of LLFormer includes an \colorbox {yellow}{axis-based transformer block} and a \colorbox {yellow}{cross-layer attention fusion block}. In the former, \colorbox {yellow}{axis-based multi-head self-attention} performs self-attention on the height and width axis across the channel dimension sequentially to reduce the computational complexity, and a \colorbox {yellow}{dual gated feed-forward network} employs a gated mechanism to focus more on useful features. The cross-layer attention fusion block learns the attention weights of features in different layers when fusing them. \relax }}{7}{figure.caption.3}\protected@file@percent }
\newlabel{fig: LLFormer architecture}{{3}{7}{\textbf {LLFormer architecture}. The core design of LLFormer includes an \colorbox {yellow}{axis-based transformer block} and a \colorbox {yellow}{cross-layer attention fusion block}. In the former, \colorbox {yellow}{axis-based multi-head self-attention} performs self-attention on the height and width axis across the channel dimension sequentially to reduce the computational complexity, and a \colorbox {yellow}{dual gated feed-forward network} employs a gated mechanism to focus more on useful features. The cross-layer attention fusion block learns the attention weights of features in different layers when fusing them. \relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Axis-based Transformer Block}{7}{subsubsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Dual Gated Feed-forward Network(DGFN)}{7}{subsubsection.2.1.2}\protected@file@percent }
\newlabel{eq: FFN}{{4}{7}{Dual Gated Feed-forward Network(DGFN)}{equation.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces   The architecture of our Axis-based Multi-head Self-Attention and Dual Gated Feed-Forward Network. From left to right, the components are Height Axis Multi-head Attention, Width Axis Multi-head Attention, and Dual Gated Feed-Forward Network. \relax }}{8}{figure.caption.4}\protected@file@percent }
\newlabel{fig: Axis-based Transformer Block & Dual Gated Feed-forward Network(GDFN)}{{4}{8}{The architecture of our Axis-based Multi-head Self-Attention and Dual Gated Feed-Forward Network. From left to right, the components are Height Axis Multi-head Attention, Width Axis Multi-head Attention, and Dual Gated Feed-Forward Network. \relax }{figure.caption.4}{}}
\newlabel{eq: DGFN}{{5}{8}{Dual Gated Feed-forward Network(DGFN)}{equation.2.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Cross-layer Attention Fusion Block}{8}{equation.2.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces   The architecture of the proposed Cross-layer Attention Fusion Block. This block efficiently integrates features from different layers with a layer correlation attention matrix. \relax }}{8}{figure.caption.5}\protected@file@percent }
\newlabel{fig: Cross-layer_Attention_Fusion_Block}{{5}{8}{The architecture of the proposed Cross-layer Attention Fusion Block. This block efficiently integrates features from different layers with a layer correlation attention matrix. \relax }{figure.caption.5}{}}
\bibstyle{unsrt}
\bibdata{reference}
\newlabel{eq: DGFN}{{6}{9}{Cross-layer Attention Fusion Block}{equation.2.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Result}{9}{subsubsection.2.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces   Benchmarking study on the UHD-LOL4K and UHD-LOL8K subsets. $\dag  $; $\ddag  $; $\S  $;$\triangle $ and ? indicate the traditional methods, supervised CNN-based methods, unsupervised CNN-based methods, zero-shot methods and transformer-based methods. The top three results are marked in red, blue and purple, respectively. Input \relax }}{9}{figure.caption.6}\protected@file@percent }
\newlabel{fig: result_in_UHD-LOL}{{6}{9}{Benchmarking study on the UHD-LOL4K and UHD-LOL8K subsets. $\dag $; $\ddag $; $\S $;$\triangle $ and ? indicate the traditional methods, supervised CNN-based methods, unsupervised CNN-based methods, zero-shot methods and transformer-based methods. The top three results are marked in red, blue and purple, respectively. Input \relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}个人工作进展}{9}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}思考}{9}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}下周工作计划}{9}{section.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces   Comparison results on LOL and MIT-Adobe FiveK datasets in terms of PSNR, SSIM, LPIPS and MAE. The top three results are marked in red, blue and purple, respectively. Same as (Zamir et al. 2020), we consider images from expert C for the MIT-Adobe FiveK dataset. \relax }}{10}{figure.caption.7}\protected@file@percent }
\newlabel{fig: result_in_LOL_and_MIT}{{7}{10}{Comparison results on LOL and MIT-Adobe FiveK datasets in terms of PSNR, SSIM, LPIPS and MAE. The top three results are marked in red, blue and purple, respectively. Same as (Zamir et al. 2020), we consider images from expert C for the MIT-Adobe FiveK dataset. \relax }{figure.caption.7}{}}
\gdef \@abspage@last{10}
