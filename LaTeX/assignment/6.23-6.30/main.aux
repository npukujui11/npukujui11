\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Pre-Knowledge}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Transformer}{1}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}Idea}{2}{subsubsection.1.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}Architecture}{2}{subsubsection.1.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Encoder}{2}{figure.caption.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Decoder}{2}{figure.caption.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces   The Transformer - model architecture. \relax }}{3}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig: Transformer}{{1}{3}{The Transformer - model architecture. \relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.3}Attention Mechanism}{4}{subsubsection.1.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Scaled dot-product attention}{4}{subsubsection.1.1.3}\protected@file@percent }
\newlabel{eq: multiplicative attention}{{1}{4}{Scaled dot-product attention}{equation.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces   (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel. \relax }}{4}{figure.caption.2}\protected@file@percent }
\newlabel{fig: Transformer_attention}{{2}{4}{(left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel. \relax }{figure.caption.2}{}}
\newlabel{eq: Transformer_attention}{{2}{4}{Scaled dot-product attention}{equation.1.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Multi-head attention}{5}{equation.1.2}\protected@file@percent }
\newlabel{eq: Multi-head attention}{{3}{5}{Multi-head attention}{equation.1.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.4}Layer normalization}{5}{subsubsection.1.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Batch normalization}{5}{equation.1.5}\protected@file@percent }
\newlabel{eq: normalized_1}{{4}{5}{}{equation.1.4}{}}
\newlabel{eq: normalized_2}{{5}{5}{}{equation.1.5}{}}
\newlabel{eq: Batch normalization}{{6}{6}{Batch normalization}{equation.1.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces   For a given layer, the mean during BN will be 1X. Each training data gets this mean subtracted from it and divided by $\sqrt  {(var + \epsilon )}$ and then shifted and scaled. To find the mean and var, we use all the examples in the training batch. \relax }}{6}{figure.caption.3}\protected@file@percent }
\newlabel{fig: Batch_normlization}{{3}{6}{For a given layer, the mean during BN will be 1X. Each training data gets this mean subtracted from it and divided by $\sqrt {(var + \epsilon )}$ and then shifted and scaled. To find the mean and var, we use all the examples in the training batch. \relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces   A conventional feedforward neural network. \relax }}{6}{figure.caption.4}\protected@file@percent }
\newlabel{fig: Neural Network}{{4}{6}{A conventional feedforward neural network. \relax }{figure.caption.4}{}}
\newlabel{eq: cell output}{{7}{7}{Batch normalization}{equation.1.7}{}}
\newlabel{eq: cell output(BN)}{{8}{7}{Batch normalization}{equation.1.8}{}}
\@writefile{toc}{\contentsline {paragraph}{Layer normalization}{7}{equation.1.8}\protected@file@percent }
\newlabel{eq: Layer normalization}{{9}{7}{Layer normalization}{equation.1.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.5}Mask}{7}{subsubsection.1.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces   For a given layer, the mean during LN will be NX1. We compute the mean and var for every single sample for each layer independently and then do the LN operations using those computed values. \relax }}{8}{figure.caption.5}\protected@file@percent }
\newlabel{fig: Layer_normlization}{{5}{8}{For a given layer, the mean during LN will be NX1. We compute the mean and var for every single sample for each layer independently and then do the LN operations using those computed values. \relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Padding mask}{8}{subsubsection.1.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Sequence mask}{8}{subsubsection.1.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.6}Positional encoding}{8}{subsubsection.1.1.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  Sequence mask. \relax }}{9}{figure.caption.6}\protected@file@percent }
\newlabel{fig: subsequent mask}{{6}{9}{Sequence mask. \relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Paper reading}{9}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Ultra-High-Definition Low-Light Image Enhancement}{9}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Introduce}{9}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Innovation}{9}{subsubsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Axis-based Transformer Block}{9}{subsubsection.2.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces   \textbf  {LLFormer architecture}. The core design of LLFormer includes an \colorbox {yellow}{axis-based transformer block} and a \colorbox {yellow}{cross-layer attention fusion block}. In the former, \colorbox {yellow}{axis-based multi-head self-attention} performs self-attention on the height and width axis across the channel dimension sequentially to reduce the computational complexity, and a \colorbox {yellow}{dual gated feed-forward network} employs a gated mechanism to focus more on useful features. The cross-layer attention fusion block learns the attention weights of features in different layers when fusing them. \relax }}{10}{figure.caption.7}\protected@file@percent }
\newlabel{fig: LLFormer architecture}{{7}{10}{\textbf {LLFormer architecture}. The core design of LLFormer includes an \colorbox {yellow}{axis-based transformer block} and a \colorbox {yellow}{cross-layer attention fusion block}. In the former, \colorbox {yellow}{axis-based multi-head self-attention} performs self-attention on the height and width axis across the channel dimension sequentially to reduce the computational complexity, and a \colorbox {yellow}{dual gated feed-forward network} employs a gated mechanism to focus more on useful features. The cross-layer attention fusion block learns the attention weights of features in different layers when fusing them. \relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {paragraph}{Dual Gated Feed-forward Network(DGFN)}{10}{subsubsection.2.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces   The architecture of our Axis-based Multi-head Self-Attention and Dual Gated Feed-Forward Network. From left to right, the components are Height Axis Multi-head Attention, Width Axis Multi-head Attention, and Dual Gated Feed-Forward Network. \relax }}{10}{figure.caption.8}\protected@file@percent }
\newlabel{fig: Axis-based Transformer Block & Dual Gated Feed-forward Network(GDFN)}{{8}{10}{The architecture of our Axis-based Multi-head Self-Attention and Dual Gated Feed-Forward Network. From left to right, the components are Height Axis Multi-head Attention, Width Axis Multi-head Attention, and Dual Gated Feed-Forward Network. \relax }{figure.caption.8}{}}
\newlabel{eq: FFN}{{10}{10}{Dual Gated Feed-forward Network(DGFN)}{equation.2.10}{}}
\newlabel{eq: DGFN}{{11}{10}{Dual Gated Feed-forward Network(DGFN)}{equation.2.11}{}}
\@writefile{toc}{\contentsline {paragraph}{Cross-layer Attention Fusion Block}{11}{equation.2.11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces   The architecture of the proposed Cross-layer Attention Fusion Block. This block efficiently integrates features from different layers with a layer correlation attention matrix. \relax }}{11}{figure.caption.9}\protected@file@percent }
\newlabel{fig: Cross-layer_Attention_Fusion_Block}{{9}{11}{The architecture of the proposed Cross-layer Attention Fusion Block. This block efficiently integrates features from different layers with a layer correlation attention matrix. \relax }{figure.caption.9}{}}
\newlabel{eq: DGFN}{{12}{11}{Cross-layer Attention Fusion Block}{equation.2.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Result}{11}{subsubsection.2.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces   Benchmarking study on the UHD-LOL4K and UHD-LOL8K subsets. $\dag  $; $\ddag  $; $\S  $;$\triangle $ and ? indicate the traditional methods, supervised CNN-based methods, unsupervised CNN-based methods, zero-shot methods and transformer-based methods. The top three results are marked in red, blue and purple, respectively. Input \relax }}{11}{figure.caption.10}\protected@file@percent }
\newlabel{fig: result_in_UHD-LOL}{{10}{11}{Benchmarking study on the UHD-LOL4K and UHD-LOL8K subsets. $\dag $; $\ddag $; $\S $;$\triangle $ and ? indicate the traditional methods, supervised CNN-based methods, unsupervised CNN-based methods, zero-shot methods and transformer-based methods. The top three results are marked in red, blue and purple, respectively. Input \relax }{figure.caption.10}{}}
\bibstyle{unsrt}
\bibdata{reference}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces   Comparison results on LOL and MIT-Adobe FiveK datasets in terms of PSNR, SSIM, LPIPS and MAE. The top three results are marked in red, blue and purple, respectively. Same as (Zamir et al. 2020), we consider images from expert C for the MIT-Adobe FiveK dataset. \relax }}{12}{figure.caption.11}\protected@file@percent }
\newlabel{fig: result_in_LOL_and_MIT}{{11}{12}{Comparison results on LOL and MIT-Adobe FiveK datasets in terms of PSNR, SSIM, LPIPS and MAE. The top three results are marked in red, blue and purple, respectively. Same as (Zamir et al. 2020), we consider images from expert C for the MIT-Adobe FiveK dataset. \relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}个人工作进展}{12}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}思考}{12}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}下周工作计划}{12}{section.4}\protected@file@percent }
\gdef \@abspage@last{12}
