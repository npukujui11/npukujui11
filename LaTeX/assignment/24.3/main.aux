\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/D>>}
\citation{ji1994adaptive}
\citation{land1965,land1977retinex,jobson1997properties}
\citation{yang2021locally,zhang2020attention}
\citation{jain1991unsupervised,lowe2004distinctive,ojala2002multiresolution}
\citation{dinh20231m}
\citation{cao2022swin}
\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Pre-Knowledge}{3}{part.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1}引言}{3}{section.1}\protected@file@percent }
\citation{wei2018deep}
\citation{cai2018learning}
\citation{jiang2019learning}
\@writefile{toc}{\contentsline {section}{\numberline {2}实验计划}{4}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.0.1}Dataset}{4}{subsubsection.2.0.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces  Summary of paired training datasets. 'Syn' represents Synthetic.\relax }}{4}{table.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab: Paired_training_datases}{{1}{4}{Summary of paired training datasets. 'Syn' represents Synthetic.\relax }{table.caption.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.0.2}Train}{4}{subsubsection.2.0.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.0.3}Performance Evaluation}{4}{subsubsection.2.0.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.0.4}Loss Function}{4}{subsubsection.2.0.4}\protected@file@percent }
\newlabel{eq: huber loss}{{1}{4}{Loss Function}{equation.2.1}{}}
\newlabel{eq: perceptual loss}{{2}{4}{Loss Function}{equation.2.2}{}}
\newlabel{eq: revised_SSIM loss}{{3}{5}{Loss Function}{equation.2.3}{}}
\@writefile{toc}{\contentsline {part}{II\hspace  {1em}Paper Reading}{5}{part.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Lightweight Model}{5}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}(2023.8)1M parameters are enough? A lightweight CNN-based model for medical image segmentation}{5}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1M的参数就足够了吗？一种基于 CNN 的轻量级医学图像分割模型}{5}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(APSIPA ASC 2023 2区) doi: 10.1109/APSIPAASC58517.2023}{5}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Research Background}{5}{subsubsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Contribution}{5}{subsubsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Approach}{6}{subsubsection.3.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces   The proposed U-Lite architecture. \relax }}{6}{figure.caption.2}\protected@file@percent }
\newlabel{fig: U-Lite Architecture}{{1}{6}{The proposed U-Lite architecture. \relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Axial Depthwise Separable Convolution Module}{6}{figure.caption.3}\protected@file@percent }
\citation{liu2021swin}
\citation{dong2022cswin}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces   Architectures of (a) Vision Permutator, (b) ConvNext, and (c) Proposed Axial DW Convolution module. The proposed module is inspired by Vision Permutator’s and ConvNext's designs. \relax }}{7}{figure.caption.3}\protected@file@percent }
\newlabel{fig: Axial Depthwise Convolution module}{{2}{7}{Architectures of (a) Vision Permutator, (b) ConvNext, and (c) Proposed Axial DW Convolution module. The proposed module is inspired by Vision Permutator’s and ConvNext's designs. \relax }{figure.caption.3}{}}
\newlabel{eq: Axial Depthwise Convolution module}{{6}{8}{Axial Depthwise Separable Convolution Module}{equation.3.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces   The receptive field comparison between Convolution $3 \times 3$, Vision Permutator, and Axial convolution $7 \times 7$. Axial convolution $7 \times 7$ offers a large receptive field compared with Convolution $3 \times 3$ while using fewer computational parameters than Vision Permutator. \relax }}{8}{figure.caption.4}\protected@file@percent }
\newlabel{fig: Receptive Field}{{3}{8}{The receptive field comparison between Convolution $3 \times 3$, Vision Permutator, and Axial convolution $7 \times 7$. Axial convolution $7 \times 7$ offers a large receptive field compared with Convolution $3 \times 3$ while using fewer computational parameters than Vision Permutator. \relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Encoder Block and Decoder Block}{8}{figure.caption.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Bottleneck Block}{8}{figure.caption.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces   Encoder, decoder, and bottleneck blocks. Designed based on Depthwise Separable Convolution concept. Each block adopts one Batch Normalization layer and ends with a GELU activation function. \relax }}{9}{figure.caption.5}\protected@file@percent }
\newlabel{fig: Encoder and Decoder}{{4}{9}{Encoder, decoder, and bottleneck blocks. Designed based on Depthwise Separable Convolution concept. Each block adopts one Batch Normalization layer and ends with a GELU activation function. \relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}Future}{9}{subsubsection.3.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Edge Detection}{9}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}(2022.3)Survey of Image Edge Detection}{9}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{图像边缘检测综述}{9}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(Frontiers in Signal Processing 2区) doi: 10.3389/frsip.2022.826967}{9}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Research Background}{9}{subsubsection.4.1.1}\protected@file@percent }
\citation{bertasius2015high}
\citation{xie2015holistically}
\citation{liu2017richer}
\citation{deng2020deep}
\citation{su2021pixel}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Contribution}{10}{subsubsection.4.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces   Development of edge detection algorithms based on traditional and deep learning methods. \relax }}{10}{figure.caption.6}\protected@file@percent }
\newlabel{fig: Development}{{5}{10}{Development of edge detection algorithms based on traditional and deep learning methods. \relax }{figure.caption.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces  Performance comparison of conventional edge detection based algorithms on the BSDS500 dataset.\relax }}{10}{table.caption.7}\protected@file@percent }
\newlabel{tab: Algorithms on the BSDS500 dataset}{{2}{10}{Performance comparison of conventional edge detection based algorithms on the BSDS500 dataset.\relax }{table.caption.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces  Performance comparison of conventional edge detection based algorithms on the NYUD dataset.\relax }}{10}{table.caption.8}\protected@file@percent }
\newlabel{tab: Algorithms on the NYUD dataset}{{3}{10}{Performance comparison of conventional edge detection based algorithms on the NYUD dataset.\relax }{table.caption.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces  Performance comparison of deep learning-based edge detection algorithms on the BSDS500 dataset.\relax }}{10}{table.caption.9}\protected@file@percent }
\newlabel{tab: Deep learning on the BSDS500 dataset}{{4}{10}{Performance comparison of deep learning-based edge detection algorithms on the BSDS500 dataset.\relax }{table.caption.9}{}}
\citation{krizhevsky2012imagenet}
\citation{simonyan2014very}
\citation{he2016deep}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces  Performance comparison of deep learning-based edge detection algorithms on the NYUD dataset.\relax }}{11}{table.caption.10}\protected@file@percent }
\newlabel{tab: Deep learning on the NYUD dataset}{{5}{11}{Performance comparison of deep learning-based edge detection algorithms on the NYUD dataset.\relax }{table.caption.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Approach}{11}{subsubsection.4.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Evaluation Indicators}{11}{subsubsection.4.1.3}\protected@file@percent }
\newlabel{eq: F-Score}{{7}{11}{Evaluation Indicators}{equation.4.7}{}}
\newlabel{eq: F-Score1}{{8}{11}{Evaluation Indicators}{equation.4.8}{}}
\newlabel{eq: F-Score2}{{9}{11}{Evaluation Indicators}{equation.4.9}{}}
\newlabel{eq: F-Score3}{{10}{11}{Evaluation Indicators}{equation.4.10}{}}
\@writefile{toc}{\contentsline {paragraph}{Backbone Network}{11}{equation.4.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.4}Future}{12}{subsubsection.4.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}(2020)Dense Extreme Inception Network: Towards a Robust CNN Model for Edge Detection}{12}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{DeixNeD: 面向一个鲁棒的CNN边缘检测模型}{12}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(CVPR 2020) doi: 10.48550/arXiv.1909.01955}{12}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Research Background}{12}{subsubsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Contribution}{12}{subsubsection.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Approach}{12}{subsubsection.4.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces   Proposed architecture: Dense Extreme Inception Network, consists of an encoder composed by six main blocks (showed in light gray). The main blocks are connected between them through 1x1 convolutional blocks. Each of the main blocks is composed by sub-blocks that are densely interconnected by the output of the previous main block. The output from each of the main blocks is fed to an upsampling block that produces an intermediate edge-map in order to build a Scale Space Volume, which is used to compose a final fused edge-map. \relax }}{13}{figure.caption.11}\protected@file@percent }
\newlabel{fig: DeixNed Architecture}{{6}{13}{Proposed architecture: Dense Extreme Inception Network, consists of an encoder composed by six main blocks (showed in light gray). The main blocks are connected between them through 1x1 convolutional blocks. Each of the main blocks is composed by sub-blocks that are densely interconnected by the output of the previous main block. The output from each of the main blocks is fed to an upsampling block that produces an intermediate edge-map in order to build a Scale Space Volume, which is used to compose a final fused edge-map. \relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces   Detail of the upsampling block that receives as input the learned features extracted from each of the main blocks. The features are fed into a stack of learned convolutional and transposed convolutional filters in order to extract an intermediate edge-map. \relax }}{13}{figure.caption.12}\protected@file@percent }
\newlabel{fig: DeixNed Architecture Block}{{7}{13}{Detail of the upsampling block that receives as input the learned features extracted from each of the main blocks. The features are fed into a stack of learned convolutional and transposed convolutional filters in order to extract an intermediate edge-map. \relax }{figure.caption.12}{}}
\bibstyle{unsrt}
\bibdata{reference}
\bibcite{ji1994adaptive}{1}
\bibcite{land1965}{2}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4}Future}{14}{subsubsection.4.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}(2020.10) Deep Structural Contour Detection}{14}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{深结构轮廓检测}{14}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(ACM MM 2020) doi: 10.1145/3394171.3413750}{14}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Research Background}{14}{subsubsection.4.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Contribution}{14}{subsubsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Approach}{14}{subsubsection.4.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces   Illustration of the proposed network. The left and the right are the encoder and the decoder, respectively. We adopt the VGG16 as our encoder. The encoder extracts multi-scale, multi-level features and the decoder fuse the features and recover the feature resolution to the original. The middle rectangle is the proposed hyper convolutional module. It adopts three conv blocks and captures dense connection among the hierarchical features. The module can significantly improve model performance. To the best view, we omit the connections between the encoder features and the decoder features. \relax }}{14}{figure.caption.13}\protected@file@percent }
\newlabel{fig: DSCD proposed network}{{8}{14}{Illustration of the proposed network. The left and the right are the encoder and the decoder, respectively. We adopt the VGG16 as our encoder. The encoder extracts multi-scale, multi-level features and the decoder fuse the features and recover the feature resolution to the original. The middle rectangle is the proposed hyper convolutional module. It adopts three conv blocks and captures dense connection among the hierarchical features. The module can significantly improve model performance. To the best view, we omit the connections between the encoder features and the decoder features. \relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.4}Future}{14}{subsubsection.4.3.4}\protected@file@percent }
\bibcite{land1977retinex}{3}
\bibcite{jobson1997properties}{4}
\bibcite{yang2021locally}{5}
\bibcite{zhang2020attention}{6}
\bibcite{jain1991unsupervised}{7}
\bibcite{lowe2004distinctive}{8}
\bibcite{ojala2002multiresolution}{9}
\bibcite{dinh20231m}{10}
\bibcite{cao2022swin}{11}
\bibcite{wei2018deep}{12}
\bibcite{cai2018learning}{13}
\bibcite{jiang2019learning}{14}
\bibcite{liu2021swin}{15}
\bibcite{dong2022cswin}{16}
\bibcite{bertasius2015high}{17}
\bibcite{xie2015holistically}{18}
\bibcite{liu2017richer}{19}
\bibcite{deng2020deep}{20}
\bibcite{su2021pixel}{21}
\bibcite{krizhevsky2012imagenet}{22}
\bibcite{simonyan2014very}{23}
\bibcite{he2016deep}{24}
\newlabel{LastPage}{{}{16}{}{page.16}{}}
\xdef\lastpage@lastpage{16}
\xdef\lastpage@lastpageHy{16}
\gdef \@abspage@last{16}
