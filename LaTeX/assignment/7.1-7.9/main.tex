\documentclass[letterpaper,10pt]{article}

\usepackage{tabularx} % extra features for tabular environment
\usepackage{amsmath}  % improve math presentation
\usepackage{graphicx} % takes care of graphic including machinery
\usepackage[margin=1in,letterpaper]{geometry} % decreases margins
\usepackage{cite} % takes care of citations
\usepackage[final]{hyperref} % adds hyper links inside the generated pdf file
\usepackage{ctex}
\usepackage{titlesec}
%\usepackage{CJKutf8, CJK}
\usepackage{makecell}                 % 三线表-竖线
\usepackage{booktabs}                 % 三线表-短细横线
% \usepackage{natbib}
\usepackage{graphicx}				  % 表格单元格逆时针
\usepackage{multirow}				  % 合并单元格
\usepackage{array}
\usepackage{amssymb}				  % 勾
\usepackage{amsmath}
\usepackage{longtable}                % 导入 longtable 宏包，表格自动换行
\usepackage{caption}
\usepackage{subcaption}               % 设置子图
\usepackage{color}					  % 文本颜色包
\usepackage{xcolor}
\usepackage{bbm}					  % 输入指示函数
\usepackage{tablefootnote}			  % 表格注释
\usepackage{pythonhighlight}
\usepackage{fancyhdr}
\usepackage{lastpage}
\pagestyle{fancy}
\fancyhf{}
\fancyhead{}
\fancyfoot{}
\fancyhead[R]{\small Page \thepage\ of \pageref*{LastPage}}
\fancyhead[L]{\small Report}

\usepackage{listings}                 % 导入代码块
\usepackage{xcolor}
\lstset{
	numbers=left, 
	tabsize=1,
	columns=flexible, 
	numberstyle=  \small, 
	keywordstyle= \color{ blue!70},
	commentstyle= \color{red!50!green!50!blue!50}, 
	frame=shadowbox, % 阴影效果
	rulesepcolor= \color{ red!20!green!20!blue!20} ,
	escapeinside=``, % 英文分号中可写入中文
	xleftmargin=2em,
	xrightmargin=2em, 
	aboveskip=1em,
} 

\hypersetup{
	colorlinks=true,       % false: boxed links; true: colored links
	linkcolor=blue,        % color of internal links
	citecolor=blue,        % color of links to bibliography
	filecolor=magenta,     % color of file links
	urlcolor=blue         
}
%++++++++++++++++++++++++++++++++++++++++
\titleformat{\section}{\Large\bfseries\songti}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries\songti}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalsize\bfseries\songti}{\thesubsubsection}{1em}{}
\titleformat{\paragraph}{\small\bfseries\songti}{\paragraph}{1em}{}
\titleformat{\subparagraph}{\footnotesize\bfseries\songti}{\subparagraph}{1em}{}

\begin{document}
	
	
	\title{\songti \zihao{4}7月1日-7月9日工作汇报}
	\author{\textrm{Ku Jui}}
	\date{\textrm{July 2023}}
	\maketitle
	
	\renewcommand{\figurename}{Figure} % 可以重新定义abstract，因为ctex会覆盖thebibliography
	% 	\begin{abstract}
		%		In this experiment we studied a very important physical effect by measuring the
		%		dependence of a quantity $V$ of the quantity $X$ for two different sample
		%		temperatures.  Our experimental measurements confirmed the quadratic dependence
		%		$V = kX^2$ predicted by Someone's first law. The value of the mystery parameter
		%		$k = 15.4\pm 0.5$~s was extracted from the fit. This value is
		%		not consistent with the theoretically predicted $k_{theory}=17.34$~s. We attribute %this
		%		discrepancy to low efficiency of our $V$-detector.
		%	\end{abstract}
	\renewcommand{\contentsname}{Contents}
	\renewcommand{\tablename}{Table}
	\tableofcontents  % 自动生成目录
	
	\section{Pre-Knowledge}
		
	\subsection{Transformer \cite{vaswani2017attention}}
	
	在Attention引入以前，seq2seq模型处理机器翻译task最大的问题就是由于对于长距离的信息不能有效的提取和记忆，导致了信息的大量丢失。即使在引入Attention之后，也会因为对关系的捕捉不足，而出现翻译效果不理想。因为在这样的翻译任务中，需要发现的关系有三种：
	
	\begin{itemize}
		\item [(1)]
			源句内部的关系;
		\item [(2)]
			目标句内部的关系;
		\item [(3)]
			源句与目标句之间的关系;
	\end{itemize}
	
	之前的seq2seq模型只捕捉了源句与目标句之间的关系，而忽略了源句、目标句内部的关系，源句内部和目标句内部还是在用RNN，对远距离信息的捕捉能力很差。除了对远距离关系难以学习的不足以外，RNN 还有一点不足，那就是训练慢。因为它默认是按时序来进行处理的，一个个单词词从左到右看过去，导致 RNN 不能像 CNN 一样，充分利用 GPU 的并行运算优势。
	
	\subsubsection{Idea}
	
	\begin{itemize}
	
	\item [(1)] 刚刚提到了翻译任务存在有三种关系，原来的模型只学到其中一种，即源句与目标句之间的关系，Transformer\textbf{引入self-attention的机制将三种关系全部做了学习。}
	\item [(2)]	
	Transformer\textbf{提出了multi-head attention的机制}，分别学习对应的三种关系，使用了全Attention的结构。
		
	\item[(2)] 
	对于词语的位置，Transformer \textbf{使用positional encoding机制进行数据预处理}，增大了模型的并行性，取得了更好的实验效果。
	
	\end{itemize}
	
	\subsubsection{Architecture}
	
	\begin{figure}[htbp]
		% read manual to see what [ht] means and for other possible options
		\centering 
		\includegraphics[width=0.8\columnwidth]{picture/Transfromer_architecture}
		%\captionsetup{font=scriptsize}
		\caption{
			\label{fig: Transformer} The Transformer - model architecture.
		}
	\end{figure}
	
	如Fig.\ref{fig: Transformer}所示，\textbf{Transformer}模型也是使用经典的encoer-decoder架构，由encoder和decoder两部分组成。
	
	上图的左半边用Nx框中，为encoder的一层。\textbf{encoder一共有6层这样的结构。}
	
	上图的右半边用Nx框中，为decoder的一层。\textbf{decoder一共有6层这样的结构。}
	
	输入序列经过\textbf{word embedding}和\textbf{positional encoding}相加后，输入到encoder。
	
	输出序列经过\textbf{word embedding}和\textbf{positional encoding}相加后，输入到decoder。
	
	最后，decoder输出的结果，经过一个线性层，然后计算softmax。
	
	\paragraph{Encoder}
	
	encoder由6层相同的层组成，每一层分别由两部分组成：
	
	\begin{itemize}
		\item {}
			第一部分是一个multi-head self-attention mechanism;
		\item {}
			第二部分是一个position-wise feed-forward network，是一个全连接层.
	\end{itemize}
	
	两个部分，都有一个\textbf{残差连接(residual connection)}，然后接着一个\textbf{Layer Normalization}。
	
	
	\paragraph{Decoder}
	
	和encoder类似，decoder由6个相同的层组成，每一个层包括以下3个部分：
	
	\begin{itemize}
	\item {}
		第一个部分是multi-head self-attention mechanism
	\item {}
		第二部分是multi-head context-attention mechanism
	\item {}
		第三部分是一个position-wise feed-forward network
	\end{itemize}
	
	与encoder类似，上面三个部分的每一个部分，都有一个残差连接，后接一个Layer Normalization。
	
	但是，decoder出现了一个新的部分\textbf{multi-head context-attention}
	
	\subsubsection{Attention Mechanism}
	
	\textbf{Attention}是指，对于某个时刻的输出$y$，它在输入$x$上各个部分的注意力。这个注意力实际上可以理解为权重。
	
	\paragraph{Scaled dot-product attention}
	
	Transformer模型基于乘性注意力(multiplicative attention)，采用\textbf{scaled dot-product attention}，即\textbf{两个隐状态进行点积。}
	
	\begin{equation}
		\begin{aligned}
			\textbf{score}(\mathbf{s_t},\mathbf{h_i})=\frac{\mathbf{s_t^\top h_i}}{\sqrt{n}}
		\end{aligned}
		\label{eq: multiplicative attention}
	\end{equation}
	
	其中$h_i$为输入序列隐状态，$s_t$为输出序列的隐状态。
	
	\begin{figure}[htbp]
		% read manual to see what [ht] means and for other possible options
		\centering 
		\includegraphics[width=0.8\columnwidth]{picture/Transfromer_attention}
		\captionsetup{font=scriptsize}
		\caption{
			\label{fig: Transformer_attention} (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.
		}
	\end{figure}
	
	\textbf{self-attention}实际上就是，\textbf{输出序列就是输入序列！}因此，计算自己的attention得分，就叫做self-attention！
	
	\textbf{context-attention}是encoder和decoder之间的attention。
	
	从Fig.\ref{fig: Transformer_attention}(left)看出，Transformer中的attention机制可以被描述为，通过确定$Q$和$K$之间的相似程度来选择$V$，
	\begin{equation}
		\begin{aligned}
			\textbf{Attention}(Q,K,V)=\text{softmax}(\frac{QK^\top}{\sqrt{d_k}})V
		\end{aligned}
		\label{eq: Transformer_attention}
	\end{equation}
	
	其中，$d_k$表示的是$K$的维度。\footnote{\textbf{为什么需要加上这个缩放因子$d_k$呢？}论文里给出了解释：对于$d_k$很大的时候，点积得到的结果维度很大，使得结果处于softmax函数梯度很小的区域。
		
	梯度很小的情况下会对反向传播不利，为了克服这个负面影响，除以一个缩放因子，可以一定程度上减缓这种情况。}
	
	$Q, K, V$具体指代什么？\footnote{https://www.zhihu.com/question/427629601}
	
	在Transformer模型中，$Q, K, V$ 是用于执行自注意力机制（Self-Attention）的输入向量。它们分别表示查询(query)，键(key)和值(value)。
	
	\begin{itemize}
		\item {}
		
		Query($Q$)：Query表示查询向量。在自注意力机制中，每个Query向量都会与一组Key和Value向量进行相似度计算，以确定对不同Value向量的注意力权重。可以将Query视为要查询的信息或要关注的特征。
		
		\item {}
		
		Key($K$)：Key表示被查询信息与其他信息的相关性的向量。键向量用于与查询向量进行点积运算，以计算注意力权重。
		
		\item {}
		
		Value($V$)：Value表示被查询信息的向量。值向量用于根据注意力权重加权求和的方式来生成最终的上下文表示。	
	\end{itemize}
			
	在 Transformer 中，自注意力机制通过将查询向量与所有的键向量进行点积运算，然后将结果经过 softmax 归一化，得到注意力权重。这些注意力权重与对应的值向量进行加权求和，得到最终的上下文表示。
	
	Transformer模型中自注意力机制中$Q, K, V$的来源、用途以及维度的一些关键特征？
	
	\begin{itemize}
		\item {}
		
			在encoder的\textbf{self-attention}中，$Q, K, V$ 都来自同一个地方（相等），他们是上一层 encoder 的输出。对于第一层 encoder ，它们就是 word embedding 和 positional encoding 相加得到的输入。
		
		\item {}
		
			在decoder的\textbf{self-attention}中，$Q, K, V$ 都来自于同一个地方（相等），它们是上一层 decoder 的输出。对于第一层decoder，它们就是 word embedding 和 positional encoding 相加得到的输入。但是对于 decoder ，我们不希望它能获得下一个 time step （即将来的信息），因此我们需要进行 sequence masking 。

		\item {}
		
			在encoder-decoder attention中，$Q$来自于decoder的上一层的输出，$K$ 和 $V$ 来自于 encoder 的输出，$K$ 和 $V$ 是一样的。
			
		\item {}
			
			$Q$、$K$、$V$三者的维度一样，即 $d_q=d_k=d_v$。
	\end{itemize}
	
	\paragraph{Multi-head attention}
	
	什么是\textbf{multi-head attention}？
	
	如Fig.\ref{fig: Transformer_attention}(right)所示，作者发现将$Q,K,V$通过一个线性映射之后，分成 $h$ 份，对每一份进行 \textbf{scaled dot-product attention} 效果更好(其实就是将原始的输入序列进行多组的自注意力处理过程，然后再将每一组自注意力的结果拼接起来进行线性变换得到的输出结果。)然后，把各个部分的结果合并起来，再次经过线性映射，得到最终的输出。超参数 $h$ 表示的是 heads 数量\footnote{论文中的数量默认是8}。
	
	\begin{equation}
		\begin{aligned}
			& \textbf{MultiHead}(Q,K,V)=\textbf{Concat}(\textbf{head}_{1},\ldots,\textbf{head}_{h})W^{O}, \\
			& \textbf{where head}_{i} = \textbf{Attention}(QW_i^Q,KW_i^K,VW_i^V)
		\end{aligned}
		\label{eq: Multi-head attention}
	\end{equation}
	
	其中，$W_i^Q \in \mathbb{R}^{d_{model} \times d_q}$, $W_i^K \in \mathbb{R}^{d_{model} \times d_k}$, $W_i^V \in \mathbb{R}^{d_{model} \times d_v}$, $W^O \in \mathbb{R}^{hd_v \times d_{model}}$
	
	为什么要\textbf{multi-head attention}？
	
	Self-attention机制解决了传统序列模型再编码过程中都需要顺序进行的弊端（仅仅需要对原始输入进行几次矩阵变换便能够得到最终包含有不同位置注意力信息的编码向量），但Self-attention也存在缺陷：\textbf{模型在对当前位置的信息进行编码时，会过度的将注意力集中于自身的位置}，因此作者提出了通过多头注意力机制来解决这一问题。同时，使用多头注意力机制还能够给予注意力层的输出包含有不同子空间中的编码表示信息，从而增强模型的表达能力。
	
	\subsubsection{Layer normalization}
	
	\begin{itemize}
		\item {} Normalization 有很多种，但是它们都有一个共同的目的，那就是把输入转化成均值为 0 方差为 1 的数据。
		
		\item {} Normalization 是一种用于标准化数据的预处理技术\footnote{例如，假设我们有汽车租赁服务。首先，我们希望根据竞争对手的数据预测每辆车的合理价格。每辆车有两个特征：车龄和总行驶公里数。它们的范围可能非常不同，从 0 到 30 年不等，而距离可能从 0 到数十万公里。我们不希望特征在范围上存在这些差异，因为具有较高范围的值可能会使我们的模型产生偏差，从而赋予它们夸大的重要性。
		
		有两种主要方法可以标准化数据。最直接的方法是将其缩放到 0 到 1 的范围：
		
		\begin{equation}
			\begin{aligned}
				x_{normalized} = \frac{x-m}{x_{max}-x_{min}}
			\end{aligned}
			\label{eq: normalized_1}
		\end{equation}
		
		$x$是要标准化的数据点、$m$是数据集的平均值、$x_{max}$是最高值、$x_{min}$是最小值。该技术通常用于数据的输入。大范围的非标准化数据点可能会导致神经网络不稳定。相对较大的输入可能会级联到各层，从而导致梯度爆炸等问题。
		
		用于标准化数据的另一种技术是使用以下公式强制数据点的平均值为 0，标准差为 1：
		
		\begin{equation}
			\begin{aligned}
				x_{normalized} = \frac{x-\mu}{\sigma}
			\end{aligned}
			\label{eq: normalized_2}
		\end{equation}
		
		$x$ 是要标准化的数据点、$\mu$是数据集的平均值、$\sigma$是数据集的标准差。经过Eq.\ref{eq: normalized_2}后，每个数据点都模仿标准正态分布。拥有正态分布的所有特征，它们不会有偏差，因此，我们的模型会学得更好。
		}。换句话说，在同一范围内具有不同的数据源。在训练之前不对数据进行标准化可能会导致我们的网络出现问题，从而使训练变得更加困难并降低其学习速度。
	\end{itemize}
	
	
	\paragraph{Batch normalization}
	
	Batch normalization是在神经网络层之间而非在原始数据中完成的标准化方法，其主要思想是：\textbf{在每一层的每一批数据上进行归一化。}它是按照小批量而不是完整数据集完成的。它可以加快训练速度并使用更高的学习率，使学习变得容易。
	
	可以将Batch normalization的归一化公式定义为： 
	
	\begin{equation}
		\begin{aligned}
			z^{N} = \left( \frac{z-\mu_{z}}{\sigma_{z}} \right)
		\end{aligned}
		\label{eq: Batch normalization}
	\end{equation}
	其中$\mu_z$是神经元输出的平均值和$\sigma_z$神经元输出的标准差。
	
	我们可能会对输入数据进行归一化，但是经过该网络层的作用后，我们的的数据已经不再是归一化的了。随着这种情况的发展，数据的偏差越来越大，我的反向传播需要考虑到这些大的偏差，这就迫使我们只能使用较小的学习率来防止梯度消失或者梯度爆炸。
	
	
	Batch normalization的具体做法就是对每一小批数据，在批这个方向上做归一化。如Fig.\ref{fig: Batch_normlization}所示：
	
	\begin{figure}[htbp]
		% read manual to see what [ht] means and for other possible options
		\centering 
		\includegraphics[width=0.6\columnwidth]{picture/batch_normalization}
		\captionsetup{font=scriptsize}
		\caption{
			\label{fig: Batch_normlization} For a given layer, the mean during BN will be 1X. Each training data gets this mean subtracted from it and divided by $\sqrt{(var + \epsilon)}$ and then shifted and scaled. To find the mean and var, we use all the examples in the training batch.
		}
	\end{figure}
	
	具体Batch normalization是如何应用的？
	
	\begin{figure}[htbp]
		\centering 
		\includegraphics[width=0.8\columnwidth]{picture/neural-network}
		\captionsetup{font=scriptsize}
		\caption{
			\label{fig: Neural Network} 
			A conventional feedforward neural network.
		}
	\end{figure}
	
	在Fig.\ref{fig: Neural Network}中展示了一个常规的前馈神经网络，其中$x_i$是输入、$z$是神经元的输出、$a$是激活函数的输出、$y$是整个网络的输出。
	
	通常，没有Batch normalization的神经元将按Eq.\ref{eq: cell output}计算。
	
	\begin{equation}
		\begin{aligned}
			& z = g \left( w,x \right) + b \\
			& a = f(z) \\
		\end{aligned}
		\label{eq: cell output}
	\end{equation}
	
	$g()$是神经元的线性变换，$w$是神经元的权重，$b$是神经元的偏置，$f()$是激活函数。该模型学习参数$w$和$b$。添加Batch normalization之后，按Eq.\ref{eq: cell output(BN)}计算
	
	\begin{equation}
		\begin{aligned}
			& z = g \left( w,x \right)\\
			& BN(z_i) = \left(\frac{z_i-\mu_B}{\sqrt{\sigma_B^2 + \epsilon}}\right) \cdot \gamma + \beta \\
			& a = f(BN(z_i)) 
		\end{aligned}
		\label{eq: cell output(BN)}
	\end{equation}
	
	$z_i$是Batch normalization的输出，$\mu_B$是神经元输出的均值，$\sigma_B$是神经元输出的标准差，$\epsilon$是一个小的随机噪声（为了稳定性）。$\gamma$为尺度参数，$\beta$为位移参数，这两个参数可以从每一个epoch和其他参数（神经元权重）中学习得到\footnote{当我们在层上应用Batch normalization时，我们限制输入服从正态分布，这最终将限制网络的学习性能。为了解决这个问题，一般乘以尺度参数$\gamma$，并添加位移参数$\beta$。}。
	
	Batch normalization的好处是：
	
	\begin{itemize}
		\item {}
		
		\textbf{深度神经网络可以训练得更快：}虽然由于前向传播过程中的额外归一化计算和反向传播过程中需要训练的额外超参数，每次训练迭代都会变慢，但它应该收敛得更快；因此，训练总体上应该更快。	
		
		\item {}
		\textbf{更高的学习率：}梯度下降通常需要较小的学习率才能使网络收敛。随着网络变得更深，梯度在反向传播过程中变得更小，因此需要更多的迭代。使用批量归一化可以提高学习率，从而提高训练速度。
		
		\item {}
		\textbf{更容易初始化权重：}权重初始化可能很困难，特别是在创建更深的网络时。批量归一化降低了对初始起始重量的敏感性。
		
	\end{itemize}
	
	\paragraph{Layer normalization}
	
	Layer normalization在许多方面与 BN 是类似的，如Eq.\ref{eq: Layer normalization}，但是Layer normalization一般多用于循环架构中，如RNN结构。
	
	\begin{equation}
		\begin{aligned}
			\text{LN} = \left(\frac{z_i-\mu_L}{\sqrt{\sigma_L^2 + \epsilon}}\right) \cdot \gamma + \beta 
		\end{aligned}
		\label{eq: Layer normalization}
	\end{equation}
	
	从Eq.\ref{eq: Layer normalization}中可以看出，Layer normalization作用于每层的每一个样本，其计算特定训练点的特定层的均值和方差。
	
	Layer normalization与 BN 有点不同， LN独立计算每层每个样本的均值和方差，然后使用这些计算值进行 LN 运算(Eq.\ref{eq: Layer normalization})。Layer normalization的过程如Fig.\ref{fig: Layer_normlization}所示，其在每个样本上计算均值和方差，而不是像BN在每个批次上计算均值和方差。
	
	\begin{figure}[htbp]
		% read manual to see what [ht] means and for other possible options
		\centering 
		\includegraphics[width=0.6\columnwidth]{picture/layer_normalization}
		\captionsetup{font=scriptsize}
		\caption{
			\label{fig: Layer_normlization} For a given layer, the mean during LN will be NX1. We compute the mean and var for every single sample for each layer independently and then do the LN operations using those computed values.
		}
	\end{figure}
	
	\subsubsection{Mask}
	
	Mask 表示掩码，它对某些值进行掩盖，使其在参数更新时不产生效果。Transformer 模型里面涉及两种mask，分别是 padding mask 和 sequence mask 。\footnote{其中，padding mask在所有的scaled dot-product attention里面都需要用到，而sequence mask只有在decoder的self-attention里面用到。}
	
	\paragraph{Padding mask}
	
	因为每个批次输入序列长度是不一样的，因此我们需要对不同长度的序列进行对齐，具体来说，就是给在较短的序列后面填充 0。因为这些填充的位置其实是没有意义的，Attention 机制不应该把注意力放在这些位置上，所以具体的做法是，把这些位置的值加上一个非常大的负数(负无穷)，这样经过 softmax 后这些位置的概率就会接近 0，不会影响 Attention 的结果。
	
	\paragraph{Sequence mask}
	
	Fig.\ref{fig: Transformer_attention}(left)为 Multi-head attention中的一个模块，对于decoder中的第一个 Multi-head attention 层，也需要添加一个mask模块进去。这是为了使得decoder看不见未来的信息。也就是说，对于一个序列，在 time\_step 为 t 的时刻，我们的解码输出应该只能依赖于 t 时刻之前的输出，而不能依赖 t 时刻之后的输出。因此我们通过增加Mask的方法，把 t 时刻之后的信息给隐藏起来。\footnote{具体的做法为：\textbf{产生一个上三角矩阵(Fig.\ref{fig: subsequent mask})，下三角的值全为1，上三角的值权威0，对角线也是1。}把这个矩阵作用在每一个序列上，就能达到目的。}
	
	\begin{figure}[htb]
		% read manual to see what [ht] means and for other possible options
		\centering 
		\includegraphics[width=0.4\columnwidth]{picture/subsequent_mask}
		\captionsetup{font=scriptsize}
		\caption{
			\label{fig: subsequent mask}Sequence mask.
		}
	\end{figure}
	
	\subsubsection{Positional encoding}

	Transformer 摒弃了之前机器翻译任务中常用的 RNN 结构，使得并行性更好。RNN的这一结构天生考虑了词语的先后顺序关系。当 Transformer 模型不使用 RNN 结构时，它就要想办法通过其它机制把位置信息传输到 encoder ，否则可能会出现“所有词语都对了，但是无法组成有意义的语句”的情况。为了解决这个问题，论文使用了 Positional Encoding 对序列中词语出现的位置进行了编码。
	
	在具体实现时，采用正余弦函数来表示Positional encoding
	
	\begin{equation}
		\begin{aligned}
			& \text{PE}\left(pos, 2i\right) = \sin \left(pos/10000^{2i/d_{model}}\right) \\
			& \text{PE}\left(pos, 2i+1\right) = \cos \left(pos/10000^{2i/d_{model}}\right)
		\end{aligned}
		\label{eq: Positional encoding}
	\end{equation}
	
	其中， $pos$ 是指词语在序列中的位置，$d_{model}$\footnote{论文中默认维度是512}是模型的维度，该公式的意思是：给定词语的位置 $pos$ ，就可以把它编码成$d_{model}$维的向量。可以看出，在偶数位置，使用正弦编码，在奇数位置，使用余弦编码。\footnote{正弦函数能够表达相对位置信息，主要数学依据见Eq.\ref{eq: Sum-to-product identities}	
		
	\begin{equation}
		\begin{aligned}
			& \sin \left(\alpha + \beta \right) = \sin \alpha \cos \beta + \cos \alpha \sin \beta \\
			& \cos \left(\alpha + \beta \right) = \cos \alpha \cos \beta - \sin \alpha \sin \beta
		\end{aligned}
		\label{eq: Sum-to-product identities}
	\end{equation}
	Eq.\ref{eq: Sum-to-product identities}表明，对于词汇之间的位置偏移 k, $\text{PE}(pos + k)$可以表示为$\text{PE}(pos)$和$\text{PE}(k)$的组合形式，即具备表达相对位置的功能。
	}
		
	\subsubsection{Work embedding}
	
	Word embedding 是将 word 看作最小的一个单元，将文本空间中的某个 word ，通过一定的方法，映射或者说嵌入(embedding)到另一个数值向量空间。Word embedding的输入是原始文本中的一组不重叠的词汇，将它们放到一个字典里面，例如：["cat","eat","apple"]，就可以作为一个输入。
	
	Word embedding 的输出就是每个 word 的向量表示，变成一个矩阵，如Fig.\ref{fig: word embedding}所示，通过一个4维向量来表示单词。对于这个二维浮点矩阵，里面的权重都是可以训练的参数，Word embedding 的过程就是如何去构建这个矩阵\footnote{论文中 word embedding 矩阵是一个$\texttt{vocab\_size} \times \texttt{embedding\_size}$的二维张量。在论文中，其中\texttt{vocab\_size}是词典的大小，\texttt{embedding\_size}是词嵌入的维度大小，论文中维度大小是512。}。
	
	\begin{figure}[htbp]
		% read manual to see what [ht] means and for other possible options
		\centering 
		\includegraphics[width=1.0\columnwidth]{picture/word_embedding}
		\captionsetup{font=scriptsize}
		\caption{
			\label{fig: word embedding}Word embedding.
		}
	\end{figure}
	
	Word embedding 的优点是可以将每一单词映射到一个低维、稠密的实数向量上，向量的每一维代表单词的潜在特征，且该特征能够捕捉到有用的语法和语义属性。词嵌入的最大优势在于我们可以相对快速、有效地根据向量间的距离（比如余弦距离）判断单词的相似性，原因在于语义或语法相似的单词拥有相似的向量表示（见Fig.\ref{fig: word embeding 2d}）。词嵌入的构建主要依赖于神经网络模型，神经网络能够非常灵活地以线性组合方式表示任意 n-gram 词组，且参数以线性速度増长，相对于基于矩阵和聚类的分布表示，词向量能够捕捉到更多复杂、有用的语义信息。
	
	深度学习 word2vec 是 Google 的一款将词表征为实数值向量的高效工具, 其利用深度学习的思想, 通过训练把对文本内容的处理简化为 k 维向量空间中的向量运算, 而向量空间上的相似度可以用来表示文本语义上的相似度。在实际应用场景中,  word2vec 可以用于计算词的相似度、文档的相似度等, 也可以用来对文档进行情感分析等。
	
	\begin{figure}[htbp]
		% read manual to see what [ht] means and for other possible options
		\centering 
		\includegraphics[width=0.5\columnwidth]{picture/word-embeding-2d}
		\captionsetup{font=scriptsize}
		\caption{
			\label{fig: word embeding 2d}Embedding Later, words with similar meanings will be similar in vector space, and naturally have the effect after clustering, which is a kind of unsupervised learning.
		}
	\end{figure}
	
	深度学习 word2vec 中有 CBOW 和 Skip-gram 两种神经网络框架可用于训练并得到参数, 参数迭代更新后得到的就是每个字的词向量。
	
	\subsubsection{Position-wise Feed-Forward network}
	
	除了注意力子层之外，编码器和解码器中的每个层都包含一个完全连接的前馈网络，该网络单独且相同地应用于每个位置。它由两个线性变换组成，中间有一个ReLU激活\footnote{$ReLU: \ f\left(x\right) = \max \left(0,x\right)$}。
	
	\begin{equation}
		\begin{aligned}
			\text{FFN}(x) = \max \left(0,xW_1+b_1\right)W_2 + b_2
		\end{aligned}
		\label{eq: Position-wise Feed-Forward network}
	\end{equation}
	
	其中，$W_1$和$W_2$的大小均是 $d_{model} \times d_{ff}$\footnote{$d_{ff}$少则2040，多则4096。全连接层一直是神经网络中的参数量大户，Transformer也不例外。}。
	
	Transformer中的FFN全称是 Position-wise Feed-Forward network，重点在于 Position-wise ，区别于普通的全连接网络，这里 FFN 的输入是序列中每个位置上的元素，而不是整个序列，所以每个元素完全可以独立计算，最极端节省内存的做法是遍历序列，每次只取一个元素得到 FFN 的结果，但是这样做时间消耗太大，“分段”的含义就是做下折中，将序列分成 $N$ 段，也就是 $N$ 个子序列，每次读取一个子序列进行 FFN 计算，最后将 $N$ 份的结果拼接，如Eq.\ref{eq: Position-wise Feed-Forward network}所示。
	
	\begin{equation}
		\begin{aligned}
			Y = [Y_1;Y_2;\ldots;Y_N] = [X_1 + \text{FFN}(X_1);X_2 + \text{FFN}(X_2); \ldots; X_N + \text{FFN}(X_N)]
		\end{aligned}
		\label{eq: Position-wise Feed-Forward network}
	\end{equation}
	
	分段 FFN 只是一种计算上的技巧，计算结果和原始 FFN 完全一致，所以不会影响到模型效果，好处是不需要一次性将整个序列$\left(batch\_size , L, d_{model}\right)$读入内存，劣势是会增加额外的时间开销。
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                                                %% 
%%								          Paper reading                                           %%
%%                                                                                                %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	

	
	\section{Paper reading}
	
	\subsection{Ultra-High-Definition Low-Light Image Enhancement \cite{wang2022ultrahighdefinition}}
	
	\subsubsection{Introduce}
	
	设备捕捉超高清(UHD)图像和视频的能力对图像处理管道提出了新的要求。
	
	本文考虑了低光亮图像增强(LLIE)的任务，构建了两种不同分辨率的数据集Ultra High Definition Low-Light Image Enhancement(UHD-LOL)，并在不同方法进行基准测试。作者提出一种基于\textbf{Transformer}的微光增强方法LLFormer。LLFormer的核心组件是基于轴的多头自注意和跨层注意融合块，显著降低了线性复杂度。
	
	\begin{figure}[htbp]
		% read manual to see what [ht] means and for other possible options
		\centering 
		\includegraphics[width=\columnwidth]{picture/LLFormer_architecture}
		\captionsetup{font=scriptsize}
		\caption{
			\label{fig: LLFormer architecture} % spaces are big no-no withing labels
			% things like fig: are optional in the label but it helps
			% to orient yourself when you have multiple figures,
			% equations and tables
			\textbf{LLFormer architecture}. The core design of LLFormer includes an \colorbox{yellow}{axis-based transformer block} and a \colorbox{yellow}{cross-layer attention fusion block}. In the former, \colorbox{yellow}{axis-based multi-head self-attention} performs self-attention on the height and width axis across the channel dimension sequentially to reduce the computational complexity, and a \colorbox{yellow}{dual gated feed-forward network} employs a gated mechanism to focus more on useful features. The cross-layer attention fusion block learns the attention weights of features in different layers when fusing them.
			}
	\end{figure}
	LLFormer的核心设计包括一个基于轴的变压器块(Axis-based Transformer Block)和一个跨层注意力融合块(Dual Gated Feed-forward Network)。在前者中，基于轴的多头自注意在通道维度上依次对高度和宽度轴进行自注意，以降低计算复杂度，而双门控前馈网络采用门控机制来更多地关注有用的特征。跨层注意力融合块在融合不同层中的特征时学习它们的注意力权重。
	
	\subsubsection{Innovation}
	
	LLFormer 的整体框架如Fig.\ref{fig: LLFormer architecture}所示，可以看出和 Restormer 有些类似。作者改进了三个点：
	\begin{itemize}
		\item {(1)} Transformer block里面修改了attention；
		\item {(2)} Transformer block里修改了FFN；
		\item {(3)} 添加了cross-layer attention。
	\end{itemize}
	
	\paragraph{Axis-based Transformer Block}
	
	\textbf{Transformer}在图像修复中应用的难点在于计算复杂度高，在$Q$和$K$计算相似性时，对于输入为$(C,H,W)$的特征需要进行$(C,H,W) \times (C,H,W)$的矩阵运算。论文使用基于轴的多头注意力，因此，作者分为两个步骤，第一步相似性计算的是$H \times H$，叫做 \texttt{height-axis attention} 。第二步相似性计算的是 $W \times W$，叫做 \texttt{width-axis attention} 。(这里可以对比 Restormer，只是在C这个维度计算相似性)。。
	
	\paragraph{Dual Gated Feed-forward Network(DGFN)}
	
	在Restormer中，FFN有两个分支，其中有一个分支上使用GELU激活对另一个分支添加门控。
	在该文中，如图Fig.\ref{fig: Axis-based Transformer Block & Dual Gated Feed-forward Network(GDFN)}c，作者在FFN中引入双门控机制，提出双门控前馈网络 DGFN 来捕获特征中的更重要的信息。FFN两个分支都使用 GELU 激活，再互相给另外一个分支添加门控增强非线性建模能力。
	
	\begin{figure}[htbp]
		% read manual to see what [ht] means and for other possible options
		\centering 
		\includegraphics[width=\columnwidth]{picture/Axis_based_Transformer_Block_&_Dual_Gated_Feed-forward_Network(GDFN)}
		\captionsetup{font=scriptsize}
		\caption{
			\label{fig: Axis-based Transformer Block & Dual Gated Feed-forward Network(GDFN)} % spaces are big no-no withing labels
			% things like fig: are optional in the label but it helps
			% to orient yourself when you have multiple figures,
			% equations and tables
			The architecture of our Axis-based Multi-head Self-Attention and Dual Gated Feed-Forward Network. From left to right, the components are Height Axis Multi-head Attention, Width Axis Multi-head Attention, and Dual Gated Feed-Forward Network.
		}
	\end{figure}
	
	\begin{equation}
		\begin{aligned}
			& \mathbf{F}_\prime = \text{A-MSA} \left(\text{LN}(\textbf{F}_\textbf{in})\right) + \mathbf{F}_\mathbf{in}, \\
			& \mathbf{F}_\mathbf{out} = \text{DGFN} \left(\text{LN}\mathbf{F}^\prime)\right) + \mathbf{F}^\prime,
		\end{aligned}
		\label{eq: FFN}
	\end{equation}
	
	其中，\text{DGFN}公式如下：
	
	\begin{equation}
		\begin{aligned}
			& \mathbf{DG} = \phi \left(W_{3\times 3}^1 W_{1\times1}^1 \mathbf{Y} \right) \odot \left(W_{3\times3}^2 W_{1\times1}^2\mathbf{Y} \right)\\ 
			& \quad \quad  + \left(W_{3\times3}^1W_{1\times1}^1\mathbf{Y}\right) \odot \phi \left(W_{3\times3}^2 W_{1\times1}^2 \mathbf{Y}\right), \\
			& \hat{\mathbf{Y}} = W_{1\times1}\textbf{DG}\left(\mathbf{Y}\right) + \mathbf{Y}
		\end{aligned}
		\label{eq: DGFN}
	\end{equation}
	
	\paragraph{Cross-layer Attention Fusion Block}
	
	\begin{figure}[htbp]
		% read manual to see what [ht] means and for other possible options
		\centering 
		\includegraphics[width=\columnwidth]{picture/Cross-layer_Attention_Fusion_Block}
		\captionsetup{font=scriptsize}
		\caption{
			\label{fig: Cross-layer_Attention_Fusion_Block} % spaces are big no-no withing labels
			% things like fig: are optional in the label but it helps
			% to orient yourself when you have multiple figures,
			% equations and tables
			The architecture of the proposed Cross-layer Attention Fusion Block. This block efficiently integrates features from different layers with a layer correlation attention matrix.
		}
	\end{figure}
	
	 网络一般有多层，但大多方法没有考虑层与层之间特征的关联，限制了表示能力。论文使用cross-layer attention获取不同层特征间的相关性并融合。从论文整体架构图中可看到，网络输入有三个Transformer block，能得到三个特征输出。论文通过cross-layer attention 运算，计算一个3x3的相似性矩阵，给输入的特征进行加权，从而达到强调重要特征、抑制不重要特征的作用。（不同层的激活是对特定类别的响应，并且可以使用子注意力机制自适应地学习特征相关性。）
	
	\begin{equation}
		\begin{aligned}
			& \hat{\mathbf{F}}_{\mathbf{out}} = W_{1\times1}^1 \text{Layer\_Attention}(\hat{\mathbf{Q}}, \hat{\mathbf{K}}, \hat{\mathbf{V}}) + \hat{\mathbf{F}_{\mathbf{in}}},\\ 
			& \text{Layer\_Attention}(\hat{\mathbf{Q}}, \hat{\mathbf{K}}, \hat{\mathbf{V}}) = \hat{\mathbf{V}}\text{softmax}\left(\hat{\mathbf{Q}}\hat{\mathbf{K}}/\mathbf{\alpha}\right), 
		\end{aligned}
		\label{eq: DGFN}
	\end{equation}
	
	\subsubsection{Result}
	
	结果见Fig.\ref{fig: result_in_UHD-LOL} 和 Fig.\ref{fig: result_in_LOL_and_MIT}
	
	\begin{figure}[htbp]
		% read manual to see what [ht] means and for other possible options
		\centering 
		\includegraphics[width=\columnwidth]{picture/Results_on_UHD-LOL}
		\captionsetup{font=scriptsize}
		\caption{
			\label{fig: result_in_UHD-LOL} % spaces are big no-no withing labels
			% things like fig: are optional in the label but it helps
			% to orient yourself when you have multiple figures,
			% equations and tables
			Benchmarking study on the UHD-LOL4K and UHD-LOL8K subsets. $\dag$; $\ddag$; $\S$;$\triangle$ and ? indicate the traditional methods, supervised CNN-based methods, unsupervised CNN-based methods, zero-shot methods and transformer-based methods. The top three results are marked in red, blue and purple, respectively.
			Input
		}
	\end{figure}
	
	\begin{figure}[htbp]
		% read manual to see what [ht] means and for other possible options
		\centering 
		\includegraphics[width=\columnwidth]{picture/Results_on_LOL_and_MIT-Adobe_FiveK}
		\captionsetup{font=scriptsize}
		\caption{
			\label{fig: result_in_LOL_and_MIT} % spaces are big no-no withing labels
			% things like fig: are optional in the label but it helps
			% to orient yourself when you have multiple figures,
			% equations and tables
			Comparison results on LOL and MIT-Adobe FiveK datasets in terms of PSNR, SSIM, LPIPS and MAE. The top three results are marked in red, blue and purple, respectively. Same as (Zamir et al. 2020), we consider images from expert C for the MIT-Adobe FiveK dataset.
		}
	\end{figure}
	
	\section{个人工作进展}
	
	\textcolor{red}{完成了对 Transformer 原始模型的介绍，了解核心模块自注意力机制和多头注意力机制，以及对 Transformer 模型的创新点有了比较系统的了解。}
	
	\subsection{思考}
	
	\begin{itemize}
		\item [(1)] 以后的工作中如果有涉及可以尝试使用双门控来增强局部信息的提取能力，在有依据的情况下可以将注意力计算分解为多步，通过多步计算来控制计算量。
		
		\item [(2)] Self-attention Module 到底在 CV 中能做什么？
		
		\textcolor{red}{本质上，Multi-head attention 机制就是分解成多个Self-attention机制，self-attention的特点在于无视词(token)之间的距离直接计算依赖关系，从而能够学习到序列的内部结构。基于这种能力，往CV方面拓展，主要体现在以下几个方面：}
		
		\textcolor{red}{\textbf{图像分类}：通过在卷积神经网络（CNN）中引入self-attention模块，可以帮助模型更好地捕捉图像中的重要区域和特征，提升图像分类的性能。Self-attention模块能够自动学习图像中不同位置之间的关联性，并为不同区域分配不同的注意力权重。}
		
		\textcolor{red}{\textbf{目标检测和定位}：在目标检测任务中，self-attention模块可以在不同尺度的特征图上应用，以更准确地定位和识别目标对象。它可以自动聚焦于目标对象的重要特征，并对背景区域进行抑制，从而提高检测和定位的准确性。}
		
		\textcolor{red}{\textbf{图像分割}：在语义分割和实例分割任务中，self-attention模块可以增强模型对图像中的局部和全局上下文信息的感知能力。通过学习像素之间的关系和相互作用，self-attention模块能够更好地理解图像的结构和语义，并生成更精确的分割结果。}
		
		\textcolor{red}{\textbf{图像生成}：在图像生成任务中，self-attention模块可以帮助生成更细节丰富、更真实的图像。通过对生成器的不同位置应用self-attention，模型可以更好地捕捉全局和局部之间的关联性，从而提高生成图像的质量和一致性。}
		
		\textcolor{red}{当涉及到计算机视觉(CV)中的 Transformer 模型时，以下三个方面需要考虑，尽管已经取得了一些进展，但仍然存在挑战：}
		
		\begin{itemize}
			\item {}
			\textcolor{red}{\textbf{计算效率}：目前 Transformer 模型在计算效率方面仍无法与 CNN 相媲美。由于图像和视频的信息量远大于文本数据，传统的 Transformer 模型在 CV 领域仍面临较高的计算开销。为了更好地适应 CV 数据，需要进一步研究和改进 CV 专用的 Transformer 结构。这是一个研究热点领域，预计未来会有更多相关工作涌现。}
			
			\item {}
			\textcolor{red}{\textbf{应用设配}：除了对基础网络结构的改进外，进一步推动 Transformer 在 CV 领域的发展还需要在各个 CV 下游任务中取得成功。这意味着不仅仅是将 Transformer 替换 CNN 的骨干网络，还需要对 Transformer 进行更多应用适配。此外，还需要针对 CV 任务对 Transformer 进行专门的改进，以展示 Transformer 相较于 CNN 的优势之处。}
			
			\item {}
			\textcolor{red}{\textbf{关键任务的突破}:尽管 Transformer 在许多 CV 任务上取得了成功，但仍有一些任务尚未完全克服。例如，在视频领域中，仍存在一些挑战，如人脸识别、识别和检测任务的准确率仍有提升空间。克服这些难题需要更深入的研究和创新。}
		\end{itemize}	
		
	\end{itemize}

	
	\section{下周工作计划}
	
	(1) 继续补充和细化基本 Transformer 结构原理。
	
	(2) 了解的Transformer模型是通用模型，而且应用在NLP领域，具体细化到CV领域，会有领域差异，目前已经了解到的应用到CV领域的Transformer模型，有ViT，DeiT，Swin，对它们进行一个初步的了解
	
	%	\section{Analysis}
	
	%	In this section you will need to show your experimental results. Use tables and
	%	graphs when it is possible. Table~\ref{tbl:bins} is an example.
	
	%	\begin{table}[ht]
		%		\begin{center}
			%			\caption{Every table needs a caption.}
			%			\label{tbl:bins} % spaces are big no-no withing labels
			%			\begin{tabular}{|ccc|} 
				%				\hline
				%				\multicolumn{1}{|c}{$x$ (m)} & \multicolumn{1}{c|}{$V$ (V)} & \multicolumn{1}{c|}{$V$ (V)} \\
				%				\hline
				%				0.0044151 &   0.0030871 &   0.0030871\\
				%				0.0021633 &   0.0021343 &   0.0030871\\
				%				0.0003600 &   0.0018642 &   0.0030871\\
				%				0.0023831 &   0.0013287 &   0.0030871\\
				%				\hline
				%			\end{tabular}
			%		\end{center}
		%	\end{table}
	%	
	%	Analysis of equation~\ref{eq:aperp} shows ...
	%	
	%	Note: this section can be integrated with the previous one as long as you
	%	address the issue. Here explain how you determine uncertainties for different
	%	measured values. Suppose that in the experiment you make a series of
	%	measurements of a resistance of the wire $R$ for different applied voltages
	%	$V$, then you calculate the temperature from the resistance using a known
	%	equation and make a plot  temperature vs. voltage squared. Again suppose that
	%	this dependence is expected to be linear~\cite{Cyr}, and the proportionality coefficient
	%	is extracted from the graph. Then what you need to explain is that for the
	%	resistance and the voltage the uncertainties are instrumental (since each
	%	measurements in done only once), and they are $\dots$. Then give an equation
	%	for calculating the uncertainty of the temperature from the resistance
	%	uncertainty. Finally explain how the uncertainty of the slop of the graph was
	%	found (computer fitting, graphical method, \emph{etc}.)
	%	
	%	If in the process of data analysis you found any noticeable systematic
	%	error(s), you have to explain them in this section of the report.
	%	
	%	It is also recommended to plot the data graphically to efficiently illustrate
	%	any points of discussion. For example, it is easy to conclude that the
	%	experiment and theory match each other rather well if you look at
	%	Fig.~\ref{fig:samplesetup} and Fig.~\ref{fig:exp_plots}.
	%	
	%	\begin{figure}[ht] 
		%		\centering
		%		\includegraphics[width=0.5\columnwidth]{sr_squeezing_vs_detuning}
		%		
		%		% some figures do not need to be too wide
		%		\caption{
			%			\label{fig:exp_plots}  
			%			Every plot must have axes labeled.
			%		}
		%	\end{figure}
	
	
	%	\section{Conclusions}
	%	Here you briefly summarize your findings.
	
	%++++++++++++++++++++++++++++++++++++++++
	% References section will be created automatically 
	% with inclusion of "thebibliography" environment
	% as it shown below. See text starting with line
	% \begin{thebibliography}{99}
		% Note: with this approach it is YOUR responsibility to put them in order
		% of appearance.
		
		\renewcommand{\refname}{References}
		
		
		%	\begin{thebibliography}{00}
			
			%		\bibitem{b1}\label{cite:b1}
			%		W. Wang, C. Wei, W. Yang and J. Liu, "GLADNet: Low-Light Enhancement Network with Global Awareness," 2018 13th IEEE International Conference on Automatic Face \& Gesture Recognition (FG 2018), Xi'an, China, 2018, pp. 751-755, DOI: 10.1109/FG.2018.00118.
			
			%		\bibitem{b2}\label{cite:b2}
			%		A.\ Mahajan, K.\ Somaraj and M. Sameer, "Adopting Artificial Intelligence Powered ConvNet To Detect Epileptic Seizures," 2020 IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES), Langkawi Island, Malaysia, 2021, pp. 427-432, DOI: 10.1109/IECBES48179.2021.9398832.
			
			%		\bibitem{Cyr}
			%		N.\ Cyr, M.\ T$\hat{e}$tu, and M.\ Breton,
			% "All-optical microwave frequency standard: a proposal,"
			%		IEEE Trans.\ Instrum.\ Meas.\ \textbf{42}, 640 (1993).
			
			
			
			%	\end{thebibliography}
		
		\bibliographystyle{unsrt}
		\bibliography{reference}
		
		
	\end{document}
