\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/D>>}
\citation{zhang2023frc}
\citation{ronneberger2015u}
\citation{zhang2021accurate}
\citation{sandler2018mobilenetv2}
\citation{guo2019pipeline}
\citation{lin2017feature}
\@writefile{toc}{\contentsline {section}{\numberline {1}实验进度}{2}{section.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Linear bottlenecks 和 Inverted residuals\relax }}{2}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig: Base module}{{1}{2}{Linear bottlenecks 和 Inverted residuals\relax }{figure.caption.1}{}}
\citation{zeiler2014visualizing}
\newlabel{fig: myplot_different_color_channels_low00010}{{2a}{3}{low00010\relax }{figure.caption.2}{}}
\newlabel{sub@fig: myplot_different_color_channels_low00010}{{a}{3}{low00010\relax }{figure.caption.2}{}}
\newlabel{fig: myplot_different_color_channels_low00747}{{2b}{3}{low00747\relax }{figure.caption.2}{}}
\newlabel{sub@fig: myplot_different_color_channels_low00747}{{b}{3}{low00747\relax }{figure.caption.2}{}}
\newlabel{fig: myplot_different_color_channels_low00776}{{2c}{3}{low00776\relax }{figure.caption.2}{}}
\newlabel{sub@fig: myplot_different_color_channels_low00776}{{c}{3}{low00776\relax }{figure.caption.2}{}}
\newlabel{fig: myplot_different_color_channels_r097088c1t}{{3a}{3}{r097088c1t\relax }{figure.caption.3}{}}
\newlabel{sub@fig: myplot_different_color_channels_r097088c1t}{{a}{3}{r097088c1t\relax }{figure.caption.3}{}}
\newlabel{fig: myplot_different_color_channels_r141669e5t}{{3b}{3}{r141669e5t\relax }{figure.caption.3}{}}
\newlabel{sub@fig: myplot_different_color_channels_r141669e5t}{{b}{3}{r141669e5t\relax }{figure.caption.3}{}}
\newlabel{fig: myplot_different_color_channels_r145221d9t}{{3c}{3}{r145221d9t\relax }{figure.caption.3}{}}
\newlabel{sub@fig: myplot_different_color_channels_r145221d9t}{{c}{3}{r145221d9t\relax }{figure.caption.3}{}}
\citation{zhu2023biformer}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces CBAM\relax }}{4}{figure.caption.4}\protected@file@percent }
\newlabel{fig: CBAM}{{4}{4}{CBAM\relax }{figure.caption.4}{}}
\newlabel{eq: v(x)}{{3}{4}{}{equation.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Specific structure of Spatial Attention Module embedded with self-attention mechanism\relax }}{4}{figure.caption.5}\protected@file@percent }
\newlabel{fig: SA_mod}{{5}{4}{Specific structure of Spatial Attention Module embedded with self-attention mechanism\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The overview of BSAM\relax }}{5}{figure.caption.6}\protected@file@percent }
\newlabel{fig: BSAM}{{6}{5}{The overview of BSAM\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Vanilla attention and its sparse variants. (a) Vanilla attention operates gloabally and incurs high computational complexity and heavy memory footprint. (b)-(d) Several works attempt to alleviate the complexity by introducing sparse attention with different handcrafted patterns, such as local window, axial stripe, dilated window. (e) Deformable attention enables image-adaptive sparsity via deforming a regular grid. (f) We achieve dynamic, query-aware sparsity with bi-level routing attention, which first searches top-k (k = 3 in this case) relevant regions, and then attends to the union of them.\relax }}{5}{figure.caption.7}\protected@file@percent }
\newlabel{fig: Vanilla attention}{{7}{5}{Vanilla attention and its sparse variants. (a) Vanilla attention operates gloabally and incurs high computational complexity and heavy memory footprint. (b)-(d) Several works attempt to alleviate the complexity by introducing sparse attention with different handcrafted patterns, such as local window, axial stripe, dilated window. (e) Deformable attention enables image-adaptive sparsity via deforming a regular grid. (f) We achieve dynamic, query-aware sparsity with bi-level routing attention, which first searches top-k (k = 3 in this case) relevant regions, and then attends to the union of them.\relax }{figure.caption.7}{}}
\citation{guo2022segnext}
\citation{huang2023channel}
\citation{huang2023channel}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces 显存开销太大\relax }}{6}{figure.caption.8}\protected@file@percent }
\newlabel{fig: BSAM Experiment}{{8}{6}{显存开销太大\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces MSAM\relax }}{6}{figure.caption.9}\protected@file@percent }
\newlabel{fig: MSAM}{{9}{6}{MSAM\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces MSCA\relax }}{7}{figure.caption.10}\protected@file@percent }
\newlabel{fig: MSCA}{{10}{7}{MSCA\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Schematic representation of the refined features of the three attention mechanisms (a) SE, (b) CBAM, and (c) CPCA.\relax }}{7}{figure.caption.11}\protected@file@percent }
\newlabel{fig: SE_CBAM_CPCA}{{11}{7}{Schematic representation of the refined features of the three attention mechanisms (a) SE, (b) CBAM, and (c) CPCA.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces CPCA\relax }}{7}{figure.caption.12}\protected@file@percent }
\newlabel{fig: CPCA}{{12}{7}{CPCA\relax }{figure.caption.12}{}}
\citation{lou2023transxnet}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces CPMS\relax }}{8}{figure.caption.13}\protected@file@percent }
\newlabel{fig: CPMS}{{13}{8}{CPMS\relax }{figure.caption.13}{}}
\newlabel{fig: FLOGs(G)}{{14a}{8}{FLOGs(G)\relax }{figure.caption.14}{}}
\newlabel{sub@fig: FLOGs(G)}{{a}{8}{FLOGs(G)\relax }{figure.caption.14}{}}
\newlabel{fig: Number of Parameters(M)}{{14b}{8}{Number of Parameters(M)\relax }{figure.caption.14}{}}
\newlabel{sub@fig: Number of Parameters(M)}{{b}{8}{Number of Parameters(M)\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Comparison of Top-1 accuracy on ImageNet-1K with recent SOTA methods. Our proposed TransXNet outperforms all other models.\relax }}{8}{figure.caption.14}\protected@file@percent }
\newlabel{fig: TransXNet Outperforms}{{14}{8}{Comparison of Top-1 accuracy on ImageNet-1K with recent SOTA methods. Our proposed TransXNet outperforms all other models.\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces The overall architecture of the proposed TransXNet\relax }}{9}{figure.caption.15}\protected@file@percent }
\newlabel{fig: TransXNet}{{15}{9}{The overall architecture of the proposed TransXNet\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Workflow of the proposed D-Mixer.\relax }}{9}{figure.caption.16}\protected@file@percent }
\newlabel{fig: D-Mixer}{{16}{9}{Workflow of the proposed D-Mixer.\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {paragraph}{Overkapping Spatial Reduction Attention(OSRA)}{9}{figure.caption.16}\protected@file@percent }
\bibstyle{unsrt}
\bibdata{reference}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}其他改进}{10}{subsection.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Maxpool2d\relax }}{10}{figure.caption.17}\protected@file@percent }
\newlabel{fig: Maxpool2d}{{17}{10}{Maxpool2d\relax }{figure.caption.17}{}}
\bibcite{zhang2023frc}{1}
\bibcite{ronneberger2015u}{2}
\bibcite{zhang2021accurate}{3}
\bibcite{sandler2018mobilenetv2}{4}
\bibcite{guo2019pipeline}{5}
\bibcite{lin2017feature}{6}
\bibcite{zeiler2014visualizing}{7}
\bibcite{zhu2023biformer}{8}
\bibcite{guo2022segnext}{9}
\bibcite{huang2023channel}{10}
\bibcite{lou2023transxnet}{11}
\newlabel{LastPage}{{}{11}{}{page.11}{}}
\xdef\lastpage@lastpage{11}
\xdef\lastpage@lastpageHy{11}
\gdef \@abspage@last{11}
