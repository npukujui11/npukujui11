\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/D>>}
\citation{liu2021retinex,xu2020learning}
\citation{ueng1995gamma}
\citation{stark2000adaptive}
\citation{land1971lightness}
\citation{liu2021benchmarking}
\citation{dai2019fractional}
\citation{ma2019improved}
\@writefile{toc}{\contentsline {section}{\numberline {1}研究意义}{3}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}研究背景和现状}{3}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}传统低照度图像增强方法}{3}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}基于 Retinex 的方法}{3}{subsubsection.2.1.1}\protected@file@percent }
\newlabel{eq: Retinex model formula}{{2.1}{3}{基于 Retinex 的方法}{equation.2.1}{}}
\citation{cooper2004analysis}
\citation{202013}
\citation{cooper2004analysis}
\citation{cooper2004analysis}
\citation{stark2000adaptive}
\newlabel{eq: Retinex model formula log}{{2.2}{4}{基于 Retinex 的方法}{equation.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces   Retinex 算法处理过程 \relax }}{4}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig: Retinex model}{{1}{4}{Retinex 算法处理过程 \relax }{figure.caption.2}{}}
\newlabel{fig: Retinex Model_input}{{2a}{4}{低照度图像\relax }{figure.caption.3}{}}
\newlabel{sub@fig: Retinex Model_input}{{a}{4}{低照度图像\relax }{figure.caption.3}{}}
\newlabel{fig: Retinex Model_Retinex}{{2b}{4}{Retinex\cite {cooper2004analysis}\relax }{figure.caption.3}{}}
\newlabel{sub@fig: Retinex Model_Retinex}{{b}{4}{Retinex\cite {cooper2004analysis}\relax }{figure.caption.3}{}}
\newlabel{fig: Retinex Model_SSR}{{2c}{4}{SSR\relax }{figure.caption.3}{}}
\newlabel{sub@fig: Retinex Model_SSR}{{c}{4}{SSR\relax }{figure.caption.3}{}}
\newlabel{fig: Retinex Model_MSR}{{2d}{4}{MSR\relax }{figure.caption.3}{}}
\newlabel{sub@fig: Retinex Model_MSR}{{d}{4}{MSR\relax }{figure.caption.3}{}}
\newlabel{fig: Retinex Model_MSRCR}{{2e}{4}{MSRCR\relax }{figure.caption.3}{}}
\newlabel{sub@fig: Retinex Model_MSRCR}{{e}{4}{MSRCR\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces   传统 Retinex 算法实验结果。 \relax }}{4}{figure.caption.3}\protected@file@percent }
\newlabel{fig: Retinex Model}{{2}{4}{传统 Retinex 算法实验结果。 \relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}基于直方图的方法}{5}{subsubsection.2.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces   直方图均衡化示意图 \relax }}{5}{figure.caption.4}\protected@file@percent }
\newlabel{fig: Histogram equalization}{{3}{5}{直方图均衡化示意图 \relax }{figure.caption.4}{}}
\newlabel{eq: Histogram}{{2.3}{5}{基于直方图的方法}{equation.2.3}{}}
\citation{dong2010fast}
\citation{lore2017llnet}
\newlabel{eq: HE}{{2.4}{6}{基于直方图的方法}{equation.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}基于图像反相的方法}{6}{subsubsection.2.1.3}\protected@file@percent }
\newlabel{fig: LL input}{{\caption@xref {fig: LL input}{ on input line 238}}{6}{基于图像反相的方法}{figure.caption.5}{}}
\newlabel{sub@fig: LL input}{{}{6}{基于图像反相的方法}{figure.caption.5}{}}
\newlabel{fig: Inversed}{{\caption@xref {fig: Inversed}{ on input line 243}}{6}{基于图像反相的方法}{figure.caption.5}{}}
\newlabel{sub@fig: Inversed}{{}{6}{基于图像反相的方法}{figure.caption.5}{}}
\newlabel{fig: marked image}{{\caption@xref {fig: marked image}{ on input line 248}}{6}{基于图像反相的方法}{figure.caption.5}{}}
\newlabel{sub@fig: marked image}{{}{6}{基于图像反相的方法}{figure.caption.5}{}}
\newlabel{fig: de-haze}{{\caption@xref {fig: de-haze}{ on input line 253}}{6}{基于图像反相的方法}{figure.caption.5}{}}
\newlabel{sub@fig: de-haze}{{}{6}{基于图像反相的方法}{figure.caption.5}{}}
\newlabel{fig: Final output}{{\caption@xref {fig: Final output}{ on input line 258}}{6}{基于图像反相的方法}{figure.caption.5}{}}
\newlabel{sub@fig: Final output}{{}{6}{基于图像反相的方法}{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces   (a) 低照度图片输入 I. (b) 从输入I获得的反向图片 R.(c) 标记图像：在至少一种颜色（RGB）通道中具有低强度的像素为绿色。 (d) 去雾： 使用公式获得输出 J. (e) 最终得输出结果 E. \relax }}{6}{figure.caption.5}\protected@file@percent }
\newlabel{fig: Image Inverse}{{4}{6}{(a) 低照度图片输入 I. (b) 从输入I获得的反向图片 R.(c) 标记图像：在至少一种颜色（RGB）通道中具有低强度的像素为绿色。 (d) 去雾： 使用公式获得输出 J. (e) 最终得输出结果 E. \relax }{figure.caption.5}{}}
\newlabel{eq: Image Reverse}{{2.5}{6}{基于图像反相的方法}{equation.2.5}{}}
\citation{tang2023low}
\citation{lore2017llnet}
\citation{lv2018mbllen}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}基于深度学习的低照度图像增强方法}{7}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}有监督学习}{7}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{端到端 (End-to-end) 方法}{7}{subsubsection.2.2.1}\protected@file@percent }
\newlabel{fig: Module Structure}{{5a}{7}{Module Structure\relax }{figure.caption.6}{}}
\newlabel{sub@fig: Module Structure}{{a}{7}{Module Structure\relax }{figure.caption.6}{}}
\newlabel{fig: LLNet}{{5b}{7}{LLNet\relax }{figure.caption.6}{}}
\newlabel{sub@fig: LLNet}{{b}{7}{LLNet\relax }{figure.caption.6}{}}
\newlabel{fig: S-LLNet}{{5c}{7}{S-LLNet\relax }{figure.caption.6}{}}
\newlabel{sub@fig: S-LLNet}{{c}{7}{S-LLNet\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces   LLNet结构示意图 \relax }}{7}{figure.caption.6}\protected@file@percent }
\newlabel{fig: LLNet Architecture}{{5}{7}{LLNet结构示意图 \relax }{figure.caption.6}{}}
\citation{wang2018gladnet}
\citation{lu2020tbefn}
\citation{li2021low}
\citation{ravirathinam2021c}
\citation{lim2020dslr}
\citation{wei2018deep}
\citation{shen2017msr}
\citation{zhang2019kindling}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces   MBLLEN 结构图。 \relax }}{8}{figure.caption.7}\protected@file@percent }
\newlabel{fig: MBLLEN Architecture}{{6}{8}{MBLLEN 结构图。 \relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {paragraph}{Retinex 理论方法}{8}{figure.caption.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces   RetinexNet算法结构图。主要由三个部分组成，分解(Decomposition)，调节(Adjustment)和重构(Reconstruction)。 \relax }}{8}{figure.caption.8}\protected@file@percent }
\newlabel{fig: RetinexNet}{{7}{8}{RetinexNet算法结构图。主要由三个部分组成，分解(Decomposition)，调节(Adjustment)和重构(Reconstruction)。 \relax }{figure.caption.8}{}}
\citation{cui2022illumination}
\citation{carion2020end}
\citation{wang2023ultra}
\citation{jiang2021enlightengan}
\citation{fu2022gan}
\citation{ni2020towards}
\citation{zhang2021unsupervised}
\@writefile{toc}{\contentsline {paragraph}{基于 Transformer 的方法}{9}{figure.caption.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}无监督学习}{9}{subsubsection.2.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces   EnlightenGAN 结构图 \relax }}{9}{figure.caption.9}\protected@file@percent }
\newlabel{fig: EnlightenGAN}{{8}{9}{EnlightenGAN 结构图 \relax }{figure.caption.9}{}}
\citation{qiao2021deep}
\citation{robert2018hybridnet}
\citation{zhu2020zero}
\citation{zhang2019zero}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}半监督学习}{10}{subsubsection.2.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces   DRBN 结构图 \relax }}{10}{figure.caption.10}\protected@file@percent }
\newlabel{fig: DRBN}{{9}{10}{DRBN 结构图 \relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4}Zero-Shot 学习}{10}{subsubsection.2.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}研究目的}{10}{subsection.2.3}\protected@file@percent }
\citation{yang2021locally,zhang2020attention}
\citation{li2018multi,zamir2020learning}
\citation{li2020visual}
\citation{xu2023low}
\citation{zhu2020eemefn}
\citation{zhu2020eemefn}
\citation{rana2021edge}
\citation{rana2021edge}
\citation{xu2023low}
\citation{xu2023low}
\citation{chen2018learning}
\citation{xu2023low}
\citation{wang2004image}
\citation{chen2018learning}
\citation{xu2023low}
\citation{wang2004image}
\citation{zhu2020eemefn,rana2021edge}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces   RRDNet 结构图 \relax }}{11}{figure.caption.11}\protected@file@percent }
\newlabel{fig: RRDNet}{{10}{11}{RRDNet 结构图 \relax }{figure.caption.11}{}}
\newlabel{fig: input}{{\caption@xref {fig: input}{ on input line 414}}{11}{研究目的}{figure.caption.12}{}}
\newlabel{sub@fig: input}{{}{11}{研究目的}{figure.caption.12}{}}
\newlabel{fig: LLNet_VE_LOL}{{\caption@xref {fig: LLNet_VE_LOL}{ on input line 420}}{11}{研究目的}{figure.caption.12}{}}
\newlabel{sub@fig: LLNet_VE_LOL}{{}{11}{研究目的}{figure.caption.12}{}}
\newlabel{fig: RetinexNet_VE_LOL}{{\caption@xref {fig: RetinexNet_VE_LOL}{ on input line 426}}{11}{研究目的}{figure.caption.12}{}}
\newlabel{sub@fig: RetinexNet_VE_LOL}{{}{11}{研究目的}{figure.caption.12}{}}
\newlabel{fig: MBLLEN_LOL}{{\caption@xref {fig: MBLLEN_LOL}{ on input line 432}}{11}{研究目的}{figure.caption.12}{}}
\newlabel{sub@fig: MBLLEN_LOL}{{}{11}{研究目的}{figure.caption.12}{}}
\newlabel{fig: EnlightenGAN_VE_LOL}{{\caption@xref {fig: EnlightenGAN_VE_LOL}{ on input line 438}}{11}{研究目的}{figure.caption.12}{}}
\newlabel{sub@fig: EnlightenGAN_VE_LOL}{{}{11}{研究目的}{figure.caption.12}{}}
\newlabel{fig: RRDNet_VE_LOL}{{\caption@xref {fig: RRDNet_VE_LOL}{ on input line 444}}{11}{研究目的}{figure.caption.12}{}}
\newlabel{sub@fig: RRDNet_VE_LOL}{{}{11}{研究目的}{figure.caption.12}{}}
\newlabel{fig: DRBN_VE_LOL}{{\caption@xref {fig: DRBN_VE_LOL}{ on input line 450}}{11}{研究目的}{figure.caption.12}{}}
\newlabel{sub@fig: DRBN_VE_LOL}{{}{11}{研究目的}{figure.caption.12}{}}
\newlabel{fig: Zero-DCE++}{{\caption@xref {fig: Zero-DCE++}{ on input line 456}}{11}{研究目的}{figure.caption.12}{}}
\newlabel{sub@fig: Zero-DCE++}{{}{11}{研究目的}{figure.caption.12}{}}
\newlabel{fig: KinD++}{{\caption@xref {fig: KinD++}{ on input line 462}}{11}{研究目的}{figure.caption.12}{}}
\newlabel{sub@fig: KinD++}{{}{11}{研究目的}{figure.caption.12}{}}
\newlabel{fig: URetinexNet}{{\caption@xref {fig: URetinexNet}{ on input line 468}}{11}{研究目的}{figure.caption.12}{}}
\newlabel{sub@fig: URetinexNet}{{}{11}{研究目的}{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces   不同算法对从 VE-LOL-L 数据集采样的低照度图像的可视化结果。 \relax }}{11}{figure.caption.12}\protected@file@percent }
\newlabel{fig: VE-LOL-L Visual}{{11}{11}{不同算法对从 VE-LOL-L 数据集采样的低照度图像的可视化结果。 \relax }{figure.caption.12}{}}
\citation{xu2023low}
\citation{pietikainen2010local}
\newlabel{fig: EEMEFN}{{12a}{12}{该 LLIE 结构源自\cite {zhu2020eemefn},如其 Multi-Exposure Fusion 部分采用多曝光融合结构,与由 Initial image 生成的边缘图进行 Concat, 后续通过一个 U-Net 网络进一步恢复图像。\relax }{figure.caption.13}{}}
\newlabel{sub@fig: EEMEFN}{{a}{12}{该 LLIE 结构源自\cite {zhu2020eemefn},如其 Multi-Exposure Fusion 部分采用多曝光融合结构,与由 Initial image 生成的边缘图进行 Concat, 后续通过一个 U-Net 网络进一步恢复图像。\relax }{figure.caption.13}{}}
\newlabel{fig: EdgeNet}{{12b}{12}{该 LLIE 结构源自\cite {rana2021edge}使用 EdgeNet 首先从低光图像中过滤边缘，EnhanceNet 反复使用上采样和下采样块的组合，从局部到全局逐渐提取特征，并消除伪影和噪声。\relax }{figure.caption.13}{}}
\newlabel{sub@fig: EdgeNet}{{b}{12}{该 LLIE 结构源自\cite {rana2021edge}使用 EdgeNet 首先从低光图像中过滤边缘，EnhanceNet 反复使用上采样和下采样块的组合，从局部到全局逐渐提取特征，并消除伪影和噪声。\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces   边缘图像指导弱光图像增强的传统架构。 \relax }}{12}{figure.caption.13}\protected@file@percent }
\newlabel{fig: Traditional Architecture}{{12}{12}{边缘图像指导弱光图像增强的传统架构。 \relax }{figure.caption.13}{}}
\newlabel{fig: SMG-LLIE Architecture}{{13a}{12}{该 LLIE 结构源自\cite {xu2023low}，其提出一种基于 GAN Loss 的模型去对结构信息建模，通过获得的结构信息指导增强。\relax }{figure.caption.14}{}}
\newlabel{sub@fig: SMG-LLIE Architecture}{{a}{12}{该 LLIE 结构源自\cite {xu2023low}，其提出一种基于 GAN Loss 的模型去对结构信息建模，通过获得的结构信息指导增强。\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces   边缘图像指导弱光图像增强的最新架构(CVPR 2023)。 \relax }}{12}{figure.caption.14}\protected@file@percent }
\newlabel{fig: SMG-LLIE Overview}{{13}{12}{边缘图像指导弱光图像增强的最新架构(CVPR 2023)。 \relax }{figure.caption.14}{}}
\citation{maini2009study}
\citation{liu2017richer}
\citation{xu2023low}
\newlabel{fig: Input}{{14a}{13}{Input\relax }{figure.caption.15}{}}
\newlabel{sub@fig: Input}{{a}{13}{Input\relax }{figure.caption.15}{}}
\newlabel{fig: Structure of (a)}{{14b}{13}{Structure of (a)\relax }{figure.caption.15}{}}
\newlabel{sub@fig: Structure of (a)}{{b}{13}{Structure of (a)\relax }{figure.caption.15}{}}
\newlabel{fig: SNR (CVPR 2022)}{{14c}{13}{SNR (CVPR 2022)\relax }{figure.caption.15}{}}
\newlabel{sub@fig: SNR (CVPR 2022)}{{c}{13}{SNR (CVPR 2022)\relax }{figure.caption.15}{}}
\newlabel{fig: Structure Modeling}{{14d}{13}{Structure Modeling\relax }{figure.caption.15}{}}
\newlabel{sub@fig: Structure Modeling}{{d}{13}{Structure Modeling\relax }{figure.caption.15}{}}
\newlabel{fig: SMG-LLIE}{{14e}{13}{SMG-LLIE\relax }{figure.caption.15}{}}
\newlabel{sub@fig: SMG-LLIE}{{e}{13}{SMG-LLIE\relax }{figure.caption.15}{}}
\newlabel{fig: Ground Truth}{{14f}{13}{Ground Truth\relax }{figure.caption.15}{}}
\newlabel{sub@fig: Ground Truth}{{f}{13}{Ground Truth\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces   SID-sRGB\cite  {chen2018learning}中一张弱光图片, 通过SOTA方法 (c) 和\cite  {xu2023low}提出的方法 (e)增强。作者的方法可以从输入的图像中合成结构图(d)，使细节更清晰，对比度更清晰，颜色更鲜艳。虽然(c)的 PSNR 为 28.17，但其 SSIM 低为 0.75。作者的方法在dB和SSIM\cite  {wang2004image}的得分都很高，分别为28.60 dB 和 0.80。 \relax }}{13}{figure.caption.15}\protected@file@percent }
\newlabel{fig: Structural Information}{{14}{13}{SID-sRGB\cite {chen2018learning}中一张弱光图片, 通过SOTA方法 (c) 和\cite {xu2023low}提出的方法 (e)增强。作者的方法可以从输入的图像中合成结构图(d)，使细节更清晰，对比度更清晰，颜色更鲜艳。虽然(c)的 PSNR 为 28.17，但其 SSIM 低为 0.75。作者的方法在dB和SSIM\cite {wang2004image}的得分都很高，分别为28.60 dB 和 0.80。 \relax }{figure.caption.15}{}}
\citation{woo2018cbam}
\citation{ramachandran2019stand}
\citation{woo2018cbam}
\citation{li2023scconv}
\@writefile{toc}{\contentsline {section}{\numberline {3}采用的方法}{14}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}数据集}{14}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}文献调研与支撑}{14}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}已完成工作}{14}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}下一步工作}{14}{section.7}\protected@file@percent }
\citation{chen2018learning}
\citation{chen2018learning,zamir2021learning}
\citation{chen2018learning}
\citation{chen2018learning,zamir2021learning}
\citation{meng2020gia}
\citation{chen2018learning,zamir2021learning}
\citation{meng2020gia}
\citation{chen2018learning,meng2020gia,zamir2021learning}
\citation{zhou2018unet++,zhou2019unet++}
\citation{yang2021locally,zhang2020attention}
\citation{li2018multi,zamir2020learning}
\citation{li2020visual}
\citation{xu2020learning}
\@writefile{toc}{\contentsline {paragraph}{U-Net for LLIE}{15}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{CNN for LLIE}{15}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}方向}{15}{section.8}\protected@file@percent }
\citation{wang2022uformer}
\citation{peng2021conformer}
\citation{peng2021conformer}
\citation{wang2022uformer}
\citation{li2023effective}
\citation{li2023effective}
\citation{xu2023low}
\citation{zhu2020eemefn}
\citation{rana2021edge}
\bibstyle{unsrt}
\bibdata{reference}
\bibcite{liu2021retinex}{1}
\bibcite{xu2020learning}{2}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}方向一}{16}{subsection.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}方向二}{16}{subsection.8.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces   Low-Lighting 图像恢复网络。结构受 Conformer\cite  {peng2021conformer} （见 Fig. \ref {fig: Conformer architecture}）启发，将原来结构中 CNN 分支的 ResNet 结构修改为 U-Net 结构。其采用一个 U-Net 和 ViT 的并行架构，通过 U-Net 结构得到一个弱恢复的弱特征图 $\mathcal  {F}_2$，通过 ViT 融合的特征可以初步增强弱特征图 $\mathcal  {F}_2$，ViT 的输出经过 Patch Expanding 的特征图 $\mathcal  {F}_3$ 经过与 U-Net 输出的特征图 $\mathcal  {F}_2$ 融合之后得到一个初步恢复的图片 output image，用以后续参与图片的进一步恢复。 \relax }}{17}{figure.caption.18}\protected@file@percent }
\newlabel{fig: The proposed architecture}{{15}{17}{Low-Lighting 图像恢复网络。结构受 Conformer\cite {peng2021conformer} （见 Fig. \ref {fig: Conformer architecture}）启发，将原来结构中 CNN 分支的 ResNet 结构修改为 U-Net 结构。其采用一个 U-Net 和 ViT 的并行架构，通过 U-Net 结构得到一个弱恢复的弱特征图 $\mathcal {F}_2$，通过 ViT 融合的特征可以初步增强弱特征图 $\mathcal {F}_2$，ViT 的输出经过 Patch Expanding 的特征图 $\mathcal {F}_3$ 经过与 U-Net 输出的特征图 $\mathcal {F}_2$ 融合之后得到一个初步恢复的图片 output image，用以后续参与图片的进一步恢复。 \relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces   Uformer 的结构。 \relax }}{17}{figure.caption.19}\protected@file@percent }
\newlabel{fig: Uformer}{{16}{17}{Uformer 的结构。 \relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces   MCLNet 的结构 \relax }}{18}{figure.caption.20}\protected@file@percent }
\newlabel{fig: MCLNet}{{17}{18}{MCLNet 的结构 \relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces   该结构的边缘图直接从 $I$ 中通过 StyleGAN 得到边缘图 $I_s$, 而非从 $I_{\alpha }$ 中通过边缘网络获取边缘图。 \relax }}{18}{figure.caption.21}\protected@file@percent }
\newlabel{fig: Overview}{{18}{18}{该结构的边缘图直接从 $I$ 中通过 StyleGAN 得到边缘图 $I_s$, 而非从 $I_{\alpha }$ 中通过边缘网络获取边缘图。 \relax }{figure.caption.21}{}}
\bibcite{ueng1995gamma}{3}
\bibcite{stark2000adaptive}{4}
\bibcite{land1971lightness}{5}
\bibcite{liu2021benchmarking}{6}
\bibcite{dai2019fractional}{7}
\bibcite{ma2019improved}{8}
\bibcite{cooper2004analysis}{9}
\bibcite{202013}{10}
\bibcite{dong2010fast}{11}
\bibcite{lore2017llnet}{12}
\bibcite{tang2023low}{13}
\bibcite{lv2018mbllen}{14}
\bibcite{wang2018gladnet}{15}
\bibcite{lu2020tbefn}{16}
\bibcite{li2021low}{17}
\bibcite{ravirathinam2021c}{18}
\bibcite{lim2020dslr}{19}
\bibcite{wei2018deep}{20}
\bibcite{shen2017msr}{21}
\bibcite{zhang2019kindling}{22}
\bibcite{cui2022illumination}{23}
\bibcite{carion2020end}{24}
\bibcite{wang2023ultra}{25}
\bibcite{jiang2021enlightengan}{26}
\bibcite{fu2022gan}{27}
\bibcite{ni2020towards}{28}
\bibcite{zhang2021unsupervised}{29}
\bibcite{qiao2021deep}{30}
\bibcite{robert2018hybridnet}{31}
\bibcite{zhu2020zero}{32}
\bibcite{zhang2019zero}{33}
\bibcite{yang2021locally}{34}
\bibcite{zhang2020attention}{35}
\bibcite{li2018multi}{36}
\bibcite{zamir2020learning}{37}
\bibcite{li2020visual}{38}
\bibcite{xu2023low}{39}
\bibcite{zhu2020eemefn}{40}
\bibcite{rana2021edge}{41}
\bibcite{chen2018learning}{42}
\bibcite{wang2004image}{43}
\bibcite{pietikainen2010local}{44}
\bibcite{maini2009study}{45}
\bibcite{liu2017richer}{46}
\bibcite{woo2018cbam}{47}
\bibcite{ramachandran2019stand}{48}
\bibcite{li2023scconv}{49}
\bibcite{zamir2021learning}{50}
\bibcite{meng2020gia}{51}
\bibcite{zhou2018unet++}{52}
\bibcite{zhou2019unet++}{53}
\bibcite{wang2022uformer}{54}
\bibcite{peng2021conformer}{55}
\bibcite{li2023effective}{56}
\newlabel{LastPage}{{}{22}{}{page.22}{}}
\xdef\lastpage@lastpage{22}
\xdef\lastpage@lastpageHy{22}
\gdef \@abspage@last{22}
