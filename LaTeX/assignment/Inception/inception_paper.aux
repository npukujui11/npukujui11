\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/D>>}
\citation{liu2021retinex,xu2020learning}
\citation{ueng1995gamma}
\citation{stark2000adaptive}
\citation{land1971lightness}
\citation{liu2021benchmarking}
\citation{dai2019fractional}
\citation{ma2019improved}
\@writefile{toc}{\contentsline {section}{\numberline {1}研究意义}{4}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}研究背景和现状}{4}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}传统低照度图像增强方法}{4}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}基于 Retinex 的方法}{4}{subsubsection.2.1.1}\protected@file@percent }
\newlabel{eq: Retinex model formula}{{2.1}{4}{基于 Retinex 的方法}{equation.2.1}{}}
\newlabel{eq: Retinex model formula log}{{2.2}{4}{基于 Retinex 的方法}{equation.2.2}{}}
\citation{cooper2004analysis}
\citation{202013}
\citation{cooper2004analysis}
\citation{cooper2004analysis}
\citation{stark2000adaptive}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces   Retinex 算法处理过程 \relax }}{5}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig: Retinex model}{{1}{5}{Retinex 算法处理过程 \relax }{figure.caption.2}{}}
\newlabel{fig: Retinex Model_input}{{2a}{5}{低照度图像\relax }{figure.caption.3}{}}
\newlabel{sub@fig: Retinex Model_input}{{a}{5}{低照度图像\relax }{figure.caption.3}{}}
\newlabel{fig: Retinex Model_Retinex}{{2b}{5}{Retinex\cite {cooper2004analysis}\relax }{figure.caption.3}{}}
\newlabel{sub@fig: Retinex Model_Retinex}{{b}{5}{Retinex\cite {cooper2004analysis}\relax }{figure.caption.3}{}}
\newlabel{fig: Retinex Model_SSR}{{2c}{5}{SSR\relax }{figure.caption.3}{}}
\newlabel{sub@fig: Retinex Model_SSR}{{c}{5}{SSR\relax }{figure.caption.3}{}}
\newlabel{fig: Retinex Model_MSR}{{2d}{5}{MSR\relax }{figure.caption.3}{}}
\newlabel{sub@fig: Retinex Model_MSR}{{d}{5}{MSR\relax }{figure.caption.3}{}}
\newlabel{fig: Retinex Model_MSRCR}{{2e}{5}{MSRCR\relax }{figure.caption.3}{}}
\newlabel{sub@fig: Retinex Model_MSRCR}{{e}{5}{MSRCR\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces   传统 Retinex 算法实验结果。 \relax }}{5}{figure.caption.3}\protected@file@percent }
\newlabel{fig: Retinex Model}{{2}{5}{传统 Retinex 算法实验结果。 \relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}基于直方图的方法}{5}{subsubsection.2.1.2}\protected@file@percent }
\newlabel{eq: Histogram}{{2.3}{5}{基于直方图的方法}{equation.2.3}{}}
\citation{dong2010fast}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces   直方图均衡化示意图 \relax }}{6}{figure.caption.4}\protected@file@percent }
\newlabel{fig: Histogram equalization}{{3}{6}{直方图均衡化示意图 \relax }{figure.caption.4}{}}
\newlabel{eq: HE}{{2.4}{6}{基于直方图的方法}{equation.2.4}{}}
\citation{lore2017llnet}
\citation{tang2023low}
\citation{lore2017llnet}
\citation{lv2018mbllen}
\citation{wang2018gladnet}
\citation{lu2020tbefn}
\citation{li2021low}
\citation{ravirathinam2021c}
\citation{lim2020dslr}
\newlabel{fig: LL input}{{\caption@xref {fig: LL input}{ on input line 248}}{7}{基于图像反相的方法}{figure.caption.5}{}}
\newlabel{sub@fig: LL input}{{}{7}{基于图像反相的方法}{figure.caption.5}{}}
\newlabel{fig: Inversed}{{\caption@xref {fig: Inversed}{ on input line 253}}{7}{基于图像反相的方法}{figure.caption.5}{}}
\newlabel{sub@fig: Inversed}{{}{7}{基于图像反相的方法}{figure.caption.5}{}}
\newlabel{fig: marked image}{{\caption@xref {fig: marked image}{ on input line 258}}{7}{基于图像反相的方法}{figure.caption.5}{}}
\newlabel{sub@fig: marked image}{{}{7}{基于图像反相的方法}{figure.caption.5}{}}
\newlabel{fig: de-haze}{{\caption@xref {fig: de-haze}{ on input line 263}}{7}{基于图像反相的方法}{figure.caption.5}{}}
\newlabel{sub@fig: de-haze}{{}{7}{基于图像反相的方法}{figure.caption.5}{}}
\newlabel{fig: Final output}{{\caption@xref {fig: Final output}{ on input line 268}}{7}{基于图像反相的方法}{figure.caption.5}{}}
\newlabel{sub@fig: Final output}{{}{7}{基于图像反相的方法}{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces   (a) 低照度图片输入 I. (b) 从输入I获得的反向图片 R.(c) 标记图像：在至少一种颜色（RGB）通道中具有低强度的像素为绿色。 (d) 去雾： 使用公式获得输出 J. (e) 最终得输出结果 E. \relax }}{7}{figure.caption.5}\protected@file@percent }
\newlabel{fig: Image Inverse}{{4}{7}{(a) 低照度图片输入 I. (b) 从输入I获得的反向图片 R.(c) 标记图像：在至少一种颜色（RGB）通道中具有低强度的像素为绿色。 (d) 去雾： 使用公式获得输出 J. (e) 最终得输出结果 E. \relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}基于图像反相的方法}{7}{subsubsection.2.1.3}\protected@file@percent }
\newlabel{eq: Image Reverse}{{2.5}{7}{基于图像反相的方法}{equation.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}基于深度学习的低照度图像增强方法}{7}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}有监督学习}{7}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{端到端 (End-to-end) 方法}{7}{subsubsection.2.2.1}\protected@file@percent }
\citation{wei2018deep}
\citation{shen2017msr}
\citation{zhang2019kindling}
\citation{cui2022illumination}
\citation{carion2020end}
\citation{wang2023ultra}
\newlabel{fig: Module Structure}{{5a}{8}{Module Structure\relax }{figure.caption.6}{}}
\newlabel{sub@fig: Module Structure}{{a}{8}{Module Structure\relax }{figure.caption.6}{}}
\newlabel{fig: LLNet}{{5b}{8}{LLNet\relax }{figure.caption.6}{}}
\newlabel{sub@fig: LLNet}{{b}{8}{LLNet\relax }{figure.caption.6}{}}
\newlabel{fig: S-LLNet}{{5c}{8}{S-LLNet\relax }{figure.caption.6}{}}
\newlabel{sub@fig: S-LLNet}{{c}{8}{S-LLNet\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces   LLNet结构示意图 \relax }}{8}{figure.caption.6}\protected@file@percent }
\newlabel{fig: LLNet Architecture}{{5}{8}{LLNet结构示意图 \relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Retinex 理论方法}{8}{figure.caption.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{基于 Transformer 的方法}{8}{figure.caption.8}\protected@file@percent }
\citation{jiang2021enlightengan}
\citation{fu2022gan}
\citation{ni2020towards}
\citation{zhang2021unsupervised}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces   MBLLEN 结构图。 \relax }}{9}{figure.caption.7}\protected@file@percent }
\newlabel{fig: MBLLEN Architecture}{{6}{9}{MBLLEN 结构图。 \relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces   RetinexNet算法结构图。主要由三个部分组成，分解(Decomposition)，调节(Adjustment)和重构(Reconstruction)。 \relax }}{9}{figure.caption.8}\protected@file@percent }
\newlabel{fig: RetinexNet}{{7}{9}{RetinexNet算法结构图。主要由三个部分组成，分解(Decomposition)，调节(Adjustment)和重构(Reconstruction)。 \relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}无监督学习}{9}{subsubsection.2.2.2}\protected@file@percent }
\citation{qiao2021deep}
\citation{robert2018hybridnet}
\citation{zhu2020zero}
\citation{zhang2019zero}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces   EnlightenGAN 结构图 \relax }}{10}{figure.caption.9}\protected@file@percent }
\newlabel{fig: EnlightenGAN}{{8}{10}{EnlightenGAN 结构图 \relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}半监督学习}{10}{subsubsection.2.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces   DRBN 结构图 \relax }}{10}{figure.caption.10}\protected@file@percent }
\newlabel{fig: DRBN}{{9}{10}{DRBN 结构图 \relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4}Zero-Shot 学习}{10}{subsubsection.2.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces   RRDNet 结构图 \relax }}{11}{figure.caption.11}\protected@file@percent }
\newlabel{fig: RRDNet}{{10}{11}{RRDNet 结构图 \relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}数据集}{11}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}评价指标}{11}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}客观评价指标}{11}{subsubsection.2.4.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces  图像的客观质量评价指标 \relax }}{11}{table.caption.12}\protected@file@percent }
\newlabel{tab: quality evaluation index}{{1}{11}{图像的客观质量评价指标 \relax }{table.caption.12}{}}
\@writefile{toc}{\contentsline {paragraph}{峰值信噪比 (Peak-Signal to Noise Ratio, PSNR)}{12}{table.caption.12}\protected@file@percent }
\newlabel{eq: PSNR}{{2.6}{12}{峰值信噪比 (Peak-Signal to Noise Ratio, PSNR)}{equation.2.6}{}}
\@writefile{toc}{\contentsline {paragraph}{结构相似性 (Structural Similarity, SSIM)}{12}{equation.2.6}\protected@file@percent }
\newlabel{eq: SSIM_eq}{{2.7}{12}{结构相似性 (Structural Similarity, SSIM)}{equation.2.7}{}}
\newlabel{eq: l(x，y)}{{2.8}{12}{结构相似性 (Structural Similarity, SSIM)}{equation.2.8}{}}
\newlabel{eq: C(x，y)}{{2.9}{12}{结构相似性 (Structural Similarity, SSIM)}{equation.2.9}{}}
\newlabel{eq: (x, y)}{{2.10}{12}{结构相似性 (Structural Similarity, SSIM)}{equation.2.10}{}}
\@writefile{toc}{\contentsline {paragraph}{均方误差 (Mean Square Error, MSE)}{12}{equation.2.10}\protected@file@percent }
\newlabel{eq: MSE}{{2.11}{12}{均方误差 (Mean Square Error, MSE)}{equation.2.11}{}}
\citation{yang2021locally,zhang2020attention}
\citation{li2018multi,zamir2020learning}
\citation{li2020visual}
\@writefile{toc}{\contentsline {paragraph}{信息熵 (Information Entropy, IE)}{13}{equation.2.11}\protected@file@percent }
\newlabel{eq: IE}{{2.12}{13}{信息熵 (Information Entropy, IE)}{equation.2.12}{}}
\@writefile{toc}{\contentsline {paragraph}{标准偏差 (Standard Deviation, STD)}{13}{equation.2.12}\protected@file@percent }
\newlabel{eq: STD}{{2.13}{13}{标准偏差 (Standard Deviation, STD)}{equation.2.13}{}}
\newlabel{eq: delta}{{2.14}{13}{标准偏差 (Standard Deviation, STD)}{equation.2.14}{}}
\@writefile{toc}{\contentsline {paragraph}{亮度顺序误差 (Lightness Order Error, LOE)}{13}{equation.2.14}\protected@file@percent }
\newlabel{eq: LOE}{{2.15}{13}{亮度顺序误差 (Lightness Order Error, LOE)}{equation.2.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2}主观评价指标}{13}{subsubsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{差分平均意见分数 (Differential Mean Opinion Score, MOS)}{13}{subsubsection.2.4.2}\protected@file@percent }
\newlabel{eq: MOS}{{2.16}{13}{差分平均意见分数 (Differential Mean Opinion Score, MOS)}{equation.2.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}研究目的}{13}{subsection.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.1}现有方法的局限性}{13}{subsubsection.2.5.1}\protected@file@percent }
\citation{xu2023low}
\citation{zhu2020eemefn}
\citation{zhu2020eemefn}
\citation{rana2021edge}
\citation{rana2021edge}
\citation{xu2023low}
\citation{xu2023low}
\citation{chen2018learning}
\citation{xu2023low}
\citation{wang2004image}
\citation{chen2018learning}
\citation{xu2023low}
\citation{wang2004image}
\citation{zhu2020eemefn,rana2021edge}
\newlabel{fig: input}{{\caption@xref {fig: input}{ on input line 598}}{14}{现有方法的局限性}{figure.caption.13}{}}
\newlabel{sub@fig: input}{{}{14}{现有方法的局限性}{figure.caption.13}{}}
\newlabel{fig: LLNet_VE_LOL}{{\caption@xref {fig: LLNet_VE_LOL}{ on input line 604}}{14}{现有方法的局限性}{figure.caption.13}{}}
\newlabel{sub@fig: LLNet_VE_LOL}{{}{14}{现有方法的局限性}{figure.caption.13}{}}
\newlabel{fig: RetinexNet_VE_LOL}{{\caption@xref {fig: RetinexNet_VE_LOL}{ on input line 610}}{14}{现有方法的局限性}{figure.caption.13}{}}
\newlabel{sub@fig: RetinexNet_VE_LOL}{{}{14}{现有方法的局限性}{figure.caption.13}{}}
\newlabel{fig: MBLLEN_LOL}{{\caption@xref {fig: MBLLEN_LOL}{ on input line 616}}{14}{现有方法的局限性}{figure.caption.13}{}}
\newlabel{sub@fig: MBLLEN_LOL}{{}{14}{现有方法的局限性}{figure.caption.13}{}}
\newlabel{fig: EnlightenGAN_VE_LOL}{{\caption@xref {fig: EnlightenGAN_VE_LOL}{ on input line 622}}{14}{现有方法的局限性}{figure.caption.13}{}}
\newlabel{sub@fig: EnlightenGAN_VE_LOL}{{}{14}{现有方法的局限性}{figure.caption.13}{}}
\newlabel{fig: RRDNet_VE_LOL}{{\caption@xref {fig: RRDNet_VE_LOL}{ on input line 628}}{14}{现有方法的局限性}{figure.caption.13}{}}
\newlabel{sub@fig: RRDNet_VE_LOL}{{}{14}{现有方法的局限性}{figure.caption.13}{}}
\newlabel{fig: DRBN_VE_LOL}{{\caption@xref {fig: DRBN_VE_LOL}{ on input line 634}}{14}{现有方法的局限性}{figure.caption.13}{}}
\newlabel{sub@fig: DRBN_VE_LOL}{{}{14}{现有方法的局限性}{figure.caption.13}{}}
\newlabel{fig: Zero-DCE++}{{\caption@xref {fig: Zero-DCE++}{ on input line 640}}{14}{现有方法的局限性}{figure.caption.13}{}}
\newlabel{sub@fig: Zero-DCE++}{{}{14}{现有方法的局限性}{figure.caption.13}{}}
\newlabel{fig: KinD++}{{\caption@xref {fig: KinD++}{ on input line 646}}{14}{现有方法的局限性}{figure.caption.13}{}}
\newlabel{sub@fig: KinD++}{{}{14}{现有方法的局限性}{figure.caption.13}{}}
\newlabel{fig: URetinexNet}{{\caption@xref {fig: URetinexNet}{ on input line 652}}{14}{现有方法的局限性}{figure.caption.13}{}}
\newlabel{sub@fig: URetinexNet}{{}{14}{现有方法的局限性}{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces   不同算法对从 VE-LOL-L 数据集采样的低照度图像的可视化结果。 \relax }}{14}{figure.caption.13}\protected@file@percent }
\newlabel{fig: VE-LOL-L Visual}{{11}{14}{不同算法对从 VE-LOL-L 数据集采样的低照度图像的可视化结果。 \relax }{figure.caption.13}{}}
\newlabel{fig: EEMEFN}{{12a}{14}{该 LLIE 结构源自\cite {zhu2020eemefn},如其 Multi-Exposure Fusion 部分采用多曝光融合结构,与由 Initial image 生成的边缘图进行 Concat, 后续通过一个 U-Net 网络进一步恢复图像。\relax }{figure.caption.14}{}}
\newlabel{sub@fig: EEMEFN}{{a}{14}{该 LLIE 结构源自\cite {zhu2020eemefn},如其 Multi-Exposure Fusion 部分采用多曝光融合结构,与由 Initial image 生成的边缘图进行 Concat, 后续通过一个 U-Net 网络进一步恢复图像。\relax }{figure.caption.14}{}}
\newlabel{fig: EdgeNet}{{12b}{14}{该 LLIE 结构源自\cite {rana2021edge}使用 EdgeNet 首先从低光图像中过滤边缘，EnhanceNet 反复使用上采样和下采样块的组合，从局部到全局逐渐提取特征，并消除伪影和噪声。\relax }{figure.caption.14}{}}
\newlabel{sub@fig: EdgeNet}{{b}{14}{该 LLIE 结构源自\cite {rana2021edge}使用 EdgeNet 首先从低光图像中过滤边缘，EnhanceNet 反复使用上采样和下采样块的组合，从局部到全局逐渐提取特征，并消除伪影和噪声。\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces   边缘图像指导弱光图像增强的传统架构。 \relax }}{14}{figure.caption.14}\protected@file@percent }
\newlabel{fig: Traditional Architecture}{{12}{14}{边缘图像指导弱光图像增强的传统架构。 \relax }{figure.caption.14}{}}
\citation{xu2023low}
\citation{pietikainen2010local}
\citation{maini2009study}
\citation{liu2017richer}
\citation{xu2023low}
\newlabel{fig: SMG-LLIE Architecture}{{13a}{15}{该 LLIE 结构源自\cite {xu2023low}，其提出一种基于 GAN Loss 的模型去对结构信息建模，通过获得的结构信息指导增强。\relax }{figure.caption.15}{}}
\newlabel{sub@fig: SMG-LLIE Architecture}{{a}{15}{该 LLIE 结构源自\cite {xu2023low}，其提出一种基于 GAN Loss 的模型去对结构信息建模，通过获得的结构信息指导增强。\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces   边缘图像指导弱光图像增强的最新架构(CVPR 2023)。 \relax }}{15}{figure.caption.15}\protected@file@percent }
\newlabel{fig: SMG-LLIE Overview}{{13}{15}{边缘图像指导弱光图像增强的最新架构(CVPR 2023)。 \relax }{figure.caption.15}{}}
\citation{woo2018cbam}
\citation{ramachandran2019stand}
\citation{woo2018cbam}
\citation{li2023scconv}
\citation{chen2018learning}
\citation{chen2018learning,zamir2021learning}
\citation{chen2018learning}
\citation{chen2018learning,zamir2021learning}
\citation{meng2020gia}
\citation{chen2018learning,zamir2021learning}
\citation{meng2020gia}
\citation{chen2018learning,meng2020gia,zamir2021learning}
\newlabel{fig: Input}{{14a}{16}{Input\relax }{figure.caption.16}{}}
\newlabel{sub@fig: Input}{{a}{16}{Input\relax }{figure.caption.16}{}}
\newlabel{fig: Structure of (a)}{{14b}{16}{Structure of (a)\relax }{figure.caption.16}{}}
\newlabel{sub@fig: Structure of (a)}{{b}{16}{Structure of (a)\relax }{figure.caption.16}{}}
\newlabel{fig: SNR (CVPR 2022)}{{14c}{16}{SNR (CVPR 2022)\relax }{figure.caption.16}{}}
\newlabel{sub@fig: SNR (CVPR 2022)}{{c}{16}{SNR (CVPR 2022)\relax }{figure.caption.16}{}}
\newlabel{fig: Structure Modeling}{{14d}{16}{Structure Modeling\relax }{figure.caption.16}{}}
\newlabel{sub@fig: Structure Modeling}{{d}{16}{Structure Modeling\relax }{figure.caption.16}{}}
\newlabel{fig: SMG-LLIE}{{14e}{16}{SMG-LLIE\relax }{figure.caption.16}{}}
\newlabel{sub@fig: SMG-LLIE}{{e}{16}{SMG-LLIE\relax }{figure.caption.16}{}}
\newlabel{fig: Ground Truth}{{14f}{16}{Ground Truth\relax }{figure.caption.16}{}}
\newlabel{sub@fig: Ground Truth}{{f}{16}{Ground Truth\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces   SID-sRGB\cite  {chen2018learning}中一张弱光图片, 通过SOTA方法 (c) 和\cite  {xu2023low}提出的方法 (e)增强。作者的方法可以从输入的图像中合成结构图(d)，使细节更清晰，对比度更清晰，颜色更鲜艳。虽然(c)的 PSNR 为 28.17，但其 SSIM 低为 0.75。作者的方法在dB和SSIM\cite  {wang2004image}的得分都很高，分别为28.60 dB 和 0.80。 \relax }}{16}{figure.caption.16}\protected@file@percent }
\newlabel{fig: Structural Information}{{14}{16}{SID-sRGB\cite {chen2018learning}中一张弱光图片, 通过SOTA方法 (c) 和\cite {xu2023low}提出的方法 (e)增强。作者的方法可以从输入的图像中合成结构图(d)，使细节更清晰，对比度更清晰，颜色更鲜艳。虽然(c)的 PSNR 为 28.17，但其 SSIM 低为 0.75。作者的方法在dB和SSIM\cite {wang2004image}的得分都很高，分别为28.60 dB 和 0.80。 \relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}采用的方法}{16}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}注意力机制}{16}{subsection.3.1}\protected@file@percent }
\citation{zhou2018unet++,zhou2019unet++}
\citation{yang2021locally,zhang2020attention}
\citation{li2018multi,zamir2020learning}
\citation{li2020visual}
\citation{xu2020learning}
\citation{vaswani2017attention}
\citation{dosovitskiy2020image}
\citation{wang2022ultrahighdefinition}
\citation{wang2021uformer}
\citation{chen2023cross}
\citation{jain1991unsupervised,lowe2004distinctive,ojala2002multiresolution}
\citation{lisin2005combining}
\citation{peng2021conformer}
\citation{peng2021conformer}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}U-Net网络结构}{17}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}CNN网络结构}{17}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Transformer网络结构}{17}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}具体的方法}{17}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}初步恢复图像的生成模型}{17}{subsection.4.1}\protected@file@percent }
\citation{peng2021conformer}
\citation{jain1991unsupervised,lowe2004distinctive,ojala2002multiresolution}
\citation{lisin2005combining}
\citation{karu1996there}
\citation{peng2021conformer}
\citation{peng2021conformer}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces   作者\cite  {peng2021conformer}提出的网络结构。(a) 特征图和补丁嵌入用于空间对齐的上采样和下采样。(b) CNN块、Transformer块和特征耦合单元（FCU）的实现细节。(c) 缩略图。 \relax }}{18}{figure.caption.18}\protected@file@percent }
\newlabel{fig: Conformer}{{15}{18}{作者\cite {peng2021conformer}提出的网络结构。(a) 特征图和补丁嵌入用于空间对齐的上采样和下采样。(b) CNN块、Transformer块和特征耦合单元（FCU）的实现细节。(c) 缩略图。 \relax }{figure.caption.18}{}}
\citation{woo2018cbam}
\newlabel{fig: First Architecture}{{16a}{19}{PACUT\relax }{figure.caption.19}{}}
\newlabel{sub@fig: First Architecture}{{a}{19}{PACUT\relax }{figure.caption.19}{}}
\newlabel{fig: Up-sampling and down-sampling}{{16b}{19}{Up-sampling and Down-sampling\relax }{figure.caption.19}{}}
\newlabel{sub@fig: Up-sampling and down-sampling}{{b}{19}{Up-sampling and Down-sampling\relax }{figure.caption.19}{}}
\newlabel{fig: The proposed initial architecture(Abstract Picture)}{{16c}{19}{Thumbnail of PACUT\relax }{figure.caption.19}{}}
\newlabel{sub@fig: The proposed initial architecture(Abstract Picture)}{{c}{19}{Thumbnail of PACUT\relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces   我们提出的初步恢复图架构。图\ref {fig: First Architecture} CNN 分支和 Transformer 分支以及 FCU (Feature Coupling Unit)。图\ref {fig: Up-sampling and down-sampling} 特征映射和 Patch embeddings 空间对齐的上采样和下采样过程。 图\ref {fig: The proposed initial architecture(Abstract Picture)} PACUT 的缩略图。PACUT 结构受 Conformer\cite  {peng2021conformer}启发，将原来结构中 CNN 分支的 ResNet 结构修改为 U-Net 结构。其采用一个 U-Net 和 ViT 的并行架构，通过 U-Net 结构得到一个弱恢复的弱特征图 $\mathcal  {F}_2$，通过 ViT 融合的特征可以初步增强弱特征图 $\mathcal  {F}_2$，ViT 的输出经过 Patch Expanding 的特征图 $\mathcal  {F}_3$ 经过与 U-Net 输出的特征图 $\mathcal  {F}_2$ 融合之后得到一个初步恢复的图片 output image，用以后续参与图片的进一步恢复。 \relax }}{19}{figure.caption.19}\protected@file@percent }
\newlabel{fig: PACUT}{{16}{19}{我们提出的初步恢复图架构。图\ref {fig: First Architecture} CNN 分支和 Transformer 分支以及 FCU (Feature Coupling Unit)。图\ref {fig: Up-sampling and down-sampling} 特征映射和 Patch embeddings 空间对齐的上采样和下采样过程。 图\ref {fig: The proposed initial architecture(Abstract Picture)} PACUT 的缩略图。PACUT 结构受 Conformer\cite {peng2021conformer}启发，将原来结构中 CNN 分支的 ResNet 结构修改为 U-Net 结构。其采用一个 U-Net 和 ViT 的并行架构，通过 U-Net 结构得到一个弱恢复的弱特征图 $\mathcal {F}_2$，通过 ViT 融合的特征可以初步增强弱特征图 $\mathcal {F}_2$，ViT 的输出经过 Patch Expanding 的特征图 $\mathcal {F}_3$ 经过与 U-Net 输出的特征图 $\mathcal {F}_2$ 融合之后得到一个初步恢复的图片 output image，用以后续参与图片的进一步恢复。 \relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}CNN 分支}{19}{subsubsection.4.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces   CNN分支及其所属的模块。 \relax }}{20}{figure.caption.20}\protected@file@percent }
\newlabel{fig: U-Net and AM}{{17}{20}{CNN分支及其所属的模块。 \relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {paragraph}{注意力残差多尺度块}{21}{equation.4.2}\protected@file@percent }
\newlabel{eq: ARMB}{{4.3}{21}{注意力残差多尺度块}{equation.4.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Transformer分支}{21}{subsubsection.4.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{嵌入块}{21}{subsubsection.4.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces   嵌入块的一般性过程。 \relax }}{22}{figure.caption.21}\protected@file@percent }
\newlabel{fig: Patch Embedding}{{18}{22}{嵌入块的一般性过程。 \relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces   Transformer 分支中嵌入块的过程。 \relax }}{22}{figure.caption.22}\protected@file@percent }
\newlabel{fig: Patch Embedding(ViT)}{{19}{22}{Transformer 分支中嵌入块的过程。 \relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {paragraph}{位置编码}{22}{figure.caption.22}\protected@file@percent }
\newlabel{eq: positional encoding}{{4.4}{23}{位置编码}{equation.4.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Transformer编码器}{23}{equation.4.4}\protected@file@percent }
\newlabel{eq: MSA}{{4.5}{23}{Transformer编码器}{equation.4.5}{}}
\newlabel{eq: Attention}{{4.6}{23}{Transformer编码器}{equation.4.6}{}}
\newlabel{eq: Residual Connection}{{4.7}{23}{Transformer编码器}{equation.4.7}{}}
\newlabel{eq: layernorm}{{4.8}{23}{Transformer编码器}{equation.4.8}{}}
\@writefile{toc}{\contentsline {paragraph}{特征耦合单元}{24}{equation.4.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}特征融合模块}{24}{subsubsection.4.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces   融合模块的结构。 \relax }}{24}{figure.caption.23}\protected@file@percent }
\newlabel{fig: Fusion Block}{{20}{24}{融合模块的结构。 \relax }{figure.caption.23}{}}
\newlabel{eq: capture color information}{{4.9}{24}{特征融合模块}{equation.4.9}{}}
\citation{huber1992robust}
\citation{johnson2016perceptual}
\citation{wang2004image}
\newlabel{eq: avgpool}{{4.10}{25}{特征融合模块}{equation.4.10}{}}
\newlabel{eq: maxpool}{{4.11}{25}{特征融合模块}{equation.4.11}{}}
\newlabel{eq: recalibrated feature map}{{4.12}{25}{特征融合模块}{equation.4.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.4}损失函数}{25}{subsubsection.4.1.4}\protected@file@percent }
\newlabel{eq: loss function}{{4.13}{25}{损失函数}{equation.4.13}{}}
\newlabel{eq: huber loss}{{4.14}{25}{损失函数}{equation.4.14}{}}
\newlabel{eq: perceptual loss}{{4.15}{25}{损失函数}{equation.4.15}{}}
\newlabel{eq: SSIM}{{4.16}{26}{损失函数}{equation.4.16}{}}
\newlabel{eq: SSIM loss}{{4.17}{26}{损失函数}{equation.4.17}{}}
\newlabel{eq: revised_SSIM loss}{{4.18}{26}{损失函数}{equation.4.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}边缘检测网络}{26}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}现有方法的局限性}{26}{subsubsection.4.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces   边缘检测网络。 \relax }}{26}{figure.caption.24}\protected@file@percent }
\newlabel{fig: Edge Detection Network}{{21}{26}{边缘检测网络。 \relax }{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces   我们提出的网络结构。 \relax }}{27}{figure.caption.25}\protected@file@percent }
\newlabel{fig: Total Architecture}{{22}{27}{我们提出的网络结构。 \relax }{figure.caption.25}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}基于边缘语义信息的增强网络模型}{28}{subsection.4.3}\protected@file@percent }
\citation{wei2018deep}
\citation{cai2018learning}
\citation{wang2004image}
\citation{wang2004image}
\citation{mittal2012making}
\citation{pisano1998contrast}
\citation{li2018lightennet}
\citation{lore2017llnet}
\citation{wei2018deep}
\citation{lv2018mbllen}
\citation{jiang2021enlightengan}
\citation{guo2020zero}
\citation{zhang2019kindling}
\citation{wei2018deep}
\citation{cai2018learning}
\citation{pisano1998contrast}
\citation{li2018lightennet}
\citation{lore2017llnet}
\citation{wei2018deep}
\citation{zhang2019kindling}
\citation{lv2018mbllen}
\citation{jiang2021enlightengan}
\citation{guo2020zero}
\@writefile{toc}{\contentsline {section}{\numberline {5}已完成工作}{29}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}初步恢复图像的生成模型实验结果}{29}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}实验设置}{29}{subsubsection.5.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}数据集和评价指标}{29}{subsubsection.5.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3}对比试验}{29}{subsubsection.5.1.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces   LOL和SCIE数据集在各方法下的对比值，其中红色表示最优，蓝色表示次优，$↑$表示越高越好，$↓$表示越低越好。 \relax }}{29}{table.caption.27}\protected@file@percent }
\newlabel{tab: Quantitative Comparisons on LOL-test and SCI testing datasets}{{2}{29}{LOL和SCIE数据集在各方法下的对比值，其中红色表示最优，蓝色表示次优，$↑$表示越高越好，$↓$表示越低越好。 \relax }{table.caption.27}{}}
\newlabel{fig: LLI Input}{{23a}{30}{Input\relax }{figure.caption.28}{}}
\newlabel{sub@fig: LLI Input}{{a}{30}{Input\relax }{figure.caption.28}{}}
\newlabel{fig: LightenNet}{{23b}{30}{LightenNet\relax }{figure.caption.28}{}}
\newlabel{sub@fig: LightenNet}{{b}{30}{LightenNet\relax }{figure.caption.28}{}}
\newlabel{fig: LLI LLNet}{{23c}{30}{LLNet\relax }{figure.caption.28}{}}
\newlabel{sub@fig: LLI LLNet}{{c}{30}{LLNet\relax }{figure.caption.28}{}}
\newlabel{fig: LLI RetinexNet}{{23d}{30}{RetinexNet\relax }{figure.caption.28}{}}
\newlabel{sub@fig: LLI RetinexNet}{{d}{30}{RetinexNet\relax }{figure.caption.28}{}}
\newlabel{fig: KinD}{{23e}{30}{KinD\relax }{figure.caption.28}{}}
\newlabel{sub@fig: KinD}{{e}{30}{KinD\relax }{figure.caption.28}{}}
\newlabel{fig: MBLLEN}{{23f}{30}{MBLLEN\relax }{figure.caption.28}{}}
\newlabel{sub@fig: MBLLEN}{{f}{30}{MBLLEN\relax }{figure.caption.28}{}}
\newlabel{fig: LLI EnlightenGAN}{{23g}{30}{EnlightenGAN\relax }{figure.caption.28}{}}
\newlabel{sub@fig: LLI EnlightenGAN}{{g}{30}{EnlightenGAN\relax }{figure.caption.28}{}}
\newlabel{fig: Zero-DCE}{{23h}{30}{Zero-DCE\relax }{figure.caption.28}{}}
\newlabel{sub@fig: Zero-DCE}{{h}{30}{Zero-DCE\relax }{figure.caption.28}{}}
\newlabel{fig: Ours}{{23i}{30}{Ours\relax }{figure.caption.28}{}}
\newlabel{sub@fig: Ours}{{i}{30}{Ours\relax }{figure.caption.28}{}}
\newlabel{fig: GT}{{23j}{30}{GT\relax }{figure.caption.28}{}}
\newlabel{sub@fig: GT}{{j}{30}{GT\relax }{figure.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces   不同方法在LOL测试数据集上的视觉表现。 \relax }}{30}{figure.caption.28}\protected@file@percent }
\newlabel{fig: LOL}{{23}{30}{不同方法在LOL测试数据集上的视觉表现。 \relax }{figure.caption.28}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}消融实验}{30}{subsection.5.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces   Transformer分支和Transformer预训练权重尺寸和SCConv对模型的影响。 \relax }}{30}{table.caption.29}\protected@file@percent }
\newlabel{tab: Ablation Study}{{3}{30}{Transformer分支和Transformer预训练权重尺寸和SCConv对模型的影响。 \relax }{table.caption.29}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}下一步工作}{30}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}方向}{30}{section.7}\protected@file@percent }
\citation{wang2022uformer}
\citation{peng2021conformer}
\citation{peng2021conformer}
\citation{wang2022uformer}
\citation{li2023effective}
\citation{li2023effective}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}方向一}{31}{subsection.7.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces   Low-Lighting 图像恢复网络。结构受 Conformer\cite  {peng2021conformer} （见 Fig. \ref {fig: Conformer architecture}）启发，将原来结构中 CNN 分支的 ResNet 结构修改为 U-Net 结构。其采用一个 U-Net 和 ViT 的并行架构，通过 U-Net 结构得到一个弱恢复的弱特征图 $\mathcal  {F}_2$，通过 ViT 融合的特征可以初步增强弱特征图 $\mathcal  {F}_2$，ViT 的输出经过 Patch Expanding 的特征图 $\mathcal  {F}_3$ 经过与 U-Net 输出的特征图 $\mathcal  {F}_2$ 融合之后得到一个初步恢复的图片 output image，用以后续参与图片的进一步恢复。 \relax }}{31}{figure.caption.30}\protected@file@percent }
\newlabel{fig: The proposed architecture}{{24}{31}{Low-Lighting 图像恢复网络。结构受 Conformer\cite {peng2021conformer} （见 Fig. \ref {fig: Conformer architecture}）启发，将原来结构中 CNN 分支的 ResNet 结构修改为 U-Net 结构。其采用一个 U-Net 和 ViT 的并行架构，通过 U-Net 结构得到一个弱恢复的弱特征图 $\mathcal {F}_2$，通过 ViT 融合的特征可以初步增强弱特征图 $\mathcal {F}_2$，ViT 的输出经过 Patch Expanding 的特征图 $\mathcal {F}_3$ 经过与 U-Net 输出的特征图 $\mathcal {F}_2$ 融合之后得到一个初步恢复的图片 output image，用以后续参与图片的进一步恢复。 \relax }{figure.caption.30}{}}
\citation{xu2023low}
\citation{zhu2020eemefn}
\citation{rana2021edge}
\bibstyle{unsrt}
\bibdata{reference}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces   Uformer 的结构。 \relax }}{32}{figure.caption.31}\protected@file@percent }
\newlabel{fig: Uformer}{{25}{32}{Uformer 的结构。 \relax }{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces   MCLNet 的结构 \relax }}{32}{figure.caption.32}\protected@file@percent }
\newlabel{fig: MCLNet}{{26}{32}{MCLNet 的结构 \relax }{figure.caption.32}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}方向二}{32}{subsection.7.2}\protected@file@percent }
\bibcite{liu2021retinex}{1}
\bibcite{xu2020learning}{2}
\bibcite{ueng1995gamma}{3}
\bibcite{stark2000adaptive}{4}
\bibcite{land1971lightness}{5}
\bibcite{liu2021benchmarking}{6}
\bibcite{dai2019fractional}{7}
\bibcite{ma2019improved}{8}
\bibcite{cooper2004analysis}{9}
\bibcite{202013}{10}
\bibcite{dong2010fast}{11}
\bibcite{lore2017llnet}{12}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces   该结构的边缘图直接从 $I$ 中通过 StyleGAN 得到边缘图 $I_s$, 而非从 $I_{\alpha }$ 中通过边缘网络获取边缘图。 \relax }}{33}{figure.caption.33}\protected@file@percent }
\newlabel{fig: Overview}{{27}{33}{该结构的边缘图直接从 $I$ 中通过 StyleGAN 得到边缘图 $I_s$, 而非从 $I_{\alpha }$ 中通过边缘网络获取边缘图。 \relax }{figure.caption.33}{}}
\bibcite{tang2023low}{13}
\bibcite{lv2018mbllen}{14}
\bibcite{wang2018gladnet}{15}
\bibcite{lu2020tbefn}{16}
\bibcite{li2021low}{17}
\bibcite{ravirathinam2021c}{18}
\bibcite{lim2020dslr}{19}
\bibcite{wei2018deep}{20}
\bibcite{shen2017msr}{21}
\bibcite{zhang2019kindling}{22}
\bibcite{cui2022illumination}{23}
\bibcite{carion2020end}{24}
\bibcite{wang2023ultra}{25}
\bibcite{jiang2021enlightengan}{26}
\bibcite{fu2022gan}{27}
\bibcite{ni2020towards}{28}
\bibcite{zhang2021unsupervised}{29}
\bibcite{qiao2021deep}{30}
\bibcite{robert2018hybridnet}{31}
\bibcite{zhu2020zero}{32}
\bibcite{zhang2019zero}{33}
\bibcite{yang2021locally}{34}
\bibcite{zhang2020attention}{35}
\bibcite{li2018multi}{36}
\bibcite{zamir2020learning}{37}
\bibcite{li2020visual}{38}
\bibcite{xu2023low}{39}
\bibcite{zhu2020eemefn}{40}
\bibcite{rana2021edge}{41}
\bibcite{chen2018learning}{42}
\bibcite{wang2004image}{43}
\bibcite{pietikainen2010local}{44}
\bibcite{maini2009study}{45}
\bibcite{liu2017richer}{46}
\bibcite{woo2018cbam}{47}
\bibcite{ramachandran2019stand}{48}
\bibcite{li2023scconv}{49}
\bibcite{zamir2021learning}{50}
\bibcite{meng2020gia}{51}
\bibcite{zhou2018unet++}{52}
\bibcite{zhou2019unet++}{53}
\bibcite{vaswani2017attention}{54}
\bibcite{dosovitskiy2020image}{55}
\bibcite{wang2022ultrahighdefinition}{56}
\bibcite{wang2021uformer}{57}
\bibcite{chen2023cross}{58}
\bibcite{jain1991unsupervised}{59}
\bibcite{lowe2004distinctive}{60}
\bibcite{ojala2002multiresolution}{61}
\bibcite{lisin2005combining}{62}
\bibcite{peng2021conformer}{63}
\bibcite{karu1996there}{64}
\bibcite{huber1992robust}{65}
\bibcite{johnson2016perceptual}{66}
\bibcite{cai2018learning}{67}
\bibcite{mittal2012making}{68}
\bibcite{pisano1998contrast}{69}
\bibcite{li2018lightennet}{70}
\bibcite{guo2020zero}{71}
\bibcite{wang2022uformer}{72}
\bibcite{li2023effective}{73}
\newlabel{LastPage}{{}{37}{}{page.37}{}}
\xdef\lastpage@lastpage{37}
\xdef\lastpage@lastpageHy{37}
\gdef \@abspage@last{37}
