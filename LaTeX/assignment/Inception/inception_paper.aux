\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/D>>}
\citation{liu2021retinex,xu2020learning}
\citation{ueng1995gamma}
\citation{stark2000adaptive}
\citation{land1971lightness}
\citation{liu2021benchmarking}
\citation{dai2019fractional}
\citation{ma2019improved}
\@writefile{toc}{\contentsline {section}{\numberline {1}研究意义}{3}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}研究背景和现状}{3}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}传统低照度图像增强方法}{3}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}基于 Retinex 的方法}{3}{subsubsection.2.1.1}\protected@file@percent }
\newlabel{eq: Retinex model formula}{{2.1}{3}{基于 Retinex 的方法}{equation.2.1}{}}
\newlabel{eq: Retinex model formula log}{{2.2}{3}{基于 Retinex 的方法}{equation.2.2}{}}
\citation{cooper2004analysis}
\citation{202013}
\citation{cooper2004analysis}
\citation{cooper2004analysis}
\citation{stark2000adaptive}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces   Retinex 算法处理过程 \relax }}{4}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig: Retinex model}{{1}{4}{Retinex 算法处理过程 \relax }{figure.caption.2}{}}
\newlabel{fig: Retinex Model_input}{{2a}{4}{低照度图像\relax }{figure.caption.3}{}}
\newlabel{sub@fig: Retinex Model_input}{{a}{4}{低照度图像\relax }{figure.caption.3}{}}
\newlabel{fig: Retinex Model_Retinex}{{2b}{4}{Retinex\cite {cooper2004analysis}\relax }{figure.caption.3}{}}
\newlabel{sub@fig: Retinex Model_Retinex}{{b}{4}{Retinex\cite {cooper2004analysis}\relax }{figure.caption.3}{}}
\newlabel{fig: Retinex Model_SSR}{{2c}{4}{SSR\relax }{figure.caption.3}{}}
\newlabel{sub@fig: Retinex Model_SSR}{{c}{4}{SSR\relax }{figure.caption.3}{}}
\newlabel{fig: Retinex Model_MSR}{{2d}{4}{MSR\relax }{figure.caption.3}{}}
\newlabel{sub@fig: Retinex Model_MSR}{{d}{4}{MSR\relax }{figure.caption.3}{}}
\newlabel{fig: Retinex Model_MSRCR}{{2e}{4}{MSRCR\relax }{figure.caption.3}{}}
\newlabel{sub@fig: Retinex Model_MSRCR}{{e}{4}{MSRCR\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces   传统 Retinex 算法实验结果。 \relax }}{4}{figure.caption.3}\protected@file@percent }
\newlabel{fig: Retinex Model}{{2}{4}{传统 Retinex 算法实验结果。 \relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}基于直方图的方法}{4}{subsubsection.2.1.2}\protected@file@percent }
\newlabel{eq: Histogram}{{2.3}{4}{基于直方图的方法}{equation.2.3}{}}
\citation{dong2010fast}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces   直方图均衡化示意图 \relax }}{5}{figure.caption.4}\protected@file@percent }
\newlabel{fig: Histogram equalization}{{3}{5}{直方图均衡化示意图 \relax }{figure.caption.4}{}}
\newlabel{eq: HE}{{2.4}{5}{基于直方图的方法}{equation.2.4}{}}
\citation{lore2017llnet}
\citation{tang2023low}
\citation{lore2017llnet}
\citation{lv2018mbllen}
\citation{wang2018gladnet}
\citation{lu2020tbefn}
\citation{li2021low}
\citation{ravirathinam2021c}
\citation{lim2020dslr}
\newlabel{fig: LL input}{{\caption@xref {fig: LL input}{ on input line 248}}{6}{基于图像反相的方法}{figure.caption.5}{}}
\newlabel{sub@fig: LL input}{{}{6}{基于图像反相的方法}{figure.caption.5}{}}
\newlabel{fig: Inversed}{{\caption@xref {fig: Inversed}{ on input line 253}}{6}{基于图像反相的方法}{figure.caption.5}{}}
\newlabel{sub@fig: Inversed}{{}{6}{基于图像反相的方法}{figure.caption.5}{}}
\newlabel{fig: marked image}{{\caption@xref {fig: marked image}{ on input line 258}}{6}{基于图像反相的方法}{figure.caption.5}{}}
\newlabel{sub@fig: marked image}{{}{6}{基于图像反相的方法}{figure.caption.5}{}}
\newlabel{fig: de-haze}{{\caption@xref {fig: de-haze}{ on input line 263}}{6}{基于图像反相的方法}{figure.caption.5}{}}
\newlabel{sub@fig: de-haze}{{}{6}{基于图像反相的方法}{figure.caption.5}{}}
\newlabel{fig: Final output}{{\caption@xref {fig: Final output}{ on input line 268}}{6}{基于图像反相的方法}{figure.caption.5}{}}
\newlabel{sub@fig: Final output}{{}{6}{基于图像反相的方法}{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces   (a) 低照度图片输入 I. (b) 从输入I获得的反向图片 R.(c) 标记图像：在至少一种颜色（RGB）通道中具有低强度的像素为绿色。 (d) 去雾： 使用公式获得输出 J. (e) 最终得输出结果 E. \relax }}{6}{figure.caption.5}\protected@file@percent }
\newlabel{fig: Image Inverse}{{4}{6}{(a) 低照度图片输入 I. (b) 从输入I获得的反向图片 R.(c) 标记图像：在至少一种颜色（RGB）通道中具有低强度的像素为绿色。 (d) 去雾： 使用公式获得输出 J. (e) 最终得输出结果 E. \relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}基于图像反相的方法}{6}{subsubsection.2.1.3}\protected@file@percent }
\newlabel{eq: Image Reverse}{{2.5}{6}{基于图像反相的方法}{equation.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}基于深度学习的低照度图像增强方法}{6}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}有监督学习}{6}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{端到端 (End-to-end) 方法}{6}{subsubsection.2.2.1}\protected@file@percent }
\citation{wei2018deep}
\citation{shen2017msr}
\citation{zhang2019kindling}
\citation{cui2022illumination}
\citation{carion2020end}
\citation{wang2023ultra}
\newlabel{fig: Module Structure}{{5a}{7}{Module Structure\relax }{figure.caption.6}{}}
\newlabel{sub@fig: Module Structure}{{a}{7}{Module Structure\relax }{figure.caption.6}{}}
\newlabel{fig: LLNet}{{5b}{7}{LLNet\relax }{figure.caption.6}{}}
\newlabel{sub@fig: LLNet}{{b}{7}{LLNet\relax }{figure.caption.6}{}}
\newlabel{fig: S-LLNet}{{5c}{7}{S-LLNet\relax }{figure.caption.6}{}}
\newlabel{sub@fig: S-LLNet}{{c}{7}{S-LLNet\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces   LLNet结构示意图 \relax }}{7}{figure.caption.6}\protected@file@percent }
\newlabel{fig: LLNet Architecture}{{5}{7}{LLNet结构示意图 \relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Retinex 理论方法}{7}{figure.caption.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{基于 Transformer 的方法}{7}{figure.caption.8}\protected@file@percent }
\citation{jiang2021enlightengan}
\citation{fu2022gan}
\citation{ni2020towards}
\citation{zhang2021unsupervised}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces   MBLLEN 结构图。 \relax }}{8}{figure.caption.7}\protected@file@percent }
\newlabel{fig: MBLLEN Architecture}{{6}{8}{MBLLEN 结构图。 \relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces   RetinexNet算法结构图。主要由三个部分组成，分解(Decomposition)，调节(Adjustment)和重构(Reconstruction)。 \relax }}{8}{figure.caption.8}\protected@file@percent }
\newlabel{fig: RetinexNet}{{7}{8}{RetinexNet算法结构图。主要由三个部分组成，分解(Decomposition)，调节(Adjustment)和重构(Reconstruction)。 \relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}无监督学习}{8}{subsubsection.2.2.2}\protected@file@percent }
\citation{qiao2021deep}
\citation{robert2018hybridnet}
\citation{zhu2020zero}
\citation{zhang2019zero}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces   EnlightenGAN 结构图 \relax }}{9}{figure.caption.9}\protected@file@percent }
\newlabel{fig: EnlightenGAN}{{8}{9}{EnlightenGAN 结构图 \relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}半监督学习}{9}{subsubsection.2.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces   DRBN 结构图 \relax }}{9}{figure.caption.10}\protected@file@percent }
\newlabel{fig: DRBN}{{9}{9}{DRBN 结构图 \relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4}Zero-Shot 学习}{9}{subsubsection.2.2.4}\protected@file@percent }
\citation{yang2021locally,zhang2020attention}
\citation{li2018multi,zamir2020learning}
\citation{li2020visual}
\citation{xu2023low}
\citation{zhu2020eemefn}
\citation{zhu2020eemefn}
\citation{rana2021edge}
\citation{rana2021edge}
\citation{xu2023low}
\citation{xu2023low}
\citation{chen2018learning}
\citation{xu2023low}
\citation{wang2004image}
\citation{chen2018learning}
\citation{xu2023low}
\citation{wang2004image}
\citation{zhu2020eemefn,rana2021edge}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces   RRDNet 结构图 \relax }}{10}{figure.caption.11}\protected@file@percent }
\newlabel{fig: RRDNet}{{10}{10}{RRDNet 结构图 \relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}研究目的}{10}{subsection.2.3}\protected@file@percent }
\newlabel{fig: input}{{\caption@xref {fig: input}{ on input line 420}}{10}{研究目的}{figure.caption.12}{}}
\newlabel{sub@fig: input}{{}{10}{研究目的}{figure.caption.12}{}}
\newlabel{fig: LLNet_VE_LOL}{{\caption@xref {fig: LLNet_VE_LOL}{ on input line 426}}{10}{研究目的}{figure.caption.12}{}}
\newlabel{sub@fig: LLNet_VE_LOL}{{}{10}{研究目的}{figure.caption.12}{}}
\newlabel{fig: RetinexNet_VE_LOL}{{\caption@xref {fig: RetinexNet_VE_LOL}{ on input line 432}}{10}{研究目的}{figure.caption.12}{}}
\newlabel{sub@fig: RetinexNet_VE_LOL}{{}{10}{研究目的}{figure.caption.12}{}}
\newlabel{fig: MBLLEN_LOL}{{\caption@xref {fig: MBLLEN_LOL}{ on input line 438}}{10}{研究目的}{figure.caption.12}{}}
\newlabel{sub@fig: MBLLEN_LOL}{{}{10}{研究目的}{figure.caption.12}{}}
\newlabel{fig: EnlightenGAN_VE_LOL}{{\caption@xref {fig: EnlightenGAN_VE_LOL}{ on input line 444}}{10}{研究目的}{figure.caption.12}{}}
\newlabel{sub@fig: EnlightenGAN_VE_LOL}{{}{10}{研究目的}{figure.caption.12}{}}
\newlabel{fig: RRDNet_VE_LOL}{{\caption@xref {fig: RRDNet_VE_LOL}{ on input line 450}}{10}{研究目的}{figure.caption.12}{}}
\newlabel{sub@fig: RRDNet_VE_LOL}{{}{10}{研究目的}{figure.caption.12}{}}
\newlabel{fig: DRBN_VE_LOL}{{\caption@xref {fig: DRBN_VE_LOL}{ on input line 456}}{10}{研究目的}{figure.caption.12}{}}
\newlabel{sub@fig: DRBN_VE_LOL}{{}{10}{研究目的}{figure.caption.12}{}}
\newlabel{fig: Zero-DCE++}{{\caption@xref {fig: Zero-DCE++}{ on input line 462}}{10}{研究目的}{figure.caption.12}{}}
\newlabel{sub@fig: Zero-DCE++}{{}{10}{研究目的}{figure.caption.12}{}}
\newlabel{fig: KinD++}{{\caption@xref {fig: KinD++}{ on input line 468}}{10}{研究目的}{figure.caption.12}{}}
\newlabel{sub@fig: KinD++}{{}{10}{研究目的}{figure.caption.12}{}}
\newlabel{fig: URetinexNet}{{\caption@xref {fig: URetinexNet}{ on input line 474}}{10}{研究目的}{figure.caption.12}{}}
\newlabel{sub@fig: URetinexNet}{{}{10}{研究目的}{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces   不同算法对从 VE-LOL-L 数据集采样的低照度图像的可视化结果。 \relax }}{10}{figure.caption.12}\protected@file@percent }
\newlabel{fig: VE-LOL-L Visual}{{11}{10}{不同算法对从 VE-LOL-L 数据集采样的低照度图像的可视化结果。 \relax }{figure.caption.12}{}}
\citation{xu2023low}
\citation{pietikainen2010local}
\citation{maini2009study}
\newlabel{fig: EEMEFN}{{12a}{11}{该 LLIE 结构源自\cite {zhu2020eemefn},如其 Multi-Exposure Fusion 部分采用多曝光融合结构,与由 Initial image 生成的边缘图进行 Concat, 后续通过一个 U-Net 网络进一步恢复图像。\relax }{figure.caption.13}{}}
\newlabel{sub@fig: EEMEFN}{{a}{11}{该 LLIE 结构源自\cite {zhu2020eemefn},如其 Multi-Exposure Fusion 部分采用多曝光融合结构,与由 Initial image 生成的边缘图进行 Concat, 后续通过一个 U-Net 网络进一步恢复图像。\relax }{figure.caption.13}{}}
\newlabel{fig: EdgeNet}{{12b}{11}{该 LLIE 结构源自\cite {rana2021edge}使用 EdgeNet 首先从低光图像中过滤边缘，EnhanceNet 反复使用上采样和下采样块的组合，从局部到全局逐渐提取特征，并消除伪影和噪声。\relax }{figure.caption.13}{}}
\newlabel{sub@fig: EdgeNet}{{b}{11}{该 LLIE 结构源自\cite {rana2021edge}使用 EdgeNet 首先从低光图像中过滤边缘，EnhanceNet 反复使用上采样和下采样块的组合，从局部到全局逐渐提取特征，并消除伪影和噪声。\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces   边缘图像指导弱光图像增强的传统架构。 \relax }}{11}{figure.caption.13}\protected@file@percent }
\newlabel{fig: Traditional Architecture}{{12}{11}{边缘图像指导弱光图像增强的传统架构。 \relax }{figure.caption.13}{}}
\newlabel{fig: SMG-LLIE Architecture}{{13a}{11}{该 LLIE 结构源自\cite {xu2023low}，其提出一种基于 GAN Loss 的模型去对结构信息建模，通过获得的结构信息指导增强。\relax }{figure.caption.14}{}}
\newlabel{sub@fig: SMG-LLIE Architecture}{{a}{11}{该 LLIE 结构源自\cite {xu2023low}，其提出一种基于 GAN Loss 的模型去对结构信息建模，通过获得的结构信息指导增强。\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces   边缘图像指导弱光图像增强的最新架构(CVPR 2023)。 \relax }}{11}{figure.caption.14}\protected@file@percent }
\newlabel{fig: SMG-LLIE Overview}{{13}{11}{边缘图像指导弱光图像增强的最新架构(CVPR 2023)。 \relax }{figure.caption.14}{}}
\citation{liu2017richer}
\citation{xu2023low}
\citation{woo2018cbam}
\newlabel{fig: Input}{{14a}{12}{Input\relax }{figure.caption.15}{}}
\newlabel{sub@fig: Input}{{a}{12}{Input\relax }{figure.caption.15}{}}
\newlabel{fig: Structure of (a)}{{14b}{12}{Structure of (a)\relax }{figure.caption.15}{}}
\newlabel{sub@fig: Structure of (a)}{{b}{12}{Structure of (a)\relax }{figure.caption.15}{}}
\newlabel{fig: SNR (CVPR 2022)}{{14c}{12}{SNR (CVPR 2022)\relax }{figure.caption.15}{}}
\newlabel{sub@fig: SNR (CVPR 2022)}{{c}{12}{SNR (CVPR 2022)\relax }{figure.caption.15}{}}
\newlabel{fig: Structure Modeling}{{14d}{12}{Structure Modeling\relax }{figure.caption.15}{}}
\newlabel{sub@fig: Structure Modeling}{{d}{12}{Structure Modeling\relax }{figure.caption.15}{}}
\newlabel{fig: SMG-LLIE}{{14e}{12}{SMG-LLIE\relax }{figure.caption.15}{}}
\newlabel{sub@fig: SMG-LLIE}{{e}{12}{SMG-LLIE\relax }{figure.caption.15}{}}
\newlabel{fig: Ground Truth}{{14f}{12}{Ground Truth\relax }{figure.caption.15}{}}
\newlabel{sub@fig: Ground Truth}{{f}{12}{Ground Truth\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces   SID-sRGB\cite  {chen2018learning}中一张弱光图片, 通过SOTA方法 (c) 和\cite  {xu2023low}提出的方法 (e)增强。作者的方法可以从输入的图像中合成结构图(d)，使细节更清晰，对比度更清晰，颜色更鲜艳。虽然(c)的 PSNR 为 28.17，但其 SSIM 低为 0.75。作者的方法在dB和SSIM\cite  {wang2004image}的得分都很高，分别为28.60 dB 和 0.80。 \relax }}{12}{figure.caption.15}\protected@file@percent }
\newlabel{fig: Structural Information}{{14}{12}{SID-sRGB\cite {chen2018learning}中一张弱光图片, 通过SOTA方法 (c) 和\cite {xu2023low}提出的方法 (e)增强。作者的方法可以从输入的图像中合成结构图(d)，使细节更清晰，对比度更清晰，颜色更鲜艳。虽然(c)的 PSNR 为 28.17，但其 SSIM 低为 0.75。作者的方法在dB和SSIM\cite {wang2004image}的得分都很高，分别为28.60 dB 和 0.80。 \relax }{figure.caption.15}{}}
\citation{ramachandran2019stand}
\citation{woo2018cbam}
\citation{li2023scconv}
\citation{chen2018learning}
\citation{chen2018learning,zamir2021learning}
\citation{chen2018learning}
\citation{chen2018learning,zamir2021learning}
\citation{meng2020gia}
\citation{chen2018learning,zamir2021learning}
\citation{meng2020gia}
\citation{chen2018learning,meng2020gia,zamir2021learning}
\citation{zhou2018unet++,zhou2019unet++}
\citation{yang2021locally,zhang2020attention}
\citation{li2018multi,zamir2020learning}
\citation{li2020visual}
\citation{xu2020learning}
\citation{vaswani2017attention}
\citation{dosovitskiy2020image}
\citation{wang2022ultrahighdefinition}
\citation{wang2021uformer}
\citation{chen2023cross}
\@writefile{toc}{\contentsline {section}{\numberline {3}采用的方法}{13}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}注意力机制}{13}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}U-Net网络结构}{13}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}CNN网络结构}{13}{subsection.3.3}\protected@file@percent }
\citation{jain1991unsupervised,lowe2004distinctive,ojala2002multiresolution}
\citation{lisin2005combining}
\citation{peng2021conformer}
\citation{peng2021conformer}
\citation{peng2021conformer}
\citation{jain1991unsupervised,lowe2004distinctive,ojala2002multiresolution}
\citation{lisin2005combining}
\citation{karu1996there}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Transformer网络结构}{14}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}具体方法}{14}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}初步恢复图像的生成模型}{14}{subsection.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces   作者\cite  {peng2021conformer}提出的网络结构。(a) 特征图和补丁嵌入用于空间对齐的上采样和下采样。(b) CNN块、Transformer块和特征耦合单元（FCU）的实现细节。(c) 缩略图。 \relax }}{14}{figure.caption.17}\protected@file@percent }
\newlabel{fig: Conformer}{{15}{14}{作者\cite {peng2021conformer}提出的网络结构。(a) 特征图和补丁嵌入用于空间对齐的上采样和下采样。(b) CNN块、Transformer块和特征耦合单元（FCU）的实现细节。(c) 缩略图。 \relax }{figure.caption.17}{}}
\citation{peng2021conformer}
\citation{peng2021conformer}
\newlabel{fig: First Architecture}{{16a}{15}{PACUT\relax }{figure.caption.18}{}}
\newlabel{sub@fig: First Architecture}{{a}{15}{PACUT\relax }{figure.caption.18}{}}
\newlabel{fig: Up-sampling and down-sampling}{{16b}{15}{Up-sampling and Down-sampling\relax }{figure.caption.18}{}}
\newlabel{sub@fig: Up-sampling and down-sampling}{{b}{15}{Up-sampling and Down-sampling\relax }{figure.caption.18}{}}
\newlabel{fig: The proposed initial architecture(Abstract Picture)}{{16c}{15}{Thumbnail of PACUT\relax }{figure.caption.18}{}}
\newlabel{sub@fig: The proposed initial architecture(Abstract Picture)}{{c}{15}{Thumbnail of PACUT\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces   我们提出的初步恢复图架构。图\ref {fig: First Architecture} CNN 分支和 Transformer 分支以及 FCU (Feature Coupling Unit)。图\ref {fig: Up-sampling and down-sampling} 特征映射和 Patch embeddings 空间对齐的上采样和下采样过程。 图\ref {fig: The proposed initial architecture(Abstract Picture)} PACUT 的缩略图。PACUT 结构受 Conformer\cite  {peng2021conformer}启发，将原来结构中 CNN 分支的 ResNet 结构修改为 U-Net 结构。其采用一个 U-Net 和 ViT 的并行架构，通过 U-Net 结构得到一个弱恢复的弱特征图 $\mathcal  {F}_2$，通过 ViT 融合的特征可以初步增强弱特征图 $\mathcal  {F}_2$，ViT 的输出经过 Patch Expanding 的特征图 $\mathcal  {F}_3$ 经过与 U-Net 输出的特征图 $\mathcal  {F}_2$ 融合之后得到一个初步恢复的图片 output image，用以后续参与图片的进一步恢复。 \relax }}{15}{figure.caption.18}\protected@file@percent }
\newlabel{fig: PACUT}{{16}{15}{我们提出的初步恢复图架构。图\ref {fig: First Architecture} CNN 分支和 Transformer 分支以及 FCU (Feature Coupling Unit)。图\ref {fig: Up-sampling and down-sampling} 特征映射和 Patch embeddings 空间对齐的上采样和下采样过程。 图\ref {fig: The proposed initial architecture(Abstract Picture)} PACUT 的缩略图。PACUT 结构受 Conformer\cite {peng2021conformer}启发，将原来结构中 CNN 分支的 ResNet 结构修改为 U-Net 结构。其采用一个 U-Net 和 ViT 的并行架构，通过 U-Net 结构得到一个弱恢复的弱特征图 $\mathcal {F}_2$，通过 ViT 融合的特征可以初步增强弱特征图 $\mathcal {F}_2$，ViT 的输出经过 Patch Expanding 的特征图 $\mathcal {F}_3$ 经过与 U-Net 输出的特征图 $\mathcal {F}_2$ 融合之后得到一个初步恢复的图片 output image，用以后续参与图片的进一步恢复。 \relax }{figure.caption.18}{}}
\citation{woo2018cbam}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}CNN 分支}{16}{subsubsection.4.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces   CNN分支及其所属的模块。 \relax }}{16}{figure.caption.19}\protected@file@percent }
\newlabel{fig: U-Net and AM}{{17}{16}{CNN分支及其所属的模块。 \relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {paragraph}{注意力残差多尺度块}{17}{equation.4.2}\protected@file@percent }
\newlabel{eq: ARMB}{{4.3}{17}{注意力残差多尺度块}{equation.4.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Transformer分支}{18}{subsubsection.4.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{嵌入块}{18}{subsubsection.4.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces   嵌入块的一般性过程。 \relax }}{18}{figure.caption.20}\protected@file@percent }
\newlabel{fig: Patch Embedding}{{18}{18}{嵌入块的一般性过程。 \relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces   Transformer 分支中嵌入块的过程。 \relax }}{18}{figure.caption.21}\protected@file@percent }
\newlabel{fig: Patch Embedding(ViT)}{{19}{18}{Transformer 分支中嵌入块的过程。 \relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {paragraph}{位置编码}{19}{figure.caption.21}\protected@file@percent }
\newlabel{eq: positional encoding}{{4.4}{19}{位置编码}{equation.4.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Transformer编码器}{19}{equation.4.4}\protected@file@percent }
\newlabel{eq: MSA}{{4.5}{19}{Transformer编码器}{equation.4.5}{}}
\newlabel{eq: Attention}{{4.6}{19}{Transformer编码器}{equation.4.6}{}}
\newlabel{eq: Residual Connection}{{4.7}{20}{Transformer编码器}{equation.4.7}{}}
\newlabel{eq: layernorm}{{4.8}{20}{Transformer编码器}{equation.4.8}{}}
\@writefile{toc}{\contentsline {paragraph}{特征耦合单元}{20}{equation.4.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}特征融合模块}{20}{subsubsection.4.1.3}\protected@file@percent }
\newlabel{eq: capture color information}{{4.9}{20}{特征融合模块}{equation.4.9}{}}
\citation{huber1992robust}
\citation{johnson2016perceptual}
\citation{wang2004image}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces   融合模块的结构。 \relax }}{21}{figure.caption.22}\protected@file@percent }
\newlabel{fig: Fusion Block}{{20}{21}{融合模块的结构。 \relax }{figure.caption.22}{}}
\newlabel{eq: avgpool}{{4.10}{21}{特征融合模块}{equation.4.10}{}}
\newlabel{eq: maxpool}{{4.11}{21}{特征融合模块}{equation.4.11}{}}
\newlabel{eq: recalibrated feature map}{{4.12}{21}{特征融合模块}{equation.4.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.4}损失函数}{21}{subsubsection.4.1.4}\protected@file@percent }
\newlabel{eq: loss function}{{4.13}{21}{损失函数}{equation.4.13}{}}
\newlabel{eq: huber loss}{{4.14}{22}{损失函数}{equation.4.14}{}}
\newlabel{eq: perceptual loss}{{4.15}{22}{损失函数}{equation.4.15}{}}
\newlabel{eq: SSIM}{{4.16}{22}{损失函数}{equation.4.16}{}}
\newlabel{eq: SSIM loss}{{4.17}{22}{损失函数}{equation.4.17}{}}
\newlabel{eq: revised_SSIM loss}{{4.18}{22}{损失函数}{equation.4.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}边缘检测网络}{22}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}基于边缘语义信息的增强网络模型}{22}{subsection.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces   边缘检测网络。 \relax }}{23}{figure.caption.23}\protected@file@percent }
\newlabel{fig: Edge Detection Network}{{21}{23}{边缘检测网络。 \relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces   我们提出的网络结构。 \relax }}{23}{figure.caption.24}\protected@file@percent }
\newlabel{fig: Total Architecture}{{22}{23}{我们提出的网络结构。 \relax }{figure.caption.24}{}}
\citation{wang2022uformer}
\citation{peng2021conformer}
\citation{peng2021conformer}
\citation{wang2022uformer}
\citation{li2023effective}
\citation{li2023effective}
\@writefile{toc}{\contentsline {section}{\numberline {5}数据集}{24}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}文献调研与支撑}{24}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}已完成工作}{24}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}下一步工作}{24}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}方向}{24}{section.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}方向一}{24}{subsection.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}方向二}{24}{subsection.9.2}\protected@file@percent }
\citation{xu2023low}
\citation{zhu2020eemefn}
\citation{rana2021edge}
\bibstyle{unsrt}
\bibdata{reference}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces   Low-Lighting 图像恢复网络。结构受 Conformer\cite  {peng2021conformer} （见 Fig. \ref {fig: Conformer architecture}）启发，将原来结构中 CNN 分支的 ResNet 结构修改为 U-Net 结构。其采用一个 U-Net 和 ViT 的并行架构，通过 U-Net 结构得到一个弱恢复的弱特征图 $\mathcal  {F}_2$，通过 ViT 融合的特征可以初步增强弱特征图 $\mathcal  {F}_2$，ViT 的输出经过 Patch Expanding 的特征图 $\mathcal  {F}_3$ 经过与 U-Net 输出的特征图 $\mathcal  {F}_2$ 融合之后得到一个初步恢复的图片 output image，用以后续参与图片的进一步恢复。 \relax }}{25}{figure.caption.26}\protected@file@percent }
\newlabel{fig: The proposed architecture}{{23}{25}{Low-Lighting 图像恢复网络。结构受 Conformer\cite {peng2021conformer} （见 Fig. \ref {fig: Conformer architecture}）启发，将原来结构中 CNN 分支的 ResNet 结构修改为 U-Net 结构。其采用一个 U-Net 和 ViT 的并行架构，通过 U-Net 结构得到一个弱恢复的弱特征图 $\mathcal {F}_2$，通过 ViT 融合的特征可以初步增强弱特征图 $\mathcal {F}_2$，ViT 的输出经过 Patch Expanding 的特征图 $\mathcal {F}_3$ 经过与 U-Net 输出的特征图 $\mathcal {F}_2$ 融合之后得到一个初步恢复的图片 output image，用以后续参与图片的进一步恢复。 \relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces   Uformer 的结构。 \relax }}{26}{figure.caption.27}\protected@file@percent }
\newlabel{fig: Uformer}{{24}{26}{Uformer 的结构。 \relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces   MCLNet 的结构 \relax }}{26}{figure.caption.28}\protected@file@percent }
\newlabel{fig: MCLNet}{{25}{26}{MCLNet 的结构 \relax }{figure.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces   该结构的边缘图直接从 $I$ 中通过 StyleGAN 得到边缘图 $I_s$, 而非从 $I_{\alpha }$ 中通过边缘网络获取边缘图。 \relax }}{26}{figure.caption.29}\protected@file@percent }
\newlabel{fig: Overview}{{26}{26}{该结构的边缘图直接从 $I$ 中通过 StyleGAN 得到边缘图 $I_s$, 而非从 $I_{\alpha }$ 中通过边缘网络获取边缘图。 \relax }{figure.caption.29}{}}
\bibcite{liu2021retinex}{1}
\bibcite{xu2020learning}{2}
\bibcite{ueng1995gamma}{3}
\bibcite{stark2000adaptive}{4}
\bibcite{land1971lightness}{5}
\bibcite{liu2021benchmarking}{6}
\bibcite{dai2019fractional}{7}
\bibcite{ma2019improved}{8}
\bibcite{cooper2004analysis}{9}
\bibcite{202013}{10}
\bibcite{dong2010fast}{11}
\bibcite{lore2017llnet}{12}
\bibcite{tang2023low}{13}
\bibcite{lv2018mbllen}{14}
\bibcite{wang2018gladnet}{15}
\bibcite{lu2020tbefn}{16}
\bibcite{li2021low}{17}
\bibcite{ravirathinam2021c}{18}
\bibcite{lim2020dslr}{19}
\bibcite{wei2018deep}{20}
\bibcite{shen2017msr}{21}
\bibcite{zhang2019kindling}{22}
\bibcite{cui2022illumination}{23}
\bibcite{carion2020end}{24}
\bibcite{wang2023ultra}{25}
\bibcite{jiang2021enlightengan}{26}
\bibcite{fu2022gan}{27}
\bibcite{ni2020towards}{28}
\bibcite{zhang2021unsupervised}{29}
\bibcite{qiao2021deep}{30}
\bibcite{robert2018hybridnet}{31}
\bibcite{zhu2020zero}{32}
\bibcite{zhang2019zero}{33}
\bibcite{yang2021locally}{34}
\bibcite{zhang2020attention}{35}
\bibcite{li2018multi}{36}
\bibcite{zamir2020learning}{37}
\bibcite{li2020visual}{38}
\bibcite{xu2023low}{39}
\bibcite{zhu2020eemefn}{40}
\bibcite{rana2021edge}{41}
\bibcite{chen2018learning}{42}
\bibcite{wang2004image}{43}
\bibcite{pietikainen2010local}{44}
\bibcite{maini2009study}{45}
\bibcite{liu2017richer}{46}
\bibcite{woo2018cbam}{47}
\bibcite{ramachandran2019stand}{48}
\bibcite{li2023scconv}{49}
\bibcite{zamir2021learning}{50}
\bibcite{meng2020gia}{51}
\bibcite{zhou2018unet++}{52}
\bibcite{zhou2019unet++}{53}
\bibcite{vaswani2017attention}{54}
\bibcite{dosovitskiy2020image}{55}
\bibcite{wang2022ultrahighdefinition}{56}
\bibcite{wang2021uformer}{57}
\bibcite{chen2023cross}{58}
\bibcite{jain1991unsupervised}{59}
\bibcite{lowe2004distinctive}{60}
\bibcite{ojala2002multiresolution}{61}
\bibcite{lisin2005combining}{62}
\bibcite{peng2021conformer}{63}
\bibcite{karu1996there}{64}
\bibcite{huber1992robust}{65}
\bibcite{johnson2016perceptual}{66}
\bibcite{wang2022uformer}{67}
\bibcite{li2023effective}{68}
\newlabel{LastPage}{{}{31}{}{page.31}{}}
\xdef\lastpage@lastpage{31}
\xdef\lastpage@lastpageHy{31}
\gdef \@abspage@last{31}
