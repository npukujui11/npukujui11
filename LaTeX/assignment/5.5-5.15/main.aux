\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}文献阅读}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Pre-knowledge}{1}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}loss function for CV}{2}{subsubsection.1.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{$\mathcal  {L}_1$-loss}{2}{subsubsection.1.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{$\mathcal  {L}_2$-loss}{2}{equation.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{loss正则化}{3}{equation.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Smooth $\mathcal  {L}_1$ loss}{4}{lstnumber.-1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Huber loss}{4}{equation.1.5}\protected@file@percent }
\citation{johnson2016perceptual}
\citation{johnson2016perceptual}
\@writefile{toc}{\contentsline {paragraph}{log-MSE}{5}{equation.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Perceptual loss}{5}{equation.1.7}\protected@file@percent }
\newlabel{eq: perceptual loss}{{8}{5}{Perceptual loss}{equation.1.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces   System overview. We train an image transformation network to transform input images into output images. We use a loss network pretrained for image classification to define perceptual loss functions that measure perceptual differences in content and style between images. The loss network remains fixed during the training process. \relax }}{6}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig: perceptual loss}{{1}{6}{System overview. We train an image transformation network to transform input images into output images. We use a loss network pretrained for image classification to define perceptual loss functions that measure perceptual differences in content and style between images. The loss network remains fixed during the training process. \relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {paragraph}{SSIM loss}{6}{figure.caption.1}\protected@file@percent }
\newlabel{eq: SSIM}{{9}{6}{SSIM loss}{equation.1.9}{}}
\newlabel{eq: SSIM loss}{{10}{6}{SSIM loss}{equation.1.10}{}}
\newlabel{eq: revised_SSIM loss}{{11}{6}{SSIM loss}{equation.1.11}{}}
\@writefile{toc}{\contentsline {paragraph}{MS-SSIM loss}{7}{equation.1.11}\protected@file@percent }
\newlabel{eq: MS-SSIM}{{12}{7}{MS-SSIM loss}{equation.1.12}{}}
\newlabel{eq: MS-SSIM loss}{{13}{7}{MS-SSIM loss}{equation.1.13}{}}
\@writefile{toc}{\contentsline {paragraph}{Cross-entropy loss function}{7}{equation.1.13}\protected@file@percent }
\newlabel{eq: Cross-entropy loss}{{14}{7}{Cross-entropy loss function}{equation.1.14}{}}
\newlabel{eq: Cross-entropy loss}{{15}{7}{Cross-entropy loss function}{equation.1.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}(综述2021.04)Lighting the darkness in the deep learning era}{8}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}背景}{8}{subsubsection.1.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2}介绍}{8}{subsubsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.3}细节}{8}{subsubsection.1.2.3}\protected@file@percent }
\newlabel{fig:subfig_a}{{2a}{8}{learning strategy\relax }{figure.caption.3}{}}
\newlabel{sub@fig:subfig_a}{{a}{8}{learning strategy\relax }{figure.caption.3}{}}
\newlabel{fig:subfig_b}{{2b}{8}{network structure\relax }{figure.caption.3}{}}
\newlabel{sub@fig:subfig_b}{{b}{8}{network structure\relax }{figure.caption.3}{}}
\newlabel{fig:subfig_c}{{2c}{8}{Retinex model\relax }{figure.caption.3}{}}
\newlabel{sub@fig:subfig_c}{{c}{8}{Retinex model\relax }{figure.caption.3}{}}
\newlabel{fig:subfig_d}{{2d}{8}{data format\relax }{figure.caption.3}{}}
\newlabel{sub@fig:subfig_d}{{d}{8}{data format\relax }{figure.caption.3}{}}
\newlabel{fig:subfig_e}{{2e}{8}{loss function\relax }{figure.caption.3}{}}
\newlabel{sub@fig:subfig_e}{{e}{8}{loss function\relax }{figure.caption.3}{}}
\newlabel{fig:subfig_f}{{2f}{8}{training dataset\relax }{figure.caption.3}{}}
\newlabel{sub@fig:subfig_f}{{f}{8}{training dataset\relax }{figure.caption.3}{}}
\newlabel{fig:subfig_g}{{2g}{8}{testing dataset\relax }{figure.caption.3}{}}
\newlabel{sub@fig:subfig_g}{{g}{8}{testing dataset\relax }{figure.caption.3}{}}
\newlabel{fig:subfig_h}{{2h}{8}{evaluation metric\relax }{figure.caption.3}{}}
\newlabel{sub@fig:subfig_h}{{h}{8}{evaluation metric\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces   A statictic analysis of deep learning-based LLIE methods, including learning strategy, network characteristic, Retinex model, data format, loss function, training dataset, testing dataset, and evaluation metric. Better to see with zoom. \relax }}{8}{figure.caption.3}\protected@file@percent }
\newlabel{fig: Statictic Analysis}{{2}{8}{A statictic analysis of deep learning-based LLIE methods, including learning strategy, network characteristic, Retinex model, data format, loss function, training dataset, testing dataset, and evaluation metric. Better to see with zoom. \relax }{figure.caption.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces  Summary of essential characteristics of representative deep learning-based methods, including learning strategies, network structures, loss functions, training datasets, testing datasets, evaluation metrics, data formats of input, and whether the models are Retinex-based or not. "simulated" means the testing data are simulated by the same approach as the synthetic training data. "self-selected" stands for the real-world images selected by the authors. "\#P" represents the number of trainable parameters. "-" means this item is not available or not indicated in the paper.\relax }}{9}{table.caption.2}\protected@file@percent }
\newlabel{tab: Summary}{{1}{9}{Summary of essential characteristics of representative deep learning-based methods, including learning strategies, network structures, loss functions, training datasets, testing datasets, evaluation metrics, data formats of input, and whether the models are Retinex-based or not. "simulated" means the testing data are simulated by the same approach as the synthetic training data. "self-selected" stands for the real-world images selected by the authors. "\#P" represents the number of trainable parameters. "-" means this item is not available or not indicated in the paper.\relax }{table.caption.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Network Structure}{10}{figure.caption.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Combination of Deep Model and Retinex Theory}{10}{figure.caption.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Data Format}{10}{figure.caption.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Loss Function}{10}{figure.caption.3}\protected@file@percent }
\citation{wei2018deep}
\citation{cai2018learning}
\citation{jiang2019learning}
\citation{bychkovsky2011learning}
\citation{wei2018deep}
\citation{chen2019seeing}
\citation{jiang2019learning}
\citation{guo2016lime}
\citation{wang2013naturalness}
\citation{lee2011power}
\citation{lee2013contrast}
\citation{yu2020bdd100k}
\citation{loh2019getting}
\citation{yuan2019ug}
\citation{jiang2019learning}
\@writefile{toc}{\contentsline {paragraph}{Training Datasets}{11}{figure.caption.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces  Summary of paired training datasets. 'Syn' represents Synthetic.\relax }}{11}{table.caption.4}\protected@file@percent }
\newlabel{tab: Paired_training_datases}{{2}{11}{Summary of paired training datasets. 'Syn' represents Synthetic.\relax }{table.caption.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Testing Dataset}{11}{table.caption.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces  Summary of testing datasets.\relax }}{11}{table.caption.5}\protected@file@percent }
\newlabel{tab: Testing datasets}{{3}{11}{Summary of testing datasets.\relax }{table.caption.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Benchmarking and Empirical Analysis}{11}{table.caption.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{A New Low-light Image and Video Dataset}{12}{table.caption.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces  Summary of LoLi-Phone dataset. LoLi-Phone dataset contains 120 videos (55,148 images) taken by 18 different mobile phones' cameras. "\#Video" and "\#Image" represent the number of videos and images, respectively.\relax }}{12}{table.caption.6}\protected@file@percent }
\newlabel{tab: LoLi-Phone dataset}{{4}{12}{Summary of LoLi-Phone dataset. LoLi-Phone dataset contains 120 videos (55,148 images) taken by 18 different mobile phones' cameras. "\#Video" and "\#Image" represent the number of videos and images, respectively.\relax }{table.caption.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Online Evaluation Platform}{12}{figure.caption.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces   Several images sampled from the proposed LoLiPhone dataset. The images and videos are taken by different devices under diverse lighting conditions and scenes. \relax }}{13}{figure.caption.7}\protected@file@percent }
\newlabel{fig:Sample_Loli}{{3}{13}{Several images sampled from the proposed LoLiPhone dataset. The images and videos are taken by different devices under diverse lighting conditions and scenes. \relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {paragraph}{Benchmark Results}{13}{figure.caption.7}\protected@file@percent }
\newlabel{fig: LOL-test_a}{{4a}{14}{input\relax }{figure.caption.8}{}}
\newlabel{sub@fig: LOL-test_a}{{a}{14}{input\relax }{figure.caption.8}{}}
\newlabel{fig: LOL-test_b}{{4b}{14}{LLNet\relax }{figure.caption.8}{}}
\newlabel{sub@fig: LOL-test_b}{{b}{14}{LLNet\relax }{figure.caption.8}{}}
\newlabel{fig: LOL-test_c}{{4c}{14}{LightenNet\relax }{figure.caption.8}{}}
\newlabel{sub@fig: LOL-test_c}{{c}{14}{LightenNet\relax }{figure.caption.8}{}}
\newlabel{fig: LOL-test_d}{{4d}{14}{Retinex-Net\relax }{figure.caption.8}{}}
\newlabel{sub@fig: LOL-test_d}{{d}{14}{Retinex-Net\relax }{figure.caption.8}{}}
\newlabel{fig: LOL-test_e}{{4e}{14}{MBLLEN\relax }{figure.caption.8}{}}
\newlabel{sub@fig: LOL-test_e}{{e}{14}{MBLLEN\relax }{figure.caption.8}{}}
\newlabel{fig: LOL-test_f}{{4f}{14}{KinD\relax }{figure.caption.8}{}}
\newlabel{sub@fig: LOL-test_f}{{f}{14}{KinD\relax }{figure.caption.8}{}}
\newlabel{fig: LOL-test_g}{{4g}{14}{KinD++\relax }{figure.caption.8}{}}
\newlabel{sub@fig: LOL-test_g}{{g}{14}{KinD++\relax }{figure.caption.8}{}}
\newlabel{fig: LOL-test_h}{{4h}{14}{TBEFN\relax }{figure.caption.8}{}}
\newlabel{sub@fig: LOL-test_h}{{h}{14}{TBEFN\relax }{figure.caption.8}{}}
\newlabel{fig: LOL-test_i}{{4i}{14}{DSLR\relax }{figure.caption.8}{}}
\newlabel{sub@fig: LOL-test_i}{{i}{14}{DSLR\relax }{figure.caption.8}{}}
\newlabel{fig: LOL-test_j}{{4j}{14}{EnlightenGAN\relax }{figure.caption.8}{}}
\newlabel{sub@fig: LOL-test_j}{{j}{14}{EnlightenGAN\relax }{figure.caption.8}{}}
\newlabel{fig: LOL-test_k}{{4k}{14}{DRBN\relax }{figure.caption.8}{}}
\newlabel{sub@fig: LOL-test_k}{{k}{14}{DRBN\relax }{figure.caption.8}{}}
\newlabel{fig: LOL-test_l}{{4l}{14}{ExCNet\relax }{figure.caption.8}{}}
\newlabel{sub@fig: LOL-test_l}{{l}{14}{ExCNet\relax }{figure.caption.8}{}}
\newlabel{fig: LOL-test_m}{{4m}{14}{Zero-DCE\relax }{figure.caption.8}{}}
\newlabel{sub@fig: LOL-test_m}{{m}{14}{Zero-DCE\relax }{figure.caption.8}{}}
\newlabel{fig: LOL-test_n}{{4n}{14}{RRDNet\relax }{figure.caption.8}{}}
\newlabel{sub@fig: LOL-test_n}{{n}{14}{RRDNet\relax }{figure.caption.8}{}}
\newlabel{fig: LOL-test_o}{{4o}{14}{GT\relax }{figure.caption.8}{}}
\newlabel{sub@fig: LOL-test_o}{{o}{14}{GT\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces   Visual results of different methods on a low-light image sampled from LOL-test dataset. \relax }}{14}{figure.caption.8}\protected@file@percent }
\newlabel{fig: Visual Result from LOL-test dataset}{{4}{14}{Visual results of different methods on a low-light image sampled from LOL-test dataset. \relax }{figure.caption.8}{}}
\newlabel{fig: MIT-Adobe_FiveK_a}{{5a}{14}{input\relax }{figure.caption.9}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_a}{{a}{14}{input\relax }{figure.caption.9}{}}
\newlabel{fig: MIT-Adobe_FiveK_b}{{5b}{14}{LLNet\relax }{figure.caption.9}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_b}{{b}{14}{LLNet\relax }{figure.caption.9}{}}
\newlabel{fig: MIT-Adobe_FiveK_c}{{5c}{14}{LightenNet\relax }{figure.caption.9}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_c}{{c}{14}{LightenNet\relax }{figure.caption.9}{}}
\newlabel{fig: MIT-Adobe_FiveK_d}{{5d}{14}{Retinex-Net\relax }{figure.caption.9}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_d}{{d}{14}{Retinex-Net\relax }{figure.caption.9}{}}
\newlabel{fig: MIT-Adobe_FiveK_e}{{5e}{14}{MBLLEN\relax }{figure.caption.9}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_e}{{e}{14}{MBLLEN\relax }{figure.caption.9}{}}
\newlabel{fig: MIT-Adobe_FiveK_f}{{5f}{14}{KinD\relax }{figure.caption.9}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_f}{{f}{14}{KinD\relax }{figure.caption.9}{}}
\newlabel{fig: MIT-Adobe_FiveK_g}{{5g}{14}{KinD++\relax }{figure.caption.9}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_g}{{g}{14}{KinD++\relax }{figure.caption.9}{}}
\newlabel{fig: MIT-Adobe_FiveK_h}{{5h}{14}{TBEFN\relax }{figure.caption.9}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_h}{{h}{14}{TBEFN\relax }{figure.caption.9}{}}
\newlabel{fig: MIT-Adobe_FiveK_i}{{5i}{14}{DSLR\relax }{figure.caption.9}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_i}{{i}{14}{DSLR\relax }{figure.caption.9}{}}
\newlabel{fig: MIT-Adobe_FiveK_j}{{5j}{14}{EnlightenGAN\relax }{figure.caption.9}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_j}{{j}{14}{EnlightenGAN\relax }{figure.caption.9}{}}
\newlabel{fig: MIT-Adobe_FiveK_k}{{5k}{14}{DRBN\relax }{figure.caption.9}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_k}{{k}{14}{DRBN\relax }{figure.caption.9}{}}
\newlabel{fig: MIT-Adobe_FiveK_l}{{5l}{14}{ExCNet\relax }{figure.caption.9}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_l}{{l}{14}{ExCNet\relax }{figure.caption.9}{}}
\newlabel{fig: MIT-Adobe_FiveK_m}{{5m}{14}{Zero-DCE\relax }{figure.caption.9}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_m}{{m}{14}{Zero-DCE\relax }{figure.caption.9}{}}
\newlabel{fig: MIT-Adobe_FiveK_n}{{5n}{14}{RRDNet\relax }{figure.caption.9}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_n}{{n}{14}{RRDNet\relax }{figure.caption.9}{}}
\newlabel{fig: MIT-Adobe_FiveK_o}{{5o}{14}{GT\relax }{figure.caption.9}{}}
\newlabel{sub@fig: MIT-Adobe_FiveK_o}{{o}{14}{GT\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces   Visual results of different methods on a low-light image sampled from MIT-Adobe FiveK-test dataset. \relax }}{14}{figure.caption.9}\protected@file@percent }
\newlabel{fig: Visual Result from MIT-Adobe FiveK dataset}{{5}{14}{Visual results of different methods on a low-light image sampled from MIT-Adobe FiveK-test dataset. \relax }{figure.caption.9}{}}
\newlabel{fig: LoLi-Phone-imgT_a}{{6a}{15}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_a}{{a}{15}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_b}{{6b}{15}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_b}{{b}{15}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_c}{{6c}{15}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_c}{{c}{15}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_d}{{6d}{15}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_d}{{d}{15}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_e}{{6e}{15}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_e}{{e}{15}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_f}{{6f}{15}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_f}{{f}{15}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_g}{{6g}{15}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_g}{{g}{15}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_h}{{6h}{15}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_h}{{h}{15}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_i}{{6i}{15}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_i}{{i}{15}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_j}{{6j}{15}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_j}{{j}{15}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_k}{{6k}{15}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_k}{{k}{15}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_l}{{6l}{15}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_l}{{l}{15}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_m}{{6m}{15}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_m}{{m}{15}{\relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_n}{{6n}{15}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_n}{{n}{15}{\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces   Visual results of different methods on a low-light image sampled from LoLi-Phone-imgT dataset. (a) input (b) LLNet (c) LightenNet (d) Retinex-Net (e) MBLLEN (f) KinD (g) KinD++ (h) TBEFN (i) DSLR (j) EnlightenGAN (k) DRBN (l) ExCNet (m) Zero-DCE (n) RRDNet \relax }}{15}{figure.caption.10}\protected@file@percent }
\newlabel{fig: Visual Result from LoLi-Phone-imgT dataset}{{6}{15}{Visual results of different methods on a low-light image sampled from LoLi-Phone-imgT dataset. (a) input (b) LLNet (c) LightenNet (d) Retinex-Net (e) MBLLEN (f) KinD (g) KinD++ (h) TBEFN (i) DSLR (j) EnlightenGAN (k) DRBN (l) ExCNet (m) Zero-DCE (n) RRDNet \relax }{figure.caption.10}{}}
\newlabel{fig: LoLi-Phone-imgT_1_a}{{7a}{15}{\relax }{figure.caption.11}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_a}{{a}{15}{\relax }{figure.caption.11}{}}
\newlabel{fig: LoLi-Phone-imgT_1_b}{{7b}{15}{\relax }{figure.caption.11}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_b}{{b}{15}{\relax }{figure.caption.11}{}}
\newlabel{fig: LoLi-Phone-imgT_1_c}{{7c}{15}{\relax }{figure.caption.11}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_c}{{c}{15}{\relax }{figure.caption.11}{}}
\newlabel{fig: LoLi-Phone-imgT_1_d}{{7d}{15}{\relax }{figure.caption.11}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_d}{{d}{15}{\relax }{figure.caption.11}{}}
\newlabel{fig: LoLi-Phone-imgT_1_e}{{7e}{15}{\relax }{figure.caption.11}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_e}{{e}{15}{\relax }{figure.caption.11}{}}
\newlabel{fig: LoLi-Phone-imgT_1_f}{{7f}{15}{\relax }{figure.caption.11}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_f}{{f}{15}{\relax }{figure.caption.11}{}}
\newlabel{fig: LoLi-Phone-imgT_1_g}{{7g}{15}{\relax }{figure.caption.11}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_g}{{g}{15}{\relax }{figure.caption.11}{}}
\newlabel{fig: LoLi-Phone-imgT_1_h}{{7h}{15}{\relax }{figure.caption.11}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_h}{{h}{15}{\relax }{figure.caption.11}{}}
\newlabel{fig: LoLi-Phone-imgT_1_i}{{7i}{15}{\relax }{figure.caption.11}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_i}{{i}{15}{\relax }{figure.caption.11}{}}
\newlabel{fig: LoLi-Phone-imgT_1_j}{{7j}{15}{\relax }{figure.caption.11}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_j}{{j}{15}{\relax }{figure.caption.11}{}}
\newlabel{fig: LoLi-Phone-imgT_1_k}{{7k}{15}{\relax }{figure.caption.11}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_k}{{k}{15}{\relax }{figure.caption.11}{}}
\newlabel{fig: LoLi-Phone-imgT_1_l}{{7l}{15}{\relax }{figure.caption.11}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_l}{{l}{15}{\relax }{figure.caption.11}{}}
\newlabel{fig: LoLi-Phone-imgT_1_m}{{7m}{15}{\relax }{figure.caption.11}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_m}{{m}{15}{\relax }{figure.caption.11}{}}
\newlabel{fig: LoLi-Phone-imgT_1_n}{{7n}{15}{\relax }{figure.caption.11}{}}
\newlabel{sub@fig: LoLi-Phone-imgT_1_n}{{n}{15}{\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces   Visual results of different methods on a low-light image sampled from LoLi-Phone-imgT dataset. (a) input (b) LLNet (c) LightenNet (d) Retinex-Net (e) MBLLEN (f) KinD (g) KinD++ (h) TBEFN (i) DSLR (j) EnlightenGAN (k) DRBN (l) ExCNet (m) Zero-DCE (n) RRDNet \relax }}{15}{figure.caption.11}\protected@file@percent }
\newlabel{fig: Visual Result from LoLi-Phone-imgT_1 dataset}{{7}{15}{Visual results of different methods on a low-light image sampled from LoLi-Phone-imgT dataset. (a) input (b) LLNet (c) LightenNet (d) Retinex-Net (e) MBLLEN (f) KinD (g) KinD++ (h) TBEFN (i) DSLR (j) EnlightenGAN (k) DRBN (l) ExCNet (m) Zero-DCE (n) RRDNet \relax }{figure.caption.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces  Quantitative comparisons on LOL-test and MIT-Adobe FiveK-test testing datasets in terms of MSE (×103), PSNR (in dB), SSIM, and LPIPS. The best result is in red whereas the second and third best results are in blue and purple under each case, respectively.\relax }}{16}{table.caption.12}\protected@file@percent }
\newlabel{tab: Quantitative Comparisons on LOL-test and MIT-Adobe FiveK-test testing datasets}{{5}{16}{Quantitative comparisons on LOL-test and MIT-Adobe FiveK-test testing datasets in terms of MSE (×103), PSNR (in dB), SSIM, and LPIPS. The best result is in red whereas the second and third best results are in blue and purple under each case, respectively.\relax }{table.caption.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces  Quantitative comparisons on LoLi-Phone-imgT dataset in terms of NIQE, LOE, PI, and SPAQ. The best result is in red whereas the second and third best results are in blue and purple under each case, respectively.\relax }}{16}{table.caption.13}\protected@file@percent }
\newlabel{tab: Quantitative comparisons on LoLi-Phone-imgT dataset}{{6}{16}{Quantitative comparisons on LoLi-Phone-imgT dataset in terms of NIQE, LOE, PI, and SPAQ. The best result is in red whereas the second and third best results are in blue and purple under each case, respectively.\relax }{table.caption.13}{}}
\@writefile{toc}{\contentsline {paragraph}{Computational Complexity}{17}{table.caption.13}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces  Quantitative comparisons of computational complexity in terms of runtime (in second), number of trainable parameters (\#Parameters) (in M), and FLOPs (in G). The best result is in \textcolor {red}{red} whereas the second and third best results are in \textcolor {blue}{blue} and \textcolor {purple}{purple} under each case, respectively. ‘-’ indicates the result is not available.\relax }}{17}{table.caption.14}\protected@file@percent }
\newlabel{tab: Quantitative comparisons of computational complexity}{{7}{17}{Quantitative comparisons of computational complexity in terms of runtime (in second), number of trainable parameters (\#Parameters) (in M), and FLOPs (in G). The best result is in \textcolor {red}{red} whereas the second and third best results are in \textcolor {blue}{blue} and \textcolor {purple}{purple} under each case, respectively. ‘-’ indicates the result is not available.\relax }{table.caption.14}{}}
\@writefile{toc}{\contentsline {paragraph}{Application-based Evaluation}{17}{table.caption.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Discussion}{17}{figure.caption.16}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces   The P-R curves of face detection in the dark. \relax }}{18}{figure.caption.15}\protected@file@percent }
\newlabel{fig: P-R_curves}{{8}{18}{The P-R curves of face detection in the dark. \relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {paragraph}{Future Research Directions}{18}{figure.caption.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Effective Learning Strategies}{18}{figure.caption.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Specialized Network Structures}{18}{figure.caption.16}\protected@file@percent }
\newlabel{fig: DARK_FACE_a}{{9a}{19}{input\relax }{figure.caption.16}{}}
\newlabel{sub@fig: DARK_FACE_a}{{a}{19}{input\relax }{figure.caption.16}{}}
\newlabel{fig: DARK_FACE_b}{{9b}{19}{LightenNet\relax }{figure.caption.16}{}}
\newlabel{sub@fig: DARK_FACE_b}{{b}{19}{LightenNet\relax }{figure.caption.16}{}}
\newlabel{fig: DARK_FACE_c}{{9c}{19}{Retinex-Net\relax }{figure.caption.16}{}}
\newlabel{sub@fig: DARK_FACE_c}{{c}{19}{Retinex-Net\relax }{figure.caption.16}{}}
\newlabel{fig: DARK_FACE_d}{{9d}{19}{MBLLEN\relax }{figure.caption.16}{}}
\newlabel{sub@fig: DARK_FACE_d}{{d}{19}{MBLLEN\relax }{figure.caption.16}{}}
\newlabel{fig: DARK_FACE_e}{{9e}{19}{KinD++\relax }{figure.caption.16}{}}
\newlabel{sub@fig: DARK_FACE_e}{{e}{19}{KinD++\relax }{figure.caption.16}{}}
\newlabel{fig: DARK_FACE_f}{{9f}{19}{TBEFN\relax }{figure.caption.16}{}}
\newlabel{sub@fig: DARK_FACE_f}{{f}{19}{TBEFN\relax }{figure.caption.16}{}}
\newlabel{fig: DARK_FACE_g}{{9g}{19}{DSLR\relax }{figure.caption.16}{}}
\newlabel{sub@fig: DARK_FACE_g}{{g}{19}{DSLR\relax }{figure.caption.16}{}}
\newlabel{fig: DARK_FACE_h}{{9h}{19}{EnlightenGAN\relax }{figure.caption.16}{}}
\newlabel{sub@fig: DARK_FACE_h}{{h}{19}{EnlightenGAN\relax }{figure.caption.16}{}}
\newlabel{fig:DARK_FACE_i}{{9i}{19}{DRBN\relax }{figure.caption.16}{}}
\newlabel{sub@fig:DARK_FACE_i}{{i}{19}{DRBN\relax }{figure.caption.16}{}}
\newlabel{fig: DARK_FACE_j}{{9j}{19}{ExCNet\relax }{figure.caption.16}{}}
\newlabel{sub@fig: DARK_FACE_j}{{j}{19}{ExCNet\relax }{figure.caption.16}{}}
\newlabel{fig: DARK_FACE_k}{{9k}{19}{Zero-DCE\relax }{figure.caption.16}{}}
\newlabel{sub@fig: DARK_FACE_k}{{k}{19}{Zero-DCE\relax }{figure.caption.16}{}}
\newlabel{fig: DARK_FACE_l}{{9l}{19}{RRDNet\relax }{figure.caption.16}{}}
\newlabel{sub@fig: DARK_FACE_l}{{l}{19}{RRDNet\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces   Visual results of different methods on a low-light image sampled from DARK FACE dataset. Better see with zoom in for the bounding boxes of faces. \relax }}{19}{figure.caption.16}\protected@file@percent }
\newlabel{fig: Visual Result from DARK_FACE dataset}{{9}{19}{Visual results of different methods on a low-light image sampled from DARK FACE dataset. Better see with zoom in for the bounding boxes of faces. \relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subparagraph}{Loss Function}{19}{figure.caption.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Realistic Training Data}{19}{figure.caption.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Standard Testing Data}{19}{figure.caption.16}\protected@file@percent }
\citation{fang2020perceptual}
\citation{talebi2018nima}
\@writefile{toc}{\contentsline {subparagraph}{Task-Specific Evaluation Metrics}{20}{figure.caption.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Robust Generalization Capability}{20}{figure.caption.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Extension to Low-light Video Enhancement}{20}{figure.caption.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Integrating Semantic Information}{20}{figure.caption.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.4}项目链接}{20}{subsubsection.1.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}个人工作进展}{20}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}梳理损失函数}{20}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}下周工作计划}{20}{section.3}\protected@file@percent }
\bibstyle{unsrt}
\bibdata{reference}
\bibcite{johnson2016perceptual}{1}
\bibcite{wei2018deep}{2}
\bibcite{cai2018learning}{3}
\bibcite{jiang2019learning}{4}
\bibcite{bychkovsky2011learning}{5}
\bibcite{chen2019seeing}{6}
\bibcite{guo2016lime}{7}
\bibcite{wang2013naturalness}{8}
\bibcite{lee2011power}{9}
\bibcite{lee2013contrast}{10}
\bibcite{yu2020bdd100k}{11}
\bibcite{loh2019getting}{12}
\bibcite{yuan2019ug}{13}
\bibcite{howard2017mobilenets}{14}
\bibcite{liu2020improving}{15}
\bibcite{liu2018progressive}{16}
\bibcite{liu2019auto}{17}
\bibcite{dosovitskiy2020image}{18}
\bibcite{chen2021pre}{19}
\bibcite{fang2020perceptual}{20}
\bibcite{talebi2018nima}{21}
\gdef \@abspage@last{23}
