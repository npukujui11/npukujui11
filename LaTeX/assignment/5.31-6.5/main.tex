\documentclass[letterpaper,12pt]{article}

\usepackage{tabularx} % extra features for tabular environment
\usepackage{amsmath}  % improve math presentation
\usepackage{graphicx} % takes care of graphic including machinery
\usepackage[margin=1in,letterpaper]{geometry} % decreases margins
\usepackage{cite} % takes care of citations
\usepackage[final]{hyperref} % adds hyper links inside the generated pdf file
\usepackage{ctex}
\usepackage{titlesec}
%\usepackage{CJKutf8, CJK}
\usepackage{makecell}                 % 三线表-竖线
\usepackage{booktabs}                 % 三线表-短细横线
% \usepackage{natbib}
\usepackage{graphicx}				  % 表格单元格逆时针
\usepackage{multirow}				  % 合并单元格
\usepackage{array}
\usepackage{amssymb}				  % 勾
\usepackage{amsmath}
\usepackage{longtable}                % 导入 longtable 宏包，表格自动换行
\usepackage{caption}
\usepackage{subcaption}               % 设置子图
\usepackage{color}					  % 文本颜色包
\usepackage{xcolor}
\usepackage{bbm}					  % 输入指示函数
\usepackage{tablefootnote}			  % 表格注释
\usepackage{pythonhighlight}

\usepackage{listings}                 % 导入代码块
\usepackage{xcolor}
\lstset{
	numbers=left, 
	tabsize=1,
	columns=flexible, 
	numberstyle=  \small, 
	keywordstyle= \color{ blue!70},
	commentstyle= \color{red!50!green!50!blue!50}, 
	frame=shadowbox, % 阴影效果
	rulesepcolor= \color{ red!20!green!20!blue!20} ,
	escapeinside=``, % 英文分号中可写入中文
	xleftmargin=2em,
	xrightmargin=2em, 
	aboveskip=1em,
} 

\hypersetup{
	colorlinks=true,       % false: boxed links; true: colored links
	linkcolor=blue,        % color of internal links
	citecolor=blue,        % color of links to bibliography
	filecolor=magenta,     % color of file links
	urlcolor=blue         
}
%++++++++++++++++++++++++++++++++++++++++
\titleformat{\section}{\Large\bfseries\songti}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries\songti}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalsize\bfseries\songti}{\thesubsubsection}{1em}{}
\titleformat{\paragraph}{\small\bfseries\songti}{\paragraph}{1em}{}
\titleformat{\subparagraph}{\footnotesize\bfseries\songti}{\subparagraph}{1em}{}

\begin{document}
	
	
	\title{\songti \zihao{4}5月30日-6月5日工作汇报}
	\author{\textrm{Ku Jui}}
	\date{\textrm{May 2023}}
	\maketitle
	
	\renewcommand{\figurename}{Figure} % 可以重新定义abstract，因为ctex会覆盖thebibliography
	% 	\begin{abstract}
		%		In this experiment we studied a very important physical effect by measuring the
		%		dependence of a quantity $V$ of the quantity $X$ for two different sample
		%		temperatures.  Our experimental measurements confirmed the quadratic dependence
		%		$V = kX^2$ predicted by Someone's first law. The value of the mystery parameter
		%		$k = 15.4\pm 0.5$~s was extracted from the fit. This value is
		%		not consistent with the theoretically predicted $k_{theory}=17.34$~s. We attribute %this
		%		discrepancy to low efficiency of our $V$-detector.
		%	\end{abstract}
	\renewcommand{\contentsname}{Contents}
	\renewcommand{\tablename}{Table}
	\tableofcontents  % 自动生成目录
	
	\section{资料调研}
	
		\subsection{局限性}
	
			现有的方法仍有很大的改进空间，例如如何在提高亮度的同时消除产生的噪声，如何避免颜色失真现象等。一些现有的方法可以有效地解决一个问题，但往往会忽略了其他问题。不同的方法在不同的数据集上往往具有不同的优势，即在不同的评估标准下有不同的优势。
			
			例如，在LOL数据集（目前应用最广泛的数据集）下URetinexNet具有十分卓越的性能，因为其在PSNR、SSIM、MSE指标上目前排名第一，但是URetineNet在SCIE数据集下，MBLLEN方法在PSNR、SSIM、MSE指标中排名第一。
			
			\begin{figure}[htbp] 
				% read manual to see what [ht] means and for other possible options
				\centering 
				% \includegraphics[width=0.8\columnwidth]{GLADNet}
				\begin{subfigure}{0.19\textwidth}
					\includegraphics[width=\linewidth]{VE-LOL-L/input}
					\captionsetup{font=scriptsize}
					\caption{input \\ \quad }
					\label{fig: input}
				\end{subfigure}
				\begin{subfigure}{0.19\textwidth}
					\includegraphics[width=\linewidth]{VE-LOL-L/RetinexNet}
					\captionsetup{font=scriptsize}
					\caption{RetinexNet \\ (2017)}
					\label{fig: LLNet}	
				\end{subfigure}
				\begin{subfigure}{0.19\textwidth}
					\includegraphics[width=\linewidth]{VE-LOL-L/RetinexNet}
					\captionsetup{font=scriptsize}
					\caption{RetinexNet \\ (2018)}
					\label{fig: RetinexNet}	
				\end{subfigure}
				\begin{subfigure}{0.19\textwidth}
					\includegraphics[width=\linewidth]{VE-LOL-L/MBLLEN}
					\captionsetup{font=scriptsize}
					\caption{MBLLEN \\ (2018)}
					\label{fig: MBLLEN}	
				\end{subfigure}
				\begin{subfigure}{0.19\textwidth}
					\includegraphics[width=\linewidth]{VE-LOL-L/URetinexNet}
					\captionsetup{font=scriptsize}
					\caption{URetinexNet \\ (2022)}
					\label{fig: URetinexNet}	
				\end{subfigure}
				\captionsetup{font=scriptsize}
				\caption{
					\label{fig: VE-LOL-L Visual} % spaces are big no-no withing labels
						% things like fig: are optional in the label but it helps
						% to orient yourself when you have multiple figures,
						% equations and tables
						Visual results of different algorithms on low-illumination images sampled from the VE-LOL-L dataset.
				}
			\end{figure}
			
			Fig. \ref{fig: VE-LOL-L Visual}显示了作为一个室内图像的VE-LOL-L数据集的增强效果。很明显，Fig. \ref{fig: RetinexNet}和Fig. \ref{fig: MBLLEN}是红色的，但Fig. \ref{fig: LLNet}和Fig. \ref{fig: URetinexNet}呈现的颜色更为橙色。同样，Fig. \ref{fig: RetinexNet}显示的图像中桌面物品的颜色偏差也很大，且图像的整体颜色不一致。
			
			\subsubsection{Retinex}
	
			与直接端到端网络学习的结果相比，基于深度学习和Retinex理论的方法有时具有更好的增强效果。
			
			Retinex模型方法将低照度图像分解为照度图像和反射图像，通过特定的算法减少甚至消除入射图像的影响，保留物体本质的反射特性图像。然而，该算法也存在一定的局限性。以反射贴图作为增强结果，会导致细节丢失、颜色失真等问题。此外，由于反射图大量噪声产生，该模型忽略了噪声。反射映射本身的计算是一个不适定问题，只能通过近似估计来计算。
			
			\subsubsection{评价指标}
			
			PSNR、MSE、SSIM和IE是比较经典的、流行的图像评价指标，但这些指标还远未达到人类视觉感知的评价能力。同时，虽然这些评价指标可以评价图像质量，但它们不能表达低照度增强图像与实际图像之间的关系。因此，在低照度图像增强的评价指标中，在实现人类视觉效果与机器感知之间的平衡还有相当的局限性。
			
			但是，主观视觉效果较好的增强算法在客观评价中并没有取得良好的效果，这正是目标评价指标需要改进的地方。
			
			同时，现有的客观评价指标用于其他研究领域，如图像除雾、除雨、降噪等。这些指标不是为微光图像增强而设计的，这些评价指标远未达到人类的自然感知效果。
			
			\subsubsection{数据集}
			
			LOL和LE-LOL-L数据集的测试图像的客观评价指标值比SCIE和LIME数据集更适合人类视觉效果。用成对数据集训练的算法将得到更明显的增强效果。从数据中可以看出，基于监督学习的方法比其他方法更能取得更好的效果。
			
			然而，无监督学习、Zero-Shot学习和半监督学习是当前的发展趋势，主要是因为在低光照环境下很难获得成对的图像。与其他方法相比，基于完全监督学习的方法泛化能力较低，适用性较差。此外，无监督学习方法可以通过设计适当的损失函数和网络结构，引入更多反映环境特征的先验知识。
			
			
	
		\subsection{客观评价指标}
		
		图像的客观评价指标很多，不同角度的评价标准也不尽相同。每种评价标准都有其相应的优缺点。到目前为止，还没有明确设计用于微光图像增强的评估指标。现有的图像质量评价方法可分为完全参考评价指标和无参考评价指标。
	
			\paragraph{完全参考指标}
			
			完全参考评价指标需要有一个完整的参考图像（即原始图像或专家标注的高质量图像）作为参照。通过将待评价的图像与参考图像进行比较，计算它们之间的差异来评估图像的质量。常见的完全参考评价指标包括：均方误差（Mean Square Error，MSE）、峰值信噪比（Peak Signal-to-Noise Ratio，PSNR）、结构相似性指数（Structural Similarity Index，SSIM）
			
			\paragraph{无参考评价指标}
			
			无参考评价指标不需要参考图像，仅通过待评价图像自身的特征来评估图像质量。这些指标主要基于图像的统计特性、信息熵等，无需外部参考信息。常见的无参考评价指标包括：1) 图像清晰度评估：通过图像的锐度、对比度等特征来评估图像的清晰度和质量。2) 图像亮度评估：基于图像的亮度直方图、灰度均值等特征，评估图像的亮度质量。3) 图像块失真评估：通过比较图像中局部块之间的差异，来评估图像的块状失真程度。
			
			Table \ref{tab: quality evaluation index} 列出了最新的几个经典关键的质量评价指标。
		
		
			\begin{table}[!htbp]
				\centering
				\tiny
				%\resizebox{\textwidth}{!}{ %按照宽度调整调整表格大小
					\begin{tabular}{cc}
						
						\toprule
						
						\textbf{Abbreviation} & \textbf{Full-/Non-Reference} \\
						
						\hline
						
						MSE (Mean Square Error) & Full-Reference \\
						MAE (Mean Absolute Error) & Full-Reference \\
						SNR (Signal to Noise Ratio) & Full-Reference \\
						PSNR (Peak-Signal to Noise Ratio) & Full-Reference \\
						LPIPS (Learned Perceptual Image Patch Similarity) & Full-Reference \\
						IFC (Information Fidelity Criterion) & Full-Reference \\
						VIF (Visual Information Fidelity) & Full-Reference \\
						SSIM (Structural Similarity Index) & Full-Reference \\
						IE (Information Entropy) & Non-Reference \\
						NIQE (Natural Image Quality Evaluator) & Non-Reference \\
						LOE (Lightness Order Error) & Full-Reference \\
						PI (Perceptual Index) & Non-Reference \\
						MUSIQ (Multi-scale Image Quality Transformer) & Non-Reference \\
						NIMA (Neural Image Assessment) & Non-Reference \\
						SPAQ (Smartphone Photography Attribute and Quality) & Non-Reference \\
						
						\bottomrule
						
					\end{tabular}
					%}
				\captionsetup{font=scriptsize} %设置标题字体与表格字体一致
				\caption{\label{tab: quality evaluation index}
					Abstract of objective quality evaluation index of the image.} %表格的标题
				
			\end{table}
		
			\subsubsection{峰值信噪比 (Peak-Signal to Noise Ratio, PSNR)}
		
			峰值信噪比（PSNR）是应用最广泛的指标之一。单位为dB。它用于测量两个图像之间的差异。如压缩图像与原始图像、压缩图像质量评估、恢复图像与实际图像、恢复算法性能评估等。PSNR值越高，失真越小。PSNR的公式为Eq: \ref{eq: PSNR}
			
			\begin{equation}
				\begin{aligned}
					PSNR = 10 \times \log \frac{{MaxValue}^2}{MSE}
				\end{aligned}
				\label{eq: PSNR}
			\end{equation}
			
			其中MSE是两个图像的均方误差；MaxValue是图像像素的最大值。
			
			\subsubsection{结构相似性 (Structural Similarity, SSIM)}
			
			结构相似性（SSIM）是参考图像质量评价中应用最广泛的标准。SSIM用于突出值范围为0–1的两幅图像之间的亮度、对比度和结构相似性；越接近1，两个图像就越相似。假设x和y是两个输入图像，SSIM的公式为Eq: \ref{eq: SSIM}
	
			\begin{equation}
				\begin{aligned}
					SSIM = {\left[ l(x,y) \right]}^{\alpha} {\left[C(x,y)\right]}^\beta {\left[S(x,y)\right]}^\gamma
				\end{aligned}
				\label{eq: SSIM}
			\end{equation}
	
			其中，$l(x,y)$是亮度比较，$C(x,y)$是对比度比较，$S(x,y)$是结构比较。$\alpha, \beta, \gamma$大于$0$，用于调节三部分比重。$l(x, y)$、$C(x, y)$和$S(x, y)$分别为Eq.\ref{eq: l(x，y)}, Eq.\ref{eq: C(x，y)}, Eq.\ref{eq: (x, y)}
			
			\begin{equation}
				\begin{aligned}
					l(x,y)=\frac{2\mu_x \mu_y + c_1}{\mu x^2 + \mu y^2 + c_1}
				\end{aligned}
				\label{eq: l(x，y)}
			\end{equation}
			
			\begin{equation}
				\begin{aligned}
					C(x,y) = \frac{2\sigma_{xy}+c_2}{\sigma_x^2 + \sigma_ y^2 + c^2}
				\end{aligned}
				\label{eq: C(x，y)}
			\end{equation}
			
			\begin{equation}
				\begin{aligned}
					(x, y) = \frac{\sigma_{xy} + c_3}{\sigma_{x}\sigma_{y} + c_3}
				\end{aligned}
				\label{eq: (x, y)}
			\end{equation}
			
			其中$\mu_x$和$\mu_y$表示两幅图像的平均值，$\sigma_x$和$sigma_y$分别表示两幅图的标准差。$\sigma_xy$表示两个图像的协方差。$c_1$、$c_2$和$c_3$是常数，以避免分母为0。
			
			\subsubsection{均方误差 (Mean Square Error, MSE)}
			
			均方误差（MSE）也是衡量图像质量最常用的指标之一。它是指估计值与真实值之间的平方差的期望值。在图像处理算法中，它是被处理后的图像像素值与原始像素值之间的平方差的平均值。MSE表达式如Eq.\ref{eq: MSE}
			
			\begin{equation}
				\begin{aligned}
					MSE = \frac{1}{M \times N} \sum_{i=1}^{M} \sum_{N}^{j=1}{\left[ g(x,y) - \hat{g}(x,y) \right]}^2
				\end{aligned}
				\label{eq: MSE}
			\end{equation}
			
			其中，$M$是图像的高度，$N$是图像的宽度。$g(x,y)$和$\hat{g}(x,y)$分别表示原始图像和增强图像。MSE的值越小，图像质量越好。
			
			
			\subsubsection{信息熵 (Information Entropy, IE)}
			
			信息熵（IE）反映了图像所携带的信息量。信息熵越大，图像信息越丰富，质量越好。IE用于比较不同图像中信息内容的差异。例如，在同一区域拍摄的图像也会由于额外的拍摄时间而具有不同的信息内容。IE表达式Eq.\ref{eq: IE}
			
			\begin{equation}
				\begin{aligned}
					H = -\sum_{i=0}^{M} p(k)\log_{2}p(k)
				\end{aligned}
				\label{eq: IE}
			\end{equation}
			
			其中$p(k)$是灰度级$k$的概率密度，$M$是最大灰度级
			
			\subsubsection{标准偏差 (Standard Deviation, STD)}
			
			标准差（STD），也称为标准差，表示每个数据项与平均值的平均距离。STD也是方差的平方根。STD可以反映数据集的分散程度。与图像相比，STD反映了插图与平均值之间的分散程度，是特定范围内图像对比度的衡量标准。标准偏差越大，图像中包含的信息越多，视觉效果越好。STD的表达式Eq.\ref{eq: STD}
			
			\begin{equation}
				\begin{aligned}
					STD = \sqrt{\frac{\sum_{i=1}^{M} \sum_{j=1}^{N} f(i,j)\left( f(i,j)-\delta \right)^{2}}{M \times N}}
				\end{aligned}
				\label{eq: STD}
			\end{equation}
			
			其中$M$是图像的高度，$N$是图像的宽度，$f(i, j)$表示图像的$(i, j)$处的像素的灰度值。$\delta$表示图像的平均灰度值，其表达式为Eq.\ref{eq: delta}
			
			\begin{equation}
				\begin{aligned}
					\delta = \frac{1}{M \times N} \sum_{i=1}^{M} \sum_{j=1}^{N} f(i,j)
				\end{aligned}
				\label{eq: delta}
			\end{equation}
			
			\subsubsection{亮度顺序误差 (Lightness Order Error, LOE)}
			
			亮度顺序误差（LOE）是图像亮度的顺序差异，并且通过评估图像在邻域中的亮度的顺序变化过程来评估图像的照度变化。LOE反映了图像的自然保持能力。较小的值表示图像具有更好的亮度顺序并且看起来更自然。表达式为Eq.\ref{eq: LOE}
			
			\begin{equation}
				\begin{aligned}
					RD(i,j) = \sum_{x}^{M} \sum_{y}^{N} U\left( L(i,j), L(x,y) \right) \oplus U\left( L_{e}(i,j), L(x,y) \right) 
				\end{aligned}
				\label{eq: LOE}
			\end{equation}
			
			式中，$M$是图像的高度，$N$是图像的宽度，而$\oplus$是XOR算子，$L(i, j)$和$L_{e}(x, y)$分别表示三个颜色通道中的最大值。LOE的值越小，图像保持的亮度顺序越好。
			
		\subsection{主观评价指标}
		
		\subsubsection{差分平均意见分数 (Differential Mean Opinion Score, MOS)}
		
		差分平均意见得分（MOS）或差分平均看法得分（DMOS）。其中，MOS是应用最广泛的主观IQA方法。不同的人在原始图像和增强图像之间进行主观比较，以获得MOS分数，最后获得平均分数。MOS评分范围从1到5，评分越高表示主观增强越好。MOS评分公式为Eq.\ref{MOS}
		
		\begin{equation}
			\begin{aligned}
				MOS = \frac{\sum_{n=1}^{N} R_{n}} {N}
			\end{aligned}
			\label{eq: MOS}
		\end{equation}
		
		其中$R$表示评价者对图像的满意度得分的数量，$N$表示评价者的总数。
		
%	\subsection{Pre-knowledge}
	
	
%	模型中的损失函数是用来衡量模型的预测值和真实值(Ground Truth)之间的差异程度的函数，它可以反映模型的优化方向和性能指标\footnote{https://zhuanlan.zhihu.com/p/375968083}。它是一种衡量模型预测结果与真实结果之间差异的方法，用于指导模型参数的更新。在训练过程中，通过不断最小化损失函数来优化模型参数，使模型能够更好地拟合数据\footnote{https://zhuanlan.zhihu.com/p/436809988}。因此，需要使用合适的损失函数，当模型在数据集上进行训练时，该函数可以适当地惩罚模型 \footnote{https://zhuanlan.zhihu.com/p/473113939}。
%	
%	不同的损失函数适用于不同的任务和数据分布，例如回归问题常用的有均方误差损失函数(MSE，也叫做$\mathcal{L}_{2}$损失函数)和$\mathcal{L}_{1}$损失函数，分类问题常用的有交叉熵损失函数(Cross Entropy Loss)等\footnote{https://blog.csdn.net/weixin\_57643648/article/details/122704657}。损失函数的选择会影响模型的收敛速度和精度，因此需要根据具体情况选择合适的损失函数\footnote{https://www.zhihu.com/tardis/zm/art/136047113?source\_id=1005}。
%	
%	目前常用的损失函数是从相关视觉任务中借用的，但这些损失函数可能并不完全适用于低照度图像增强LLIE。因此，需要设计更适合 LLIE 的损失函数，以更好地驱动深度网络的优化。这可以通过研究人类对图像质量的视觉感知来实现，使用深度神经网络来近似人类视觉感知，并将这些理论应用于损失函数的设计。损失函数可以分为两个大类：回归问题和分类问题。
	
%	\subsubsection{loss function for CV}
%	
%	\paragraph{$\mathcal{L}_1$-loss}
%	
%	平均绝对误差(MAE)损失，也称$\mathcal{L}_1$范数损失，计算实际值和预测值之间绝对差之和的平均值。
%	
%	\begin{equation}
%		\begin{aligned}
%			\mathcal{L}_1 = \frac{1}{N} \sum_{i=1}^{N} {\| \hat{y}_i - y_i \|}_{1}
%		\end{aligned}
%	\end{equation}
%	
%	适用于回归问题，MAE loss对异常值更具鲁棒性，尤其是当目标变量的分布有离群值时(小值或大值与平均值相差很大)。
%	
%	函数：\textit{torch.nn.L1Loss}
%	
%	\paragraph{$\mathcal{L}_2$-loss}
%	
%	均方误差(MSE)损失，也称为$\mathcal{L}_2$范数损失，计算实际值和预测值之间平方差的平均值\footnote{https://blog.csdn.net/yanyuxiangtoday/article/details/119788949}。
%	
%	\begin{equation}
%		\begin{aligned}
%			\mathcal{L}_1 = \frac{1}{N} \sum_{i=1}^{N} {\| \hat{y}_i - y_i \|}_{2}^2
%		\end{aligned}
%	\end{equation}
%	
%	
%	平方意味着较大的误差比较小的误差会产生更大的惩罚，所以$\mathcal{L}_2-loss$的收敛速度要比$\mathcal{L}_1-loss$要快得多。但是，$\mathcal{L}_2-loss$对异常点更敏感，鲁棒性差于$\mathcal{L}_1-loss$。
%	
%	$\mathcal{L}_1-loss$损失函数相比于$\mathcal{L}_2-loss$损失函数的鲁棒性更好。因为以$\mathcal{L}_2-loss$范数将误差平方化(如果误差大于1，则误差会放大很多)，模型的误差会比以$\mathcal{L}_1-loss$范数大的多，因此模型会对这种类型的样本更加敏感，这就需要调整模型来最小化误差。但是很大可能这种类型的样本是一个异常值，模型就需要调整以适应这种异常值，那么就导致训练模型的方向偏离目标了\footnote{https://zhuanlan.zhihu.com/p/137073968}。
%	
%	对于大多数回归问题，一般是使用$\mathcal{L}_2-loss$而不是$\mathcal{L}_1-loss$，$\mathcal{L}_2-loss$在LLIE中，具有的相当很有效的恢复效果，但是由于平方项，它通常会产生模糊的恢复结果，如\cite{tatarchenko2016multi}所述，平方项为相对较小的误差提供了较低的梯度。。
%	
%	函数: \textit{torch.nn.MSELoss}
%	
%	\paragraph{Regularization of the $\mathcal{L}_1$ and $\mathcal{L}_2$ loss functions}
%	
%	正则化的基本思想是通过在损失函数中加入额外信息，以便防止过拟合和提高模型泛化性能。无论哪一种正则化方式，基本思想都是希望通过限制权重的大小，使得模型不能任意拟合训练数据中的随机噪声，正则化实际是在损失函数中加入刻画模型复杂程度的指标\footnote{https://blog.csdn.net/weixin\_41960890/article/details/104891561}。
%	
%	对应的L1正则损失函数:
%	
%	\begin{equation}
%		\begin{aligned}
%			\mathcal{L}_{norm1} = \mathcal{L}_{1}{\left(\hat{y},y\right)} + \lambda \sum_{\omega}{\| \omega \|}_1
%		\end{aligned}
%	\end{equation}
%	
%	对应的L2正则损失函数：
%	
%	\begin{equation}
%		\begin{aligned}
%			\mathcal{L}_{norm2} = \mathcal{L}_{2}{\left(\hat{y},y\right)} + \lambda \sum_{\omega}{\| \omega \|}^2_2
%		\end{aligned}
%	\end{equation}
%	
%	假设$\mathcal{L}_{1}{\left(\hat{y},y\right)}$和$\mathcal{L}_{2}{\left(\hat{y},y\right)}$是未加正则项的损失，$\lambda$是一个超参，用于控制正则化项的大小，惩罚项$\omega$用于惩罚大的权重，隐式地减少自由参数的数量。
%	
%	正则化是如何降低过拟合现象的？
%	
%	正则化之所以能够降低过拟合的原因在于，正则化是结构风险最小化的一种策略实现。给损失函数加上正则化项，能使得新得到的优化目标函数$h = f + normal$ ，需要在$f$和$normal$中做一个权衡(trade-off)，如果还像原来只优化$f$的情况下，那可能得到一组解比较复杂，使得正则项$normal$比较大，那么$h$就不是最优的，因此可以看出加正则项能让解更加简单，符合奥卡姆剃刀理论，同时也比较符合在偏差和方差(方差表示模型的复杂度)分析中，通过降低模型复杂度，得到更小的泛化误差，降低过拟合程度\footnote{https://zhuanlan.zhihu.com/p/35356992}。
%	
%	PyTorch实现： $\mathcal{L}_2$正则项是通过optimizer优化器的参数 weight\_decay(float, optional) 添加的，用于设置权值衰减率，即正则化中的超参$\lambda$，默认值为0。
%	
%	\lstset{language=python,breaklines=true}
%	\begin{lstlisting}
%		optimizer = torch.optim.SGD(model.parameters(),lr=0.01,weight_decay=0.01)
%	\end{lstlisting}
%	
%	\paragraph{Smooth $\mathcal{L}_1$ loss function}
%	
%	Smooth $\mathcal{L}_1$损失函数是由Girshick R在Fast R-CNN中提出的，主要用在目标检测中防止梯度爆炸。它是一个分段函数，在$\left[-1,1\right]$之间是$\mathcal{L}_2$损失，其他区间就是$\mathcal{L}_1$损失。这样即解决了$\mathcal{L}_1$损失在0处不可导的问题，也解决了$\mathcal{L}_2$损失在异常点处梯度爆炸的问题\footnote{https://zhuanlan.zhihu.com/p/261059231}。
%	
%	\begin{equation}
%		\text{smooth} \ \mathcal{L}_1 \ \text{loss} = \frac{1}{N}\sum_{i=1}^{N}
%		\left\{
%		\begin{aligned}
%			&\frac{{\|\hat{y}_i - y_i \|}_2^{2}}{2\beta}, \| \hat{y}_i -y_i \| < \beta , \\
%			&{\|\hat{y}_i - y_i \|}_1 - \frac{1}{2}\beta, \| \hat{y}_i -y_i \| \geq \beta.
%		\end{aligned}
%		\right.
%	\end{equation}		
%	
%	一般取$\beta=1$。smooth $\mathcal{L}_1$和$\mathcal{L}_1$-loss函数的区别在于， smooth $\mathcal{L}_1$在0点附近使用$\mathcal{L}_2$使得它更加平滑, 它同时拥有$\mathcal{L}_2$-loss和$\mathcal{L}_1$-loss的部分优点。
%	
%	函数：\textit{torch.nn.SmoothL1Loss}
%	
%	\paragraph{Huber loss function}
%	
%	$\mathcal{L}_2$-loss但容易受离群点的影响，$\mathcal{L}_1$-loss对离群点更加健壮但是收敛慢，Huber Loss 则是一种将MSE与MAE结合起来，取两者优点的损失函数，也被称作Smooth Mean Absolute Error Loss。其原理很简单，就是在误差接近0时使用$\mathcal{L}_2$-loss，误差较大时使用$\mathcal{L}_1$-loss
%	
%	\begin{equation}
%		J_{Huber}(\delta)= \frac{1}{N}\sum_{i=1}^{N}
%		\left\{
%		\begin{aligned}
%			&\frac{1}{2}{\left\|\hat{y}_i - y_i \right\|}_2^{2}, \left\| \hat{y}_i -y_i \right\| < \delta , \\
%			&\delta\left({\left\|\hat{y}_i - y_i \right\|}_1 - \frac{1}{2}\delta \right), \left\| \hat{y}_i -y_i \right\| \geq \delta.
%		\end{aligned}
%		\right.
%	\end{equation}
%	
%	残差比较小时，Huber Loss是二次函数；残差比较大时，Huber Loss是线性函数(残差，即观测值和预测值之间的差值)。与$\mathcal{L}_2$-loss相比，Huber损失对数据中的异常值不那么敏感。使函数二次化的小误差值是多少取决于“超参数”$\delta$，它可以调整。当$\delta=1$时，退化成smooth $\mathcal{L}_1$ Loss。
%	
%	函数：\textit{torch.nn.HuberLoss}
%	
%	\paragraph{log-MSE}
%	
%	\begin{equation}
%		\begin{aligned}
%			J_{log-MSE} = 10\log_{10}\left(\frac{1}{N} \sum_{i=1}^{N} {\| \hat{y}_i - y_i \|}_{2}^2 \right)
%		\end{aligned}
%	\end{equation}
%	
%	\paragraph{Perceptual loss function}
%	
%	感知损失(Perceptual Loss)是一种用于比较两个看起来相似的图像的损失函数，这一损失函数由Johnson et al.\cite{johnson2016perceptual}提出。它用于比较图像之间的高层次差异，如内容和风格差异\footnote{https://deepai.org/machine-learning-glossary-and-terms/perceptual-loss-function}。它已被广泛用作图像合成任务(包括图像超分辨率和风格转换)中的有效损失项。
%	
%	感知损失函数用于比较两个看起来相似的不同图像，例如同一张照片，但偏移了一个像素。该函数用于比较图像之间的高级差异，例如内容和样式差异。感知损失函数与每像素损失函数非常相似，因为两者都用于训练前馈神经网络以进行图像转换任务。感知损失函数是一个更常用的组件，因为它通常提供有关风格迁移的更准确的结果。
%	
%	简而言之，感知损失函数的工作原理是将所有像素之间的所有平方误差相加并取平均值。这与每像素损失函数形成对比，后者对像素之间的所有绝对误差求和\cite{johnson2016perceptual}。
%	
%	作者认为感知损失函数不仅在生成高质量图像方面更准确，而且在优化后也快了三倍。神经网络模型在图像上进行训练，其中感知损失函数基于从已训练网络中提取的高级特征进行优化。
%	
%	\begin{equation}
%		\begin{aligned}
%			\ell_{feat}^{\phi,j} (\hat{y},y) = \frac{1}{C_{j}H_{j}W_{j}}{\left\| \phi_{j}(\hat{y})-\phi_{j}(y)\right\|}_{2}^2
%		\end{aligned}
%		\label{eq: perceptual loss}
%	\end{equation}
%	
%	其中$\hat{y}$为输出图像，$y$为目标图像，$\phi$为损失网络。$\phi_{j}(x)$为处理图像$x$时损失网络$\phi$的第$j$层的激活情况，如果$j$是一个卷积层，那么$\phi_{j}(x)$将是形状$C_{j} \times H_{j} \times W_{j}$的特征映射，特征重建损失是特征表示之间的欧式距离，如eq \ref{eq: perceptual loss}。
%	
%	\begin{figure}[ht] 
%		% read manual to see what [ht] means and for other possible options
%		\centering \includegraphics[width=0.8\columnwidth]{perceptual}
%		\captionsetup{font=scriptsize}
%		\caption{
%			\label{fig: perceptual loss} % spaces are big no-no withing labels
%			% things like fig: are optional in the label but it helps
%			% to orient yourself when you have multiple figures,
%			% equations and tables
%			System overview. We train an image transformation network to transform input images into output images. We use a loss network pretrained for image classification to define perceptual loss functions that measure perceptual differences in content and style between images. The loss network remains fixed during the training process.
%		}
%	\end{figure}
%	
%	Fig.\ref{fig: perceptual loss}表示经过训练以将输入图像转换为输出图像的神经网络。用于图像分类的预训练损失网络有助于通知损失函数。预先训练的网络有助于定义测量图像之间内容和风格的感知差异所需的感知损失函数。
%	
%	对于图像数据来说，网络在提取特征的过程中，较浅层通常提取边缘、颜色、亮度等低频信息，而网络较深层则提取一些细节纹理等高频信息，再深一点的网络层则提取一些具有辨别性的关键特征，也就是说，网络层越深提取的特征越抽象越高级。
%	
%	感知损失就是通过一个固定的网络(通常使用预训练的VGG16或者VGG19)，分别以真实图像(Ground Truth)、网络生成结果(Prediciton)作为其输入，得到对应的输出特征：feature\_gt、feature\_pre，然后使用feature\_gt与feature\_pre构造损失(通常为$\mathcal{L}_2$-loss)，逼近真实图像与网络生成结果之间的深层信息，也就是感知信息，相比普通的$\mathcal{L}_2$-loss而言，可以增强输出特征的细节信息\footnote{https://blog.csdn.net/qq\_43665602/article/details/127077484}。
%	
%	\paragraph{SSIM loss function}
%	
%	SSIM损失函数是一种用于衡量两幅图像之间差距的损失函数。它考虑了亮度、对比度和结构指标，这就考虑了人类视觉感知，一般而言，SSIM得到的结果会比$\mathcal{L}_1$-loss，$\mathcal{L}_2$-loss的结果更有细节\footnote{https://blog.csdn.net/u013289254/article/details/99694412}。
%	
%	每个像素$p$的\text{SSIM}被定义为
%	\begin{equation}
%		\begin{aligned}
%			\text{SSIM}(p) &= \frac{2\mu_{x}\mu_{y}+C_{1}}{\mu_{x}^2+\mu_{y}^2+C_{1}} \cdot \frac{2\sigma_{xy}+C_{2}}{\sigma_{x}^2+\sigma_{y}^{2}+C_{2}} \\
%			&= l(p)\cdot cs(p)
%		\end{aligned}
%		\label{eq: SSIM}
%	\end{equation}
%	其中省略了均值和标准偏差对像素$p$的依赖性，均值和标准差是用标准偏差为$\sigma_G,G_{\sigma_G}$
%	\begin{equation}
%		\begin{aligned}
%			&\varepsilon(p)=1-\text{SSIM}(p): \\  &\mathcal{L}^{\text{SSIM}}(P)=\frac{1}{N}\sum_{p \in P}1-\text{SSIM}(p).
%		\end{aligned}
%		\label{eq: SSIM loss}
%	\end{equation}
%	
%	eq. \ref{eq: SSIM}表明$\text{SSIM}(p)$需要关注像素$p$的邻域，这个领域的大小取决于$G_{\sigma_G}$，网络的卷积性质允许我们将SSIM损失写为
%	
%	\begin{equation}
%		\begin{aligned}
%			\mathcal{L}^{\text{SSIM}}(P)=1-\text{SSIM}(\tilde{p}).
%		\end{aligned}
%		\label{eq: revised_SSIM loss}
%	\end{equation}
%	
%	其中$\tilde{p}$是$P$的中心像素。
%	
%	\paragraph{MS-SSIM loss function}
%	
%	多尺度结构相似性(MS-SSIM)损失函数是基于多层(图片按照一定规则，由大到小缩放)的SSIM损失函数，相当于考虑了分辨率\footnote{https://blog.csdn.net/u013289254/article/details/99694412}。它是一种更为复杂的SSIM损失函数，可以更好地衡量图像之间的相似性。
%	
%	\begin{equation}
%		\begin{aligned}
%			\text{MS-SSIM}(p)=l_{M}^\alpha(p)\cdot \prod_{j=1}^M cs_{j}^{\beta_j}(p)
%		\end{aligned}
%		\label{eq: MS-SSIM}
%	\end{equation}
%	
%	其中$M,j$描述的是比例，设$\alpha=\beta_j=1$，对于$j={1,\cdots, M}$类似eq. \ref{eq: revised_SSIM loss}，利用中心像素$\tilde{p}$处计算的损失来近似贴片$P$的损失：
%	
%	\begin{equation}
%		\begin{aligned}
%			\mathcal{L}^{\text{MS-SSIM}}(P)=1-\text{MS-SSIM}(\tilde{p})
%		\end{aligned}
%		\label{eq: MS-SSIM loss}
%	\end{equation}
%	
%	\paragraph{Cross-entropy loss function}
%	
%	交叉熵损失函数是一种常用的分类问题损失函数。在二分类问题中，它的定义为Eq. \ref{eq: Cross-entropy loss}
%	\begin{equation}
%		\begin{aligned}
%			\mathcal{L}(\hat{y},y)=-\left( y\log_{\hat{y}} + (1-y) \log (1-\hat{y}) \right)
%		\end{aligned}
%		\label{eq: Cross-entropy loss}
%	\end{equation}
%	
%	其中，$\hat{y}$表示模型预测的概率值(即分类器输出)，$y$表示样本真实的类别标签。对于正例样本($y=1$)，交叉熵损失函数的值等于$log {\hat{y}}$；对于反例样本($y=0$)，交叉熵损失函数的值等于$\log (1-\hat{y})$。 因此，交叉熵损失函数的目标是最小化模型预测与实际标签之间的差距，从而让模型能够更准确地进行分类。 
%	
%	交叉熵损失函数可以推广到多分类问题中，此时它的表达式略有不同。在多分类问题中，交叉熵损失函数可以写成以下形式： 
%	\begin{equation}
%		\begin{aligned}
%			\mathcal{L}(\hat{y},y)=-\sum_{i=1}^K y_i \log \hat{y}_i
%		\end{aligned}
%		\label{eq: revised_Cross-entropy loss}
%	\end{equation} 
%	
%	其中，$K$表示类别的数量，$y_i$表示第$i$个类别的真实标签，$\hat{y}_i$表示模型对于第$i$个类别的预测概率值。交叉熵损失函数的目标仍然是最小化预测与实际标签之间的差距，从而让模型能够更准确地进行分类。
%	
%	\paragraph{Adversarial loss function}
%	
%	Adversarial loss function是生成对抗网络(GAN)的标准损失函数。GAN是由生成器(Generator)和判别器(Discriminator)组成的两个神经网络模型。生成器的目标是生成与真实数据相似的假数据，而判别器的目标是将真实数据与生成的假数据区分开来。Adversarial loss使得GAN能够不断的改进生成器的性能，使其能够生成更加逼真的数据。
%	
%	\begin{figure}[htbp] 
%		% read manual to see what [ht] means and for other possible options
%		\centering 
%		\includegraphics[width=0.8\columnwidth]{GAN_architecture}
%		\captionsetup{font=scriptsize}
%		\caption{
%			\label{fig: GAN_architecture} % spaces are big no-no withing labels
%			% things like fig: are optional in the label but it helps
%			% to orient yourself when you have multiple figures,
%			% equations and tables
%			Computational flow and structure of the GAN.
%		}
%	\end{figure}
%	
%	如Fig. \ref{fig: GAN_architecture}所示，在训练过程中，生成器和判别器相互竞争和对抗。生成器试图生成逼真的假数据$G_\theta{\left( z \right)}$以欺骗判别器，而判别器则试图准确地判断真实数据和生成的假数据。Adversarial loss通过衡量生成器生成的假数据被判别器识别为真实数据的程度，来指导生成器的训练。它的目标是最小化生成器生成的假数据与真实数据之间的差异，使得判别器难以区分它们。
%	
%	一般而言，GAN的目标函数为$V(D, G)$Eq. \ref{eq: Adversarial loss}
%	\begin{equation}
%		\begin{aligned}
%			\min_G \max_D V\left( D, G \right) = \mathbb{E}_{x \sim p_{data}(x)}\left[ \log D(x) \right] + \mathbb{E}_{z \sim p_{noise}(z)}\left[ \log \left(1- D(G(z))\right) \right]
%		\end{aligned}
%		\label{eq: Adversarial loss_another}
%	\end{equation}
%	
%	其中，$E(*)$表示分布函数的期望值，$P_{data}(x)$代表着真实样本的分布，$P_{noise}(z)$是定义在低维的噪声分布，通过参数为$\theta_{g}$的$G$映射到高维的数据空间得到$P_g=G(z,\theta_{g})$,这些参数通过交替迭代的方法来进行优化，见Fig. \ref{fig: Alternate Optimization}。Fig. \ref{fig: D_optimization}固定$G$参数不变，优化$D$的参数，即最大化$\max V\left( D, G \right)$ 等价于$\min \left[ - V(D,G) \right]$。因此，$D$的损失函数等价于Eq. \ref{eq: Adversarial loss_another}
%	\begin{equation}
%		\begin{aligned}
%			J^{D} \left( \theta^{D}, \theta^{G} \right) = -\mathbb{E}_{x \sim p_{data}(x)}\left[ \log D(x) \right] - \mathbb{E}_{\tilde{x} \sim p_{g}(x)}\left[ \log \left(1- D(\tilde{x})\right) \right]
%		\end{aligned}
%		\label{eq: Adversarial loss}
%	\end{equation}
%	
%	\begin{figure}[htbp] 
%		% read manual to see what [ht] means and for other possible options
%		\centering 
%		% \includegraphics[width=0.8\columnwidth]{GLADNet}
%		\begin{subfigure}{0.4\textwidth}
%			\includegraphics[width=\linewidth]{D_optimization}
%			\captionsetup{font=scriptsize}
%			\caption{D的优化过程}
%			\label{fig: D_optimization}
%		\end{subfigure}
%		\begin{subfigure}{0.4\textwidth}
%			\includegraphics[width=\linewidth]{G_optimization}
%			\captionsetup{font=scriptsize}
%			\caption{G的优化过程}
%			\label{fig: G_optimization}	
%		\end{subfigure}
%		\captionsetup{font=scriptsize}
%		\caption{
%			\label{fig: Alternate Optimization} % spaces are big no-no withing labels
%			% things like fig: are optional in the label but it helps
%			% to orient yourself when you have multiple figures,
%			% equations and tables
%			The optimization of the GAN parameters 
%		}
%	\end{figure}
%	
%	
%	Adversarial loss的具体形式可以根据具体的GAN架构和任务而定，常见的形式包括最小二乘损失(Least Squares Loss)、二进制交叉熵损失(Binary Cross-Entropy Loss)等。这些损失函数的选择取决于具体的生成器和判别器结构以及任务的特点\footnote{https://blog.csdn.net/qikaihuting/article/details/84950947}。
%	
%	\paragraph{Region loss function}
%	
%	Region loss function是一种用于生物医学图像分割的损失函数。它是一种多功能的损失函数，可以同时考虑类别不平衡和像素重要性，并且可以很容易地实现为softmax输出和RW(Region-wise)地图之间的像素级乘法\footnote{https://arxiv.org/abs/2108.01405}。一般在一些涉及到异常检测的图像处理中，可以通过设计region loss function来使网络能够学习异常区域的位置，但是一般在这种情况下，需要设计一种可训练的异常区域引导框架。
%	
%	在暗光增强领域，由于难以同时处理包括亮度、对比度、伪影和噪声在内的各种因素，该问题具有挑战性，在损失函数部分，结合使用Region loss function能获得不错的效果。一种基于深度学习的微光图像增强方法(MBLLEN)的损失函数的详细信息如Fig. \ref{fig: proposed_loss_function}所示，Structural loss损失旨在改善输出图像的视觉质量，Context loss则通过关注图像中更加高层的信息来提高图像的视觉质量。
%	
%	上述损失函数将图像作为一个整体。然而，对于弱光增强任务，我们需要更多地关注那些弱光区域。因此，作者提出了Region loss损失，它平衡了弱光和图像中其他区域的增强程度。为了做到这一点，作者首先提出了一个简单的策略来分离图像中的弱光区域。通过初步的实验，作者发现在所有像素中选择最暗的40\%的像素可以很好地近似于弱光区域。人们也可以提出更复杂的方法来选择暗区，事实上在文献中有很多。最后，区域损失定义如Eq. \ref{eq: Region loss}
%	\begin{equation}
%		\begin{aligned}
%			L_{Region} = w_{L} \cdot \frac{1}{m_{L}n_{L}}\sum_{i=1}^{n_{L}}\sum_{j=1}^{m_{L}} \left( \| E_{L}(i,j) - G_{L}(i,j) \| \right) + w_{H} \cdot \frac{1}{m_{H}n_{H}}\sum_{i=1}^{n_{H}}\sum_{j=1}^{m_{H}}\left( \| E_{H}(i,j) - G_{H}(i,j) \| \right)
%		\end{aligned}
%		\label{eq: Region loss}
%	\end{equation}
%	
%	其中，$E_L$和$G_L$是增强图像和地面实况的暗光区域，$E_H$和$G_H$是图像的剩余部分。	
%	\begin{figure}[htbp] 
%		% read manual to see what [ht] means and for other possible options
%		\centering 
%		\includegraphics[width=0.8\columnwidth]{proposed_loss_function}
%		\captionsetup{font=scriptsize}
%		\caption{
%			\label{fig: proposed_loss_function} % spaces are big no-no withing labels
%			% things like fig: are optional in the label but it helps
%			% to orient yourself when you have multiple figures,
%			% equations and tables
%			Data flow for training. The proposed loss function consists of three parts.
%		}
%	\end{figure}
%	
%	\paragraph{Reflectance loss function}
%	
%	Reflectance loss function是计算机视觉领域中的一种损失函数，常用于图像去雾(image dehazing)任务中。(图像去雾是指通过算法去除图像中由雾霾引起的能见度降低的效果。Reflectance loss function用于衡量生成的去雾图像与真实清晰图像之间的差异，以指导去雾模型的训练过程。)
%	
%	Reflectance loss function的核心思想是基于图像的反射率(reflectance)属性。反射率是指物体表面对入射光线的反射程度，与雾霾相关的信息主要存在于反射率中。通过计算生成的去雾图像的反射率与真实清晰图像的反射率之间的差异，可以量化生成图像的质量。
%	
%	具体来说，Reflectance loss function通常使用像素级别的差异度量函数，比如均方误差(Mean Squared Error，MSE)或结构相似性(Structural Similarity，SSIM)等来计算生成图像与真实图像之间的差异。优化过程的目标是最小化Reflectance loss，使得生成的去雾图像能够与真实清晰图像更加接近。
%	
%	通过引入Reflectance loss作为训练目标，去雾模型可以学习到更准确的雾霾分布和反射率信息，从而生成更清晰的去雾图像。这有助于提高图像去雾算法的性能和质量。
%	
%	\paragraph{Consistency loss function}
%	
%	Consistency loss function是一种在机器学习中使用的损失函数，用于提高模型的一致性。它主要用于半监督学习或自监督学习任务中，其中存在一些带有标签的数据（有目标变量的输入-输出对）和一些没有标签的数据（只有输入）。区别于全监督学习，半监督学习针对训练集标记不完整的情况：仅仅部分数据具有标签，然而大量数据是没有标签的。因此，目前半监督学习的关键问题在于如何充分地挖掘没有标签数据的价值。主流的半监督学习方法有下面几种\footnote{https://blog.csdn.net/JYZhang\_CVML/article/details/106817709}:
%	
%	(1) \textbf{自训练方法(Self-Training)}。这是一种很直观的思路：既然大量数据是没有标签的，那么能否对这些数据生成一些伪标签(Pseuo Labels)，对这些伪标签数据的训练从而利用原始的无标签数据。
%	
%	(2) \textbf{基于对抗学习的方法(Adversarial-Learning-based)}。这类方法基于一种假设，无标签数据(unlabeled data)通常具有和有标签数据(labeled data)在某种程度上类似的潜在标签。所以很自然地，可以采用GAN的图像模拟思路来进行对有标签数据进行类似于无标签数据的数据增强，进而利用无标签数据的潜在知识(latent knowledge)。
%	
%	(3) \textbf{基于一致性的方法(Consistency-based)}。 这类方法的核心思路在于一致性损失函数Consistency loss function，对于进过扰动的无标签数据，模型应该对其做出一致性的预测——可以理解成一种利用无标签数据进行网络正则化的方法。
%	
%	一致性损失函数的基本思想是，在模型对相似输入的处理应该具有一致性。它通过对输入进行一些变换或扰动，然后要求模型在这些变换后仍然能够产生相似的输出。如果模型能够在不同的变换下产生一致的输出，那么我们可以认为模型具有一定的鲁棒性和泛化能力。
%	
%	具体来说，一致性损失函数可以定义为模型在原始输入和变换后输入上的输出之间的差异。常见的一致性损失函数包括均方差损失、平均绝对误差损失、KL散度等。通过最小化一致性损失函数，模型被迫保持对输入的一致性响应，从而提高模型的泛化能力。
%	
%	需要注意的是，一致性损失函数的具体形式和应用取决于具体的任务和模型架构。它通常与其他损失函数（如分类损失函数或重构损失函数）结合使用，以共同训练模型。
%	
%	\paragraph{Color loss function}
%	
%	\begin{equation}
%		\begin{aligned}
%			\mathcal{L}_c^{i} = \sum_{p} \angle \left( (\mathcal{F}(I_i))_{p}, (\tilde{I}_i)_p \right)
%		\end{aligned}
%		\label{eq: color loss}
%	\end{equation}
%	
%	Lim\cite{9264763}基于余弦相似性基于\cite{8953588}中提出的损失函数color-loss进行了改进，针对于颜色恢复设计的损失函数，将RGB三通道看做三维向量，计算增强图像与GT之间的余弦距离(见Eq. \ref{eq: color loss}),因为在reconstruction loss中使用了$\mathcal{L}_2$-loss,但是$\mathcal{L}_2$-loss只是数值上测量色差，不能保证颜色向量的方向是相同的，可能导致颜色mismatch,所以使用了夹角的loss,Lim\cite{9264763}修改之后,见Eq. \ref{eq: Improved color loss}。
%	\begin{equation}
%		\begin{aligned}
%			\mathcal{L}_c = \sum_{k=1}^{3} \left( 1- \frac{1}{H_k \times W_k} \sum_{i \in (H_k, W_k)} \frac{\tilde{I}_k(i) \cdot I_k^{*}(i)}{{\| \tilde{I}_k(i) \|^2} {\| I_k^{*}(i) \|^2}} \right)
%		\end{aligned}
%		\label{eq: Improved color loss}
%	\end{equation}
%	
%	其中$H_k$和$W_k$分别是第k个拉普拉斯金字塔级别(Laplacian pyramid level)中的增强结果的高度和宽度。$\tilde{I}_k(i)$表示像素位置，$\cdot$表示内积。颜色越接近，颜色损失$L_c$就越接近于零。通过在恢复过程中考虑颜色向量的方向，能够成功地将增强结果的颜色属性恢复为与地面实况相似。
%	
%	文章描述该方法是通过模糊输入图像与GT的纹理、内容，仅仅保存图像的颜色信息实现图像颜色的校正。实现过程比较简单，首先构建一个高斯模糊核，然后利用高斯模糊核作为卷积核对图像进行卷积运算，得到模糊后的图像；然后计算输入图像与GT的MSE作为损失函数。(pytorch实现\footnote{https://blog.csdn.net/ztzi321/article/details/101424448})
%
%	
%	\paragraph{Laplacian loss function}
%	
%	拉普拉斯损失(Laplacian loss)是一种在计算机视觉任务中常用的损失函数，用于促进图像的平滑性。它通常用于图像生成、图像修复或图像去噪等任务中。其形式上一般定义为Eq. \ref{eq: Laplacian loss}。从形式上看，Laplacian loss function主要工作是在$\mathcal{L}_1$-loss的基础上乘了一个$2$的幂次项，并且将多层的结果相加\footnote{https://blog.csdn.net/lanceloter/article/details/121925904}。
%	
%	Laplacian loss的基本思想是鼓励生成的图像具有平滑的变化，以减少图像中的噪点或锐利边缘。它利用图像的拉普拉斯金字塔来比较生成图像和真实图像之间的结构差异。拉普拉斯金字塔\cite{burt1987laplacian}最初是为紧凑的图像表示而设计的，具有简单的操作，即缩小和扩展。许多研究人员试图将拉普拉斯金字塔应用于广泛的计算机视觉任务，例如纹理分析和合成。拉普拉斯金字塔具有线性可逆过程来重建原始输入而不损失任何信息以精确地细化局部细节的性质。Ghiasi等人\cite{ghiasi2016laplacian}提出了基于拉普拉斯金字塔的细化过程，以增强语义分割的低分辨率估计结果的局部细节。
%	\begin{equation}
%		\begin{aligned}
%			\mathcal{L}_l = \sum 2^{i-1} {\| \mathcal{L}^i(\hat{\alpha}) - \mathcal{L}^i(\alpha) \|}_1 
%		\end{aligned}
%		\label{eq: Laplacian loss}
%	\end{equation}
%	
%	具体来说，Laplacian loss首先通过将生成图像和真实图像分别构建成拉普拉斯金字塔。拉普拉斯金字塔是一种多尺度表示，其中每个级别表示了原始图像与其上一级别平滑版本之间的细节差异。然后，Laplacian loss计算两个图像金字塔之间的差异，通过比较它们在每个级别上的像素差异来量化结构上的差异。
%	
%	通过最小化Laplacian loss，生成的图像被鼓励具有更平滑的结构，从而减少图像中的噪点和锐利边缘。
%	
%	需要注意的是，具体的Laplacian loss的实现方式可能因任务而异。在某些情况下，可以使用像素级别的差异或特征级别的差异作为损失度量。此外，Laplacian loss通常与其他损失函数（如像素级别的差异、感知损失或对抗损失）结合使用，以共同训练生成模型。
	
%	\subsubsection{细节}
%	
%
%	
%	% 在LaTeX中，overfull hbox和overfull vbox的警告是由于badness值超过了tolerance值而产生的1。对于每种字体有一个标准的间距，为了使当前行适应宽度，会改变这些间距，如果badness的值超过了tolerance，就会出现overfull或者underfull的警告
%	
%	\begin{table}[!htbp]
%		\centering
%		\tiny
%		\resizebox{\columnwidth}{!}{ %按照宽度调整调整表格大小
%			\begin{tabular}{m{0.01cm}|>{\centering\arraybackslash}m{1.45cm}|>{\centering\arraybackslash}m{1.0cm}|>{\centering\arraybackslash}m{2.6cm}|>{\centering\arraybackslash}m{3.1cm}|>{\centering\arraybackslash}m{3cm}|>{\centering\arraybackslash}m{2.4cm}|>{\centering\arraybackslash}m{2.4cm}|>{\centering\arraybackslash}m{0.9cm}|>{\centering\arraybackslash}m{1.4cm}|>{\centering\arraybackslash}m{1cm}|}
%				
%				\hline
%				
%				& \textbf{Method} & \textbf{Learning} & \textbf{Network Structure} & \textbf{Loss Function} & \textbf{Training Data} & \textbf{Testing Data} & \makecell{\textbf{Evaluation Metric}} & \textbf{Format} & \textbf{Platform} & \textbf{Retinex} \\
%				
%				\hline
%				
%				\multirowcell{1}{\makecell{\centering \rotatebox{90}{\textbf{2017}}}} & LLNet & SL & SSDA &  SRR loss & simulated by Gamma Correction \& Gaussian Noise & simulated self-selected & PSNR SSIM & RGB & Theano &  \\
%				
%				\hline
%				
%				\multirowcell{6}{\makecell{\centering \rotatebox{90}{\textbf{2018}}}} & LightenNet & SL &  four layers & $L_2$ loss & simulated by random illumination values & simulated self-selected & \makecell{PSNR MAE \\ SSIM User Study} & RGB & Caffe MATLAB & \checkmark \\
%				
%				& Retinex-Net  & SL & multi-scale network & \makecell{$L_1$ loss \\ invariable reflectance loss \\ smoothness loss} & LOL simulated by adjusting histogram & self-selected & - & RGB & TensorFlow & \ \checkmark \\
%				
%				& MBLLEN & SL & multi-branch fusion & \makecell{SSIM loss \\ perceptual loss \\ region loss} & simulated by Gamma Correction \& Poisson Noise & simulated self-selected & \makecell{PSNR SSIM \\ AB VIF \\ LOE TOMI} & RGB & TensorFlow &  \\
%				
%				& SCIE & SL & frequency decomposition & \makecell{$L_2$ loss \\ $L_1$ loss \\ SSIM loss} & SCIE & SCIE & PSNR FSIM \qquad Runtime FLOPs & RGB & Caffe MATLAB &  \\
%				
%				& Chen et al. & SL & U-Net & $L_1$ loss & SID & SID & PSNR SSIM & Raw & TensorFlow &  \\
%				
%				& Deepexposure & RL & policy network GAN & deterministic policy gradient adversarial loss & MIT-Adobe FiveK & MIT-Adobe FiveK & PSNR SSIM & Raw & TensorFlow &  \\
%				
%				\hline
%				
%				\multirowcell{8}{\makecell{\centering \rotatebox{90}{\textbf{2019}\qquad\qquad \qquad\qquad\qquad}}} & Chen et al. & SL & siamese network & \makecell{$L_1$ loss \\ self-consistency loss} & DRV & DRV & \makecell{PSNR SSIM \\ MAE} & Raw & TensorFlow &  \\
%				
%				
%				& Jiang and Zheng & SL & 3D U-Net & \makecell{$L_1$ loss} & SMOID & SMOID & \makecell{PSNR SSIM \\ MSE} & Raw & TensorFlow &  \\
%				
%				& DeepUPE & SL & illumination map & \makecell{$L_1$ loss \\ smoothness loss \\ color loss} & retouched image pairs & MIT-Adobe FiveK & \makecell{PSNR SSIM \\ User Study} & RGB & TensorFlow & \checkmark \\
%				
%				& KinD & SL & three subnetworks U-Net & \makecell{reflectance similarity loss \\ illumination smoothness loss \\ mutual consistency loss \\ $L_1$ loss \\ $L_2$ loss \\ SSIM loss \\ texture similarity loss \\ illumination adjustment loss} & LOL & LOL LIME NPE MEF & \makecell{PSNR SSIM \\ LOE NIQE} & RGB & TensorFlow & \checkmark \\
%				
%				& Wang et al. & SL & two subnetworks pointwise Conv & $L_1$ loss & simulated by camera imaging model & IP100 FNF38 MPI LOL NPE & \makecell{PSNR SSIM \\ NIQE} & RGB & Caffe & \checkmark \\
%				
%				& Ren et al. & SL & U-Net like network RNN dilated Conv & \makecell{$L_2$ loss \\ perceptual loss \\ adversarial loss} & MIT-Adobe FiveK with Gamma correction \& Gaussion noise & simulated self-selected DPED & PSNR SSIM Runtime & RGB & Caffe &  \\
%				
%				& EnlightenGAN & UL & U-Net like network & \makecell{adversarial loss \\ self feature preserving loss} & unpaired real images & NPE LIME MEF DICM VV BBD-100K ExDARK & User Study NIQE Classification & RGB & PyTorch &  \\
%				
%				& ExCNet. & ZSL & fully connected layers & energy minimization loss & real images & $IE_{ps}D$ & \makecell{User Study \\ CDIQA LOD} & RGB & PyTorch &  \\
%				
%				\hline
%				
%				\multirowcell{12}{\makecell{\centering \rotatebox{90}{\textbf{2020}\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad}}} & Zero-DCE & ZSL & U-Net like network & \makecell{spatial consistency loss \\ exposure control loss \\ color constancy loss \\ illumination smoothness loss} & SICE & SICE NPE LIME MEF DICM VV DARK FACE & \makecell{User Study PI \\ PNSR SSIM \\ MAE Runtime \\ Face detection} & RGB & PyTorch & \\
%				
%				& DRBN & SSL & recursive network & \makecell{SSIM loss \\ perceptual loss \\ adversarial loss} & LOL images selected by MOS & LOL & \makecell{PSNR SSIM \\ SSIM-GC} & RGB & PyTorch & \\
%				
%				& Lv et al. & SL & U-Net like network & \makecell{Huber loss \\ SSIM loss \\ perceptual loss \\ illumination smoothness loss} & simulated by a retouching module & LOL SICE DeepUPE & \makecell{User Study PSNR \\ SSIM VIF \\ LOE NIQE \\ \#P Runtime \\ Face detection} & RGB & TensorFlow & \checkmark \\
%				
%				& Fan et al. & SL & four subnetworks U-Net like network feature modulation & \makecell{mutual smoothness loss \\ reconstruction loss \\ illumination smoothness loss \\ cross entropy loss \\ consistency loss \\ SSIM loss \\ gradient loss \\ ratio learning loss} & \makecell{simulated by \\ illumination adjustment,\\ slight color distortion,\\ and noise simulation} & simulated self-selected & \makecell{PSNR SSIM \\ NIQE} & RGB & - & \checkmark \\
%				
%				& Xu et al. & SL & \makecell{frequency decomposition \\ U-Net like network} & \makecell{$L_2$ loss \\ perceptual loss} & SID in RGB & \makecell{SID in RGB \\ self-selected} & PSNR SSIM & RGB & PyTorch & \\
%				
%				& EEMEFN & SL & \makecell{U-Net like network \\ edge detection network} & \makecell{$L_1$ loss \\ weighted cross-entropy loss} & SID & SID & PSNR SSIM & Raw & \makecell{TensorFlow \\ PaddlePaddle} & \\
%				
%				& DLN & SL & residual learning interactive factor back projection network & \makecell{SSIM loss \\ total variation loss} & simulated by illumination adjustment, slight color distortion, and noise simulation & simulated LOL & \makecell{User Study PSNR \\ SSIM NIQE} & RGB & PyTorch & \\
%				
%				& LPNet & SL & pyramid network & \makecell{$L_1$ loss \\ perceptual loss \\ luminance loss} & \makecell{LOL SID in RGB \\ MIT-Adobe FiveK} & \makecell{LOL SID in RGB \\ MIT-Adobe FiveK \\ MEF NPE DICM VV} & \makecell{PSNR SSIM \\ NIQE \#P \\ FLOPs Runtime} & RGB & PyTorch & \\
%				
%				& SIDGAN & SL & U-Net & CycleGAN loss & SIDGAN & SIDGAN & PSNR SSIM TPSNR TSSIM ATWE & Raw & TensorFlow & \\
%				
%				& RRDNet & ZSL & three subnetworks & \makecell{retinex reconstruction loss \\ texture enhancement loss \\ noise estimation loss} & - & \makecell{NPE LIME \\ MEF DICM} & NIQE CPCQI & RGB & PyTorch & \checkmark \\
%				
%				& TBEFN & SL & \makecell{three stages \\ U-Net like network} & \makecell{SSIM loss \\ perceptual loss \\ smoothness loss}& SCIE LOL & SCIE LOL DICM MEF NPE VV & \makecell{PSNR SSIM \\ NIQE Runtime \\ \#P FLOPs} & RGB & TensorFlow & \checkmark \\ 
%				
%				& DSLR & SL & \makecell{Laplacian pyramid \\ U-Net like network} & \makecell{$L_2$ loss \\ Laplacian loss \\ color loss} & MIT-Adobe FiveK & MIT-Adobe FiveK self-selected & \makecell{PSNR SSIM \\ NIQMC NIQE \\ BTMQI CaHDC} & RGB & PyTorch & \\
%				
%				\hline
%			\end{tabular}
%		}
%		\captionsetup{font=scriptsize} %设置标题字体与表格字体一致
%		\caption{\label{tab: Summary}
%			Summary of essential characteristics of representative deep learning-based methods, including learning strategies, network structures, loss functions, training datasets, testing datasets, evaluation metrics, data formats of input, and whether the models are Retinex-based or not. "simulated" means the testing data are simulated by the same approach as the synthetic training data. "self-selected" stands for the real-world images selected by the authors. "\#P" represents the number of trainable parameters. "-" means this item is not available or not indicated in the paper.} %表格的标题
%		
%	\end{table}
%	
%	Table~\ref{tab: Summary}给出了最近几年主流的基于深度学习的LLIE方案，并从不同角度对其进行了划分。
	
%	\subsection{Paper reading}
%	
%	\subsubsection{Switching gaussian mixture variational rnn for anomaly detection of diverse cdn websites\cite{dai2022switching}}
%	
%	\paragraph{Known problems}
%	
%	\textbf{问题一}：单个网站在不同时间段的非平稳依赖（The Non-stationary Dependencies）会降低深度异常检测模型的性能:
%	
%	由于用户的正常行为或CDN的调度等，相应的KPI通常表现出非平稳的时间特征，不应将其归类为服务失败或退化。但是，这些类型的预期模式很难被目前的方法捕获，这会使得目前的模型在CDN KPI异常检测方面的表现性能很差。
%	
%	从Fig. \ref{fig: cdn_kpi_a}和Fig. \ref{fig: cdn_kpi_d}可以明显看出，工作日和周末的用户请求行为是不同的。前一个网站显示了在周末相对于工作日的用户请求激增，而后一个数据显示了相反的情况。
%	
%	\begin{figure}[htbp] 
%		% read manual to see what [ht] means and for other possible options
%		\centering 
%		% \includegraphics[width=0.8\columnwidth]{GLADNet}
%		
%		\begin{subfigure}{0.2\textwidth}
%			\includegraphics[width=\linewidth]{CDN_KPI/cdn_kpi_a}
%			\captionsetup{font=scriptsize}
%			\caption{}
%			\label{fig: cdn_kpi_a}
%		\end{subfigure}
%		\begin{subfigure}{0.2\textwidth}
%			\includegraphics[width=\linewidth]{CDN_KPI/cdn_kpi_b}
%			\captionsetup{font=scriptsize}
%			\caption{}
%			\label{fig: cdn_kpi_b}
%		\end{subfigure}
%		\begin{subfigure}{0.2\textwidth}
%			\includegraphics[width=\linewidth]{CDN_KPI/cdn_kpi_c}
%			\captionsetup{font=scriptsize}
%			\caption{}
%			\label{fig: cdn_kpi_c}	
%		\end{subfigure}\\
%		\begin{subfigure}{0.2\textwidth}
%			\includegraphics[width=\linewidth]{CDN_KPI/cdn_kpi_d}
%			\captionsetup{font=scriptsize}
%			\caption{}
%			\label{fig: cdn_kpi_d}
%		\end{subfigure}
%		\begin{subfigure}{0.2\textwidth}
%			\includegraphics[width=\linewidth]{CDN_KPI/cdn_kpi_e}
%			\captionsetup{font=scriptsize}
%			\caption{}
%			\label{fig: cdn_kpi_e}
%		\end{subfigure}
%		\begin{subfigure}{0.2\textwidth}
%			\includegraphics[width=\linewidth]{CDN_KPI/cdn_kpi_f}
%			\captionsetup{font=scriptsize}
%			\caption{}
%			\label{fig: cdn_kpi_f}	
%		\end{subfigure}	
%		\captionsetup{font=scriptsize}
%		\caption{
%			\label{fig: CDN_KPI} % spaces are big no-no withing labels
%			% things like fig: are optional in the label but it helps
%			% to orient yourself when you have multiple figures,
%			% equations and tables
%			2-weeks real world typical multivariate CDN KPIs of 6-websites. Periods in light blue show the change points in KPIs; Regions highlighted
%		}
%	\end{figure}
%	
%	\textbf{问题二}：不同的网站在CDN KPI中表现出不同的特征，但其中又有一些相似特征。这使得现有的单个深度异常检测
%	模型无法良好的捕捉这种动态复杂性。
%	
%	Fig. \ref{fig: cdn_kpi_b}是另一种典型情况，即部分用户被CDN的调度中心调度到另一组边缘节点上；所以，KPI的变化时刻，发生在这段时间内。Fig. \ref{fig: cdn_kpi_a}中的视频点播网站的KPI通常与Fig. \ref{fig: cdn_kpi_e}中的直播网站有很大的不同。
%	
%	因为一般商业CDN会为数百甚至数千个网站提供服务，这些网站会因为服务类型和用户的请求行为的不同而表现出不同的特征，因此可以观察到不同网站的KPI在空间和时间特征上的变化。
%	
%	\textbf{问题三}：资源浪费
%	
%	为了对多个网站进行有效的异常检测，现有的深度异常检测方法通常为每个网站训练一个单独的模型，从而存在为每个网站训练和维护大量的单独模型的问题，不仅消耗巨大的计算和存储资源，但也增加了模型维护的成本。
%	
%	例如，Fig. \ref{fig: cdn_kpi_c}和Fig. \ref{fig: cdn_kpi_f}显示出相似的特征，但它们属于不同网站的KPI。在这种情况下，为每个网站训练一个单独的模型完全是浪费。
%	
%	\paragraph{Challenge}
%	
%	现有的许多深度学习方法，模型学习正常情况下的网络KPI，以构建并训练模型，从而应用于无监督的网络KPI异常检测，即深度异常检测。
%	
%	CDN KPI数据存在的问题:
%	
%	(1) CDN下具有多个节点，每个节点网络KPI曲线的多样性，即KPI曲线表现为周期型的，有稳定型的，也有不稳定
%	型的。
%	
%	(2) CDN下往往具备大量的节点网络，这些网络的异常种类多，核心网网元数据多，故障发生的类型也多种多样，
%	导致了异常种类也多种多样，即KPI之间呈现出非平稳的顺序关系。
%	
%	\paragraph{Author Work}
%	
%	(1) 表征不同的复杂KPI时间序列结构和动态特征；
%	
%	(2) 基于以上问题，现有的深度学习方法非常难以表示和识别CDN网下节点网络的异常，因此作者提出了一种方法，一种适用于多变量CDN KPI的切换高斯混合变分循环神经网络(SGmVRN)。
%		
%		1. SGmVRNN引入变分循环结构，并将其潜在变量分配到混合高斯分布中，以对复杂的KPI时间序列进行建模并捕捉其中的不同结构和动态特征，而在下一步中，它结合了一种切换机制来表征这些多样性，从而学习更丰富的KPI表示。
%		
%		2. 为了有效的推理，我们开发了一种向上向下的自动编码推理方法，它结合了参数的自下而上的似然性和自下而上的先验信息，以实现准确的后验近似。
%	
%	(3) 结果：通过SGmVRNN方法构造出来的模型应用到不同网站的CDN KPI异常识别，SGmVRNN模型的平衡F分数明显高于目前最优秀的模型。
%	
%	\paragraph{Proposed Method}
%	
%	SGmVRNN：将概率混合和切换机制融合到VRNN中，它将概率混合和切换机制融合到VRNN中，从而有效地模拟单个网站的多元CDN KPI相邻时间步之间的非平稳时间依赖性，以及不同网站之间的动态特性。
%		
%		SGmVRNN特点：能够在不同的时间学习不同的结构特征，并捕捉它们之间的各种时间依赖性，以表征不同CDN网站的多元KPI中的复杂结构和动态特征。
%		
%		\begin{figure}[htbp] 
%			% read manual to see what [ht] means and for other possible options
%			\centering 
%			% \includegraphics[width=0.8\columnwidth]{GLADNet}
%			
%			\begin{subfigure}{0.19\textwidth}
%				\includegraphics[width=\linewidth]{Switching/Prior}
%				\captionsetup{font=scriptsize}
%				\caption{Prior}
%				\label{fig: Prior}
%			\end{subfigure}
%			\begin{subfigure}{0.19\textwidth}
%				\includegraphics[width=\linewidth]{Switching/Generation}
%				\captionsetup{font=scriptsize}
%				\caption{Generation}
%				\label{fig: Generation}
%			\end{subfigure}
%			\begin{subfigure}{0.19\textwidth}
%				\includegraphics[width=\linewidth]{Switching/Inference}
%				\captionsetup{font=scriptsize}
%				\caption{Inference}
%				\label{fig: Inference}	
%			\end{subfigure}
%			\begin{subfigure}{0.19\textwidth}
%				\includegraphics[width=\linewidth]{Switching/Recurrent}
%				\captionsetup{font=scriptsize}
%				\caption{Recurrent}
%				\label{fig: Recurrent}
%			\end{subfigure}
%			\begin{subfigure}{0.19\textwidth}
%				\includegraphics[width=\linewidth]{Switching/Overall}
%				\captionsetup{font=scriptsize}
%				\caption{Overall}
%				\label{fig: Overall}
%			\end{subfigure}
%			\captionsetup{font=scriptsize}
%			\caption{
%				\label{fig: Switching Mechanism} % spaces are big no-no withing labels
%				% things like fig: are optional in the label but it helps
%				% to orient yourself when you have multiple figures,
%				% equations and tables
%				Graphical illustration of each operation of the SGmVRNN: (a) conditional prior of latent variables $z_{t;n}$ and $c_{t;n}$; (b) generation process of $x_{t;n}$; (c) inference of the variational distribution of $z_{t;n}$ and $c_{t;n}$; (d) updating the hidden units of the RNN recurrently; (e) overall operations of the SGmVRNN. Note that circles denote stochastic variables while diamond-shaped units are used for deterministic variables, and shaded nodes denote observed variables.
%			}
%		\end{figure}
%		
%	如何解决主要挑战？
%	
%	将切换机制(见Fig. \ref{fig: Switching Mechanism})与混合模型相结合，SGmVRNN可以利用切换机制对电流输入的刻画和多种时间变化的传输提供足够的表示能力，从而解决第一个问题。
%	
%	利用混合高斯分布潜变量很好地处理多样化的网站，从而解决第二个问题。
	
	\section{个人工作进展}
	
%	\subsection{关于“华为杯”的一点思考}
%	
%	\subsubsection{适用多场景的通用化时序预测算法}
%	
%	\cite{dai2022switching}中提出的方法SGmVRNN在可以表征复杂数据，面对拥有序列多样性的数据集（题目中提到数据可能具有不同长度、趋势性、周期性、采样粒度），作者提出了可以把潜在变量(latent variable)分配到混合高斯分布中，从而可以对复杂的时间序列进行建模。切换机制似乎可以较好的满足通用化这样一种需求。
%	
%	作者引入切换机制是为了解决CDN网络下非平稳的时间依赖性以及不同网站的KPI特征，题目描述中针对的是云计算的场景，如资源的消耗序列、用户的行为序列、机房传感器的读数序列，对不同的场景设计不同的算法，显然不太现实。二者可能具有类通性。
	
	\subsection{思考}
	
	具有融合和模型框架的算法往往具有更好的泛化能力，并且无监督学习方法比有监督学习方法更稳健。
	
	低照度图像增强算法的目的是为更高级别的视觉任务做准备。因此，当前的研究旨在制定通用的低照度图像增强算法，以服务于更高级别的视觉任务，所以低照度图像增强算法的速度应该尽可能的快，这样可以使得应用在高帧率的低照度视频增强上成为一种可能。
	
	无监督学习方法上述方法有两个限制。首先，数据集中的成对图片是有限的。其次，在成对数据集上的训练模型会导致过拟合问题。
	
	目前在有监督学习这方面的热点是结合Transformer方法，这一类方法是一种全监督和训练模式，属于超轻量级网络。这一类方法的性能比较好，一般特征往往体现在速度的提升上。。
	
	
%	\subsection{梳理损失函数}
%	
%	这一周的工作主要是接着梳理上一周未梳理完成的损失函数
%	
%	1) \textbf{Consistency loss function}一般用于半监督学习中，用于对unlabeled data进行变换和扰动，如果模型在这些不同的变换下，结果相似或者一致，就可以说明模型具有一定的鲁棒性和泛化性。同样，其没有具体的表现形式，取决于具体的任务和模型架构，一般情况下不单独使用，而是与其他损失函数结合使用，共同训练模型。此外总结了三种主流的半监督学习方法。
%	
%	2) \textbf{Color loss function}是一种实现图像颜色的校正的损失函数。一般结合其他损失函数使用，在结合$\mathcal{L}_2$-loss时，因为$\mathcal{L}_2$-loss只是能够从数值上测量色差，无法保证颜色向量的方向是相同的，这可能会导致颜色mismatch，Color loss需要计算夹角余弦。
%	
%	3) \textbf{Laplacian loss function}则是衡量预测结果与GT之间对应像素上的颜色差异。使用这个损失能够得到更好的结果。它还能捕捉全局以及局部的差异。此外，额外去了解了一下拉普拉斯金字塔的详细原理。
%	
%	从数学角度看，图像的高斯模糊过程就是图像与正态分布做卷积，在二维卷积里面，一个正方形高斯核的值中间高、四周边缘低，大致呈一个“圆形”，拉普拉斯金字塔与高斯金字塔相反，是个倒着的金字塔，也就是图片尺度逐级变大，但是这个并不是单纯把图像放大，它要计算 [原图像] 和 [下采样、上采样后图像] 两者差值，得到的结果尺度逐渐变大，形成一个金字塔。高斯金字塔下采样过程中丢失的信息在上采样后并不能完全恢复，丢失信息的情况依然存在，如果直观把原图和下采样、上采样后的图片进行对比，会发现后者仍然模糊，此时用前者减去后者得到的新图像，形成的就是拉普拉斯金字塔。这个相减的过程，能够更突出显示图像的像素信息，因为高斯核属于低通滤波，图像经过这个操作后得到的结果更加平滑，边界信息相比原图没那么明显（因为图像中高频的边缘部分都被过滤），而原图这些边界信息较为明显，至于非边界部分信号变化不大，因此相减后得到的图像更关注边界的信息，或者说对高频信号更为敏感。
%
%	4) 阅读文献《Switching gaussian mixture variational rnn for anomaly detection of diverse cdn websites》,作者提出了一种适用于多元CDN kpi的开关高斯混合变分递归神经网络(SGmVRNN)。具体来说，SGmVRNN引入了变分循环结构和分配其潜在变量到混合高斯分布模型复杂KPI时间序列和捕获各种结构和动态特征，而在下一步它包含一个转换机制来描述这些多样性，从而学习丰富的KPI表示。
%	
%	为了进行有效的推理，作者提出了一种向上-向下的自编码推理方法，该方法结合了自下而上的似然值和参数的上下先验信息，以实现精确的后验逼近。在真实世界数据上进行的大量实验表明，根据来自不同网站的CDN kpi上的f1分数，SGmVRNN的性能显著优于最先进的方法。
%	
%	
%	\subsection{复现KinD代码}
%	
%	KinD\footnote{https://github.com/zhangyhuaee/KinD}采用的是类U-Net网络，且对比使用了多种损失函数，采用的数据集相对较少，且环境配置相对简单，具体见Tab. \ref{tab: Summary}。同时，KinD\cite{10.1145/3343031.3350926}目前引用数量为543，采用的网络架构较为经典，即Retinex-based的思想。
%	
%		\subsubsection{Requirement}
%	
%		\begin{python}
%		Python
%		Tensorflow >= 1.10.0
%		numpy, PIL
%		\end{python}
%	
%		\subsubsection{Model}
%		
%		\paragraph{lrelu}
%		
%		\begin{python}
%			def lrelu(x, trainbable=None):
%				return tf.maximum(x*0.2,x)
%		\end{python}
%
%		lrelu函数实现了带有泄露的线性整流单元(Leaky ReLU)，其作用是在神经网络中引入非线性变换。
%		
%		线性整流单元(ReLU)是一种常用的激活函数，对于输入$x$，当$x$大于等于零时，输出为$x$，当$x$小于零时，输出为零。这种函数在训练神经网络时具有很好的性质，但是在某些情况下，当输入为负值时，导数为零会导致梯度消失的问题。
%		
%		为了解决这个问题，Leaky ReLU 引入了一个小的负斜率，在输入为负值时，输出为输入乘以一个小的斜率。公式表示如下：
%		
%		$ lrelu(x) = max(x * alpha, x) $
%		
%		\subparagraph{input}
%		
%		其中，x是输入，alpha是小的斜率（一般取很小的正数，如0.2），max函数选择两者中的较大值作为输出。
%		
%		\subparagraph{function}
%		
%		Leaky ReLU 通过引入负斜率，保留了负值的信息，从而解决了梯度消失问题，并且具有更好的收敛性和表达能力。
%		
%		在给定的代码中，lrelu函数接受输入x和一个可选的参数trainable（默认为None），并返回x乘以斜率0.2和x中的较大值作为输出。该函数可以作为激活函数在神经网络的层中使用，引入非线性变换，增加网络的表达能力。
%		
%		\paragraph{upsample\_and\_concat}
%		
%		\begin{python}
%			def upsample_and_concat(x1, x2, output_channels, in_channels, scope_name, trainable=True):
%			with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE) as scope:
%			pool_size = 2
%			deconv_filter = tf.get_variable('weights', [pool_size, pool_size, output_channels, in_channels], trainable= True)
%			deconv = tf.nn.conv2d_transpose(x1, deconv_filter, tf.shape(x2) , strides=[1, pool_size, pool_size, 1], name=scope_name)
%			
%			deconv_output =  tf.concat([deconv, x2],3)
%			deconv_output.set_shape([None, None, None, output_channels*2])
%			
%			return deconv_output
%		\end{python}
%		
%		函数的作用是将两个特征图进行上采样和拼接操作。在图像处理中，上采样是指将特征图的尺寸扩大，通常使用插值等方法来实现。而拼接操作则是将两个特征图在通道维度进行连接。
%		
%		这个函数常用于 U-Net 等图像分割和生成网络中，用于实现特征图的跳跃连接，从而传递低层次的细节信息到高层次的特征图中，提升模型性能和图像质量。
%		
%		\subparagraph{input} 
%		
%			x1：第一个输入特征图。
%			
%			x2：第二个输入特征图。
%			
%			output\_channels：输出特征图的通道数。
%			
%			in\_channels：输入特征图的通道数。
%			
%			scope\_name：变量作用域的名称。
%			
%			trainable：是否可训练的布尔值，默认为 True。
%		
%		\subparagraph{function}
%		
%		在给定的变量作用域下，创建一个名为 'weights' 的变量，它是一个大小为 [pool\_size, pool\_size, output\_channels, in\_channels] 的可训练的卷积核。
%		
%		使用 tf.nn.conv2d\_transpose 函数对 x1 进行转置卷积操作（也称为反卷积），使用创建的卷积核 deconv\_filter，步长为 [1, pool\_size, pool\_size, 1]，以获得上采样后的特征图 deconv。
%		
%		将 deconv 和 x2 进行通道维度上的拼接，通过 tf.concat 函数将它们在通道维度上连接起来，得到 deconv\_output。
%		
%		设置 deconv\_output 的形状为 [None, None, None, output\_channels*2]，其中 None 表示该维度的大小可以根据输入的形状自动推断。
%		
%		返回拼接后的特征图 deconv\_output。	
%		
%		\paragraph{DecomNet\_simple}
%		
%		\begin{python}
%		def DecomNet_simple(input):
%			with tf.variable_scope('DecomNet', reuse=tf.AUTO_REUSE):
%				conv1=slim.conv2d(input,32,[3,3], rate=1, activation_fn=lrelu,scope='g_conv1_1')
%				pool1=slim.max_pool2d(conv1, [2, 2], stride = 2, padding='SAME' )
%				conv2=slim.conv2d(pool1,64,[3,3], rate=1, activation_fn=lrelu,scope='g_conv2_1')
%				pool2=slim.max_pool2d(conv2, [2, 2], stride = 2, padding='SAME' )
%				conv3=slim.conv2d(pool2,128,[3,3], rate=1, activation_fn=lrelu,scope='g_conv3_1')
%				up8 =  upsample_and_concat( conv3, conv2, 64, 128 , 'g_up_1')
%				conv8=slim.conv2d(up8,  64,[3,3], rate=1, activation_fn=lrelu,scope='g_conv8_1')
%				up9 =  upsample_and_concat( conv8, conv1, 32, 64 , 'g_up_2')
%				conv9=slim.conv2d(up9,  32,[3,3], rate=1, activation_fn=lrelu,scope='g_conv9_1')
%				# Here, we use 1*1 kernel to replace the 3*3 ones in the paper to get better results.
%				conv10=slim.conv2d(conv9,3,[1,1], rate=1, activation_fn=None, scope='g_conv10')
%				R_out = tf.sigmoid(conv10)
%				
%				l_conv2=slim.conv2d(conv1,32,[3,3], rate=1, activation_fn=lrelu,scope='l_conv1_2')
%				l_conv3=tf.concat([l_conv2, conv9],3)
%				# Here, we use 1*1 kernel to replace the 3*3 ones in the paper to get better results.
%				l_conv4=slim.conv2d(l_conv3,1,[1,1], rate=1, activation_fn=None,scope='l_conv1_4')
%				L_out = tf.sigmoid(l_conv4)
%				
%				return R_out, L_out
%		\end{python}
%	
%		函数是一个图像分解网络(Image Decomposition Network)的简化版本。它的主要作用是将输入的图像分解为反射分量(Reflection component)和照明分量(Illumination component)两部分。
%		
%		函数接受一个输入图像作为参数，并在TensorFlow的变量作用域'DecomNet'下构建网络。该网络包含一系列的卷积、池化和上采样操作，以及一些连接操作和卷积层。
%	
%		\paragraph{input}
%		
%		input（一个张量，作为输入图像）。
%		
%		\paragraph{function}
%		
%		实现图像的分解网络。
%		
%		具体地，DecomNet\_simple 函数的网络结构如下：
%		
%		1. 输入图像经过第一层卷积层 conv1，输出通道数为 32，卷积核大小为 3x3，使用 Leaky ReLU 激活函数。
%		
%		2. 经过池化层 pool1，进行最大池化，池化窗口大小为 2x2，步长为 2。
%		
%		3. 经过第二层卷积层 conv2，输出通道数为 64，卷积核大小为 3x3，使用 Leaky ReLU 激活函数。
%		
%		4. 经过池化层 pool2，进行最大池化，池化窗口大小为 2x2，步长为 2。
%		
%		5. 经过第三层卷积层 conv3，输出通道数为 128，卷积核大小为 3x3，使用 Leaky ReLU 激活函数。
%		
%		6. 调用 upsample\_and\_concat 函数，将 conv3 和 conv2 进行上采样并连接起来，输出通道数为 64。
%		
%		7. 经过第八层卷积层 conv8，输出通道数为 64，卷积核大小为 3x3，使用 Leaky ReLU 激活函数。
%		
%		8. 调用 upsample\_and\_concat 函数，将 conv8 和 conv1 进行上采样并连接起来，输出通道数为 32。
%		
%		9. 经过第九层卷积层 conv9，输出通道数为 32，卷积核大小为 3x3，使用 Leaky ReLU 激活函数。
%		
%		10. 经过最后一层卷积层 conv10，输出通道数为 3，卷积核大小为 1x1，不使用激活函数。该层用于生成反射分量（Reflection component）。
%		
%		11. 使用 sigmoid 函数对 conv10 的输出进行映射，得到反射分量 R\_out。
%		
%		12. 经过第一层卷积层 l\_conv2，输出通道数为 32，卷积核大小为 3x3，使用 Leaky ReLU 激活函数。
%		
%		13. 将 l\_conv2 和 conv9 进行连接操作，得到连接特征图 l\_conv3。
%		
%		14. 经过一层卷积层 l\_conv4，输出通道数为 1，卷积核大小为 1x1，不使用激活函数。该层用于生成照明分量（Illumination component）。
%		
%		15. 使用 sigmoid 函数对 l\_conv4 的输出进行映射，得到照明分量 L\_out。
%		
%		最终，DecomNet\_simple 函数返回反射分量 R\_out 和照明分量 L\_out。这个函数的目标是将输入图像分解为反射分量和照明分量，以便进一步进行图像修复或图像增强等后续处理。
	
%		\subsubsection{Train}
%	
%			KinD Network分为三部分(Fig. \ref{fig: network})：(1)图像分解网络：Layer Decomposition Net;(2)反射分量纠正网络：Reflectance Restoration Net;(3)光照分量纠正网络：Illumination Adjustment Net。
%	
%		\begin{figure}[htbp]
%			% read manual to see what [ht] means and for other possible options
%			\centering \includegraphics[width=0.8\columnwidth]{network}
%			\captionsetup{font=scriptsize}
%			\caption{
%				\label{fig: network} % spaces are big no-no withing labels
%				% things like fig: are optional in the label but it helps
%				% to orient yourself when you have multiple figures,
%				% equations and tables
%				The architecture of our KinD network. Two branches correspond to the reflectance and illumination, respectively. From the perspective of functionality, it also can be divided into three modules, including layer decomposition, reflectance restoration, and illumination adjustment.
%			}
%		\end{figure}
%	
%		以暗光/正常光照图像($I_{low}/I_{high}$)对作为训练样本，Layer Decomposition Net对($I_{low}/I_{high}$)依次进行分解，得到光照分量$L_{low}$、$L_{high}$和反射分量$R_{low}$、$R_{high}$。再通过Reflectance Restoration Net和Illumination Adjustment Net得到$\tilde{R}_{low}$和$\tilde{L}_{low}$
%	。
%	
%			\paragraph{Layer Decomposition Net}
%		
%				Layer Decomposition Net有两个分支，一个分支用于预测反射分量，另一个分支用于预测光照分量，反射分量分支以五层Unet网络为主要网络结构，后接一个卷积层和Sigmoid层。光照分量分支由三个卷积层构成，其中还利用了反射分量分支中的特征图，具体细节可参考论文。
%				
%				Layer Decomposition Net对($I_l / I_h$)依次进行分解，得到光照分量$L_l$、$L_h$和反射分量$R_l$、$R_h$。
%				
%				从Fig. \ref{fig: network}可以看出，KinD的损失函数主要由三部分损失构成，它们分别是层分解部分损失、反射重建部分损失以及亮度调整部分损失。
%				
%				层分解(Layer Decomposition Net)部分损失定义如下：
%		
%			\begin{equation}
%				\mathcal{L}^{LD}=\mathcal{L}_{rec}^{LD}+0.01\mathcal{L}_{rs}^{LD}+0.15\mathcal{L}_{is}^{LD}+0.2\mathcal{L}_{mc}^{LD}
%			\end{equation}
%			
%			其中，$\mathcal{L}_{rs}^{LD}=\left\|R_l-R_h\right\|1$表示反射相似性损失(Reflectance Similarity)，即短曝光与长曝光图形的反射图应该是相同的；$$\mathcal{L}_{is}^{LD}=\left\| \frac{\nabla L_{l}}{\max(\left| \nabla I_{l} \right|,\varepsilon)} \right\|_{1} + \left\| {\frac{\nabla L_{h}}{\max (\left| \nabla I_{h} \right| ,\varepsilon)}}\right\|_{1}$$表示亮度平滑损失约束(Illumination Smoothness)，它度量了亮度图与输入图像之间的相对结构，边缘区域惩罚较小，平滑区域惩罚较大；$\mathcal{L}_{mc}^{LD}={\left\|M\circ\exp(-c\cdot M)\right \|}_{1}, M=\nabla \mathcal{L}_{l}+\nabla \mathcal{L}_{h}$表示相互一致性约束(Mutual Consistency)，它意味着强边缘得以保留，弱边缘被抑制；$\mathcal{L}_{rec}^{LD}={\left \| I_l-R_t \circ \mathcal{L}_l\right\|}_{1} + {\left \| I_h-R_h \circ \mathcal{L}_h\right\|}_{1}$表示重建损失(Reconstruction Error)。
%		
%			Layer Decomposition Net复现代码如下
%			\begin{python}
%			# 设置一些训练所需的参数
%			batch_size = 10
%			patch_size = 48
%				
%			# 创建一个TensorFlow会话
%			sess = tf.Session()
%				
%			# 定义输入的占位符，这些占位符用于接收低分辨率和高分辨率的输入图像。
%			input_low = tf.placeholder(tf.float32, [None, None, None, 3], name='input_low')
%			input_high = tf.placeholder(tf.float32, [None, None, None, 3], name='input_high')
%				
%			# 使用定义的模型构建计算图,DecomNet_simple是一个分解网络模型，它接受输入图像并输出反射和亮度组件
%			[R_low, I_low] = DecomNet_simple(input_low)
%			[R_high, I_high] = DecomNet_simple(input_high)
%				
%			# 将反射和亮度组件拼接起来形成输出图像
%			# 这一步操作将反射和亮度组件进行通道拼接，以生成输出图像。
%			I_low_3 = tf.concat([I_low, I_low, I_low], axis=3)
%			I_high_3 = tf.concat([I_high, I_high, I_high], axis=3)
%			output_R_low = R_low
%			output_R_high = R_high
%			output_I_low = I_low_3
%			output_I_high = I_high_3
%				
%			# 定义损失函数
%				
%			def mutual_i_loss(input_I_low, input_I_high):
%			
%			# 互信息损失函数的定义
%				...
%				
%			def mutual_i_input_loss(input_I_low, input_im):
%			# 输入互信息损失函数的定义
%			...
%				
%			recon_loss_low = tf.reduce_mean(tf.abs(R_low * I_low_3 - input_low))
%			recon_loss_high = tf.reduce_mean(tf.abs(R_high * I_high_3 - input_high))
%			equal_R_loss = tf.reduce_mean(tf.abs(R_low - R_high))
%			i_mutual_loss = mutual_i_loss(I_low, I_high)
%			i_input_mutual_loss_high = mutual_i_input_loss(I_high, input_high)
%			i_input_mutual_loss_low = mutual_i_input_loss(I_low, input_low)
%				
%			loss_Decom = 1*recon_loss_high + 1*recon_loss_low + 0.01*equal_R_loss + 0.2*i_mutual_loss + 0.15*i_input_mutual_loss_high + 0.15*i_input_mutual_loss_low
%				
%			# recon_loss_low 和 recon_loss_high 是重构损失，用于衡量输出图像与输入图像之间的差异。
%			# equal_R_loss 是反射一致性损失，用于衡量两个不同尺度下的反射分量之间的一致性。
%			# i_mutual_loss 是亮度互信息损失，用于鼓励亮度分量之间的一致性。
%			# i_input_mutual_loss_high 和 i_input_mutual_loss_low 是输入亮度与反射之间的互信息损失，用于鼓励输入图像与反射分量之间的一致性。
%			\end{python}
%			
%			最后定义优化器和训练操作
%			
%			\begin{python}
%			# 我们使用 Adam 优化器来最小化损失函数，其中只更新 DecomNet 模型的可训练变量。
%			lr = tf.placeholder(tf.float32, name='learning_rate')
%			optimizer = tf.train.AdamOptimizer(learning_rate=lr, name='AdamOptimizer')
%			var_Decom = [var for var in tf.trainable_variables() if 'DecomNet' in var.name]
%			train_op_Decom = optimizer.minimize(loss_Decom, var_list=var_Decom)
%			sess.run(tf.global_variables_initializer())
%			saver_Decom = tf.train.Saver(var_list=var_Decom)
%				
%			# 加载数据集
%			# 这里使用 glob 函数获取训练集的低分辨率和高分辨率图像的文件名，并进行排序。
%			train_low_data = []
%			train_high_data = []
%			train_low_data_names = glob('./LOLdataset/our485/low/*.png') 
%			train_low_data_names.sort()
%			train_high_data_names = glob('./LOLdataset/our485/high/*.png') 
%			train_high_data_names.sort()
%			
%			# 定义了一些辅助变量和文件夹路径
%			# epoch 表示训练的总轮数，learning_rate 表示学习率，sample_dir 是保存样本图像的文件夹路径，checkpoint_dir 是保存模型检查点的文件夹路径。
%			epoch = 2000
%			learning_rate = 0.0001
%			sample_dir = './Decom_net_train/'
%			checkpoint_dir = './checkpoint/decom_net_train/'
%			\end{python}
%			
%				最后开始训练循环
%			
%			\begin{python}
%				for epoch in range(start_epoch, epoch):
%				for batch_id in range(start_step, numBatch):
%				# 获取一个批次的训练数据
%				...
%				
%				# 执行训练操作，计算损失
%				_, loss = sess.run([train_op, train_loss], feed_dict={input_low: batch_input_low, input_high: batch_input_high, lr: learning_rate})
%				
%				# 打印训练进度和损失
%				print("%s Epoch: [%2d] [%4d/%4d] time: %4.4f, loss: %.6f" % (train_phase, epoch + 1, batch_id + 1, numBatch, time.time() - start_time, loss))
%				
%				# 每隔100个批次保存模型和样本图像
%				if (epoch + 1) % 100 == 0:
%				print('Saving sample images...')
%				sample_results = sess.run([output_R_low, output_R_high, output_I_low, output_I_high], feed_dict={input_low: sample_input_low, input_high: sample_input_high})
%				save_images(sample_results, [batch_size, 1], sample_dir + 'train_%d.png' % (epoch + 1))
%				
%				print('Saving model...')
%				saver.save(sess, checkpoint_dir + 'model.ckpt', global_step=epoch + 1)
%				
%				# 每隔500个批次降低学习率
%				if (epoch + 1) % 500 == 0:
%				learning_rate /= 10
%			\end{python}
%			
%				在每个epoch的训练过程中，当遍历完一个批次后，计算并打印损失值。
%				
%				当达到一定条件时，例如每隔100个批次，保存模型和样本图像。首先，我用当前模型对一部分样本进行推断，并将结果保存为图像文件。然后，保存模型的检查点，以便在需要时恢复模型。
%				
%				另外，每隔500个批次，将学习率除以10，以实现学习率的衰减。
%		
%			\paragraph{Reflectance Restoration Net}
%			
%				作者认为来自低光图像的反射率图比来自明亮光图像的反射率图更容易受到退化的干扰。利用更清晰的反射率作为混乱反射率的参考(非正式的地面实况)。对于寻找Restoration函数，目标简单定义如Eq. \ref{eq: Restoration loss}
%				\begin{equation}
%					\mathcal{L}^{RR}:={\left\| \hat{R}-R_{h} \right\|}^{2}_{2} - SSIM(\hat{R},R_{h}) + {\left\|\nabla\hat{R}-\nabla R_{h} \right\|}^{2}_{2}
%					\label{eq: Restoration loss}
%				\end{equation}
%				
%				Reflectance Restoration Net复现代码如下
%				
%				\begin{python}
%				# 定义一些超参数
%				batch_size = 4
%				patch_size = 384
%					
%				# 创建 TensorFlow 会话并配置 GPU 使用
%				config = tf.ConfigProto()
%				config.gpu_options.allow_growth = True
%				sess=tf.Session(config=config)
%					
%				# 定义输入占位符
%				input_decom = tf.placeholder(tf.float32, [None, None, None, 3], name='input_decom')
%				input_low_r = tf.placeholder(tf.float32, [None, None, None, 3], name='input_low_r')
%				input_low_i = tf.placeholder(tf.float32, [None, None, None, 1], name='input_low_i')
%				input_high_r = tf.placeholder(tf.float32, [None, None, None, 3], name='input_high_r')
%					
%				# 构建模型的图结构
%				[R_decom, I_decom] = DecomNet_simple(input_decom)
%				decom_output_R = R_decom
%				decom_output_I = I_decom
%				output_r = Restoration_net(input_low_r, input_low_i)
%				\end{python}
%				
%				loss\_restoration = loss\_square + loss\_ssim + loss\_grad：这一行代码定义了总的损失函数 loss\_restoration，它是由三个部分的损失函数相加而成。loss\_square 是图像修复前后像素差的平方和的均值，用来衡量修复后图像与原始图像的像素差异。loss\_ssim 是结构相似性损失，用来衡量修复后图像与原始图像的结构相似性。loss\_grad 是梯度损失，用来衡量修复后图像与原始图像的梯度差异。通过将这三个损失函数相加，可以综合考虑图像修复的像素准确性、结构相似性和梯度一致性。
%				
%				\begin{python}
%				# 定义损失函数
%				def grad_loss(input_r_low, input_r_high):
%					input_r_low_gray = tf.image.rgb_to_grayscale(input_r_low)
%					input_r_high_gray = tf.image.rgb_to_grayscale(input_r_high)
%					x_loss = tf.square(gradient(input_r_low_gray, 'x') - gradient(input_r_high_gray, 'x'))
%					y_loss = tf.square(gradient(input_r_low_gray, 'y') - gradient(input_r_high_gray, 'y'))
%					grad_loss_all = tf.reduce_mean(x_loss + y_loss)
%					return grad_loss_all
%					
%				def ssim_loss(output_r, input_high_r):
%					output_r_1 = output_r[:,:,:,0:1]
%					input_high_r_1 = input_high_r[:,:,:,0:1]
%					ssim_r_1 = tf_ssim(output_r_1, input_high_r_1)
%					output_r_2 = output_r[:,:,:,1:2]
%					input_high_r_2 = input_high_r[:,:,:,1:2]						ssim_r_2 = tf_ssim(output_r_2, input_high_r_2)
%					output_r_3 = output_r[:,:,:,2:3]
%					input_high_r_3 = input_high_r[:,:,:,2:3]
%					ssim_r_3 = tf_ssim(output_r_3, input_high_r_3)
%					ssim_r = (ssim_r_1 + ssim_r_2 + ssim_r_3)/3.0
%					loss_ssim1 = 1-ssim_r
%					return loss_ssim1
%					
%				loss_square = tf.reduce_mean(tf.square(output_r  - input_high_r))
%				loss_ssim = ssim_loss(output_r, input_high_r)			
%				loss_grad = grad_loss(output_r, input_high_r)
%				loss_restoration = loss_square + loss_grad + loss_ssim
%				\end{python}
%
%				lr = tf.placeholder(tf.float32, name='learning\_rate')：这一行代码创建了一个占位符lr，用于传入学习率。占位符是在TensorFlow中用于表示在运行时提供数据的节点。在训练过程中，可以通过向lr传递不同的学习率值来控制模型的学习速度。
%				
%				\begin{python}
%				# 定义优化器和训练操作
%				lr = tf.placeholder(tf.float32, name='learning_rate')
%				optimizer = tf.train.AdamOptimizer(lr)
%				train_op = optimizer.minimize(loss_restoration)
%				
%				# 初始化变量并创建模型保存器
%				sess.run(tf.global_variables_initializer())
%				saver=tf.train.Saver(max_to_keep=50)
%					
%				# 定义一些训练过程中需要使用的函数和变量
%				train_dataset = glob('./data/train/*')
%				steps_per_epoch = len(train_dataset) // batch_size
%				global_step = 0
%				learning_rate = 0.0001
%				\end{python}
%				
%				最后开始训练循环，for epoch in range(20): 这个循环用于控制训练的迭代次数，每个迭代称为一个 epoch。在每个 epoch 中，数据集被随机打乱，以增加数据的随机性。这样，整个训练过程会逐步优化模型参数，以达到更好的图像修复效果。
%				
%				第2行代码使用 random.shuffle 函数随机打乱训练数据集的顺序，以增加数据的随机性。
%				
%				第3行使用嵌套循环用于遍历每epoch 中的小批量数据。steps\_per\_epoch 是每个 epoch 中的步骤数量，通过将训练数据集的大小除以批量大小得到。
%				
%				第5行代码调用了 load\_data 函数，从训练数据集中加载一批数据。train\_dataset[] 表示取出当前批次的训练数据，patch\_size 是图像块的大小。
%
%				第7行代码通过调用 sess.run 函数执行了一次训练操作和损失函数的计算，并将结果赋值给变量 loss\_value、loss\_square\_value、loss\_ssim\_value 和 loss\_grad\_value。
%				
%				\begin{python}
%				for epoch in range(20):
%					random.shuffle(train_dataset)
%				for batch_id in range(steps_per_epoch):
%					start_time = time.time()					
%					input_low_r_data, input_low_i_data, input_high_r_data = load_data(train_dataset[batch_id*batch_size:(batch_id+1)*batch_size], patch_size)
%					
%					_, loss_value, loss_square_value, loss_ssim_value, loss_grad_value = sess.run([train_op, loss_restoration, loss_square, loss_ssim, loss_grad], feed_dict={input_low_r: input_low_r_data, input_low_i: input_low_i_data, input_high_r: input_high_r_data, lr: learning_rate})
%					
%					duration = time.time() - start_time
%					
%					global_step += 1
%					
%					if batch_id % 100 == 0:
%						print('Epoch {:d}/{:d} Batch {:d}/{:d} loss = {:.6f}, loss_square = {:.6f}, loss_ssim = {:.6f}, loss_grad = {:.6f}, time = {:.2f}s'.format(epoch + 1, 20, batch_id + 1, steps_per_epoch, loss_value, loss_square_value, loss_ssim_value, loss_grad_value, duration))
%				\end{python}
%				
%				通过这段代码不难看出，其实现了一个图像训练模型的过程，包括了数据预处理、模型构建、损失函数定义、优化器设置以及训练过程的迭代。它使用了TensorFlow框架和一些自定义的模块和函数来完成图像修复任务的训练。
%				
%				
%			\paragraph{Illumination Adjustment Net}
%		
%				一般而言，图像的不同场景和区域通常在透视和反射属性上不同，比如户外街景可以简单的分为三个部分：天空、地面和前景对象，这三个区域一般在透视和反射属性上是不相同的。天空通常是平滑的，通常与地面上的物体有不同的光源。与车行道相比，其他前景对象通常更明亮，包含更丰富的细节。
%		
%				由于不存在用于图像的地面实况光照级别。因此，为了满足不同的需求，需要一种机制来灵活地将一种光照条件转换为另一种光照条件。数据集中是成对的照明图。虽然不知道之间的确切关系，但是可以大致计算强度的元素级比率$\alpha(L_t / L_s)$。这个比例可以作为指标来训练一个调整函数从一个光照条件$L_s$到另一个标准$L_t$，如果调整光照的水平更高,则$\alpha>1$,否则$\alpha \le 1$。在测试阶段,$\alpha$可以由用户指定。该网络是轻量级的，包含3个conv层(2个conv+ReLu和1个conv)和1个Sigmoid层。我们注意到指标$\alpha$是作为输入的一部分扩展到一个特性映射上。(代码中是把这个比值扩展成一个常数通道与输入图像合并变成4通道)。光照分量的纠正与反射分量纠正类似，这里同样使用高光照图光照分量$L_t$，作为真值约束训练，网络预测得到的纠正后反射分量为$\tilde{L}$，等式Eq. \ref{Illumination Adjustment loss}是照明调节网的损失函数\footnote{https://blog.csdn.net/xspyzm/article/details/106162581}。
%				\begin{equation}
%					\mathcal{L}^{IA}:={\left\| \hat{L}-L_{t} \right\|}^{2}_{2} + {\left\| \left|\nabla\hat{L}\right|-\nabla L_{t} \right\|}^{2}_{2}
%					\label{Illumination Adjustment loss}
%				\end{equation}
%			
%			\begin{python}
%			# 设置一些训练相关的参数
%			batch_size = 10
%			patch_size = 48
%			
%			# 创建TensorFlow会话
%			sess = tf.Session()
%			
%			# 定义输入占位符
%			input_decom = tf.placeholder(tf.float32, [None, None, None, 3], name='input_decom')
%			input_low_i = tf.placeholder(tf.float32, [None, None, None, 1], name='input_low_i')
%			input_low_i_ratio = tf.placeholder(tf.float32, [None, None, None, 1], name='input_low_i_ratio')
%			input_high_i = tf.placeholder(tf.float32, [None, None, None, 1], name='input_high_i')
%			\end{python}
%		
%			下面的代码构建了图像处理的模型。首先通过DecomNet\_simple函数构建了一个分解网络模型，将输入的图像分解为反射分量和光照分量，分别保存在decom\_output\_R和decom\_output\_I中。然后通过Illumination\_adjust\_net函数构建了一个光照调整网络模型，用于调整输入的光照分量，保存在output\_i中。
%			
%			\begin{python}
%			# 构建模型
%			[R_decom, I_decom] = DecomNet_simple(input_decom)
%			decom_output_R = R_decom
%			decom_output_I = I_decom
%			output_i = Illumination_adjust_net(input_low_i, input_low_i_ratio)
%			\end{python}
%			
%			下列的代码定义了损失函数。grad\_loss函数计算输入的两个图像的梯度差异，并计算梯度差异的平均值。loss\_grad表示光照分量的梯度损失，loss\_square表示光照分量的均方差损失，而loss\_adjust是将梯度损失和均方差损失相加得到的总体损失。
%			
%			\begin{python}
%			# 定义损失函数
%			def grad_loss(input_i_low, input_i_high):
%				x_loss = tf.square(gradient(input_i_low, 'x') - gradient(input_i_high, 'x'))
%				y_loss = tf.square(gradient(input_i_low, 'y') - gradient(input_i_high, 'y'))
%				grad_loss_all = tf.reduce_mean(x_loss + y_loss)
%				return grad_loss_all
%			
%			loss_grad = grad_loss(output_i, input_high_i)
%			loss_square = tf.reduce_mean(tf.square(output_i  - input_high_i))
%			loss_adjust =  loss_square + loss_grad
%			\end{python}
%			
%			这些代码定义了一个Adam优化器，并使用它最小化总体损失loss\_adjust。train\_vars表示可训练的变量，这些变量将会在训练过程中更新。
%			
%			\begin{python}
%			# 定义优化器和训练操作
%			train_vars = tf.trainable_variables()
%			train_op_adjust = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss_adjust, var_list=train_vars)
%			\end{python}
%			
%			这行代码使用glob函数加载训练数据的文件路径。'./data/train/*.png'表示加载./data/train/路径下所有以.png结尾的文件。
%			
%			\begin{python}
%			# 初始化变量
%			sess.run(tf.global_variables_initializer())
%			
%			# 加载训练数据
%			train_data = glob('./data/train/*.png')
%			\end{python}
%		
%			这段代码中的循环用于进行训练。外层循环是30个训练轮次(epochs)，内层循环根据批大小将训练数据分成小批次进行训练。首先通过get\_image函数加载批次的图像文件，并将它们转换为浮点数类型的数组。然后将图像数组归一化到[0, 1]的范围。接下来，将归一化后的低光照图像和差值图像分别作为输入的低光照图像和高光照图像。将它们扩展维度，以匹配占位符的形状。然后执行训练操作train\_op\_adjust，并获取损失函数的值。
%		
%			\begin{python}
%			# 开始训练过程
%			for ep in range(30):
%				for idx in range(len(train_data)//batch_size):
%					batch_files = train_data[idx*batch_size:(idx+1)*batch_size]
%					batch = [get_image(batch_file) for batch_file in batch_files]
%					batch_images = np.array(batch).astype(np.float32)
%					batch_low_images = batch_images / 255.0
%					batch_high_images = batch_images - batch_low_images
%					batch_low_images = np.expand_dims(batch_low_images[:,:,:,0], axis=3)
%					batch_high_images = np.expand_dims(batch_high_images[:,:,:,0], axis=3)
%					batch_low_images_ratio = np.expand_dims(batch_low_images[:,:,:,0]/255.0, axis=3)
%					_, loss_adjust_value, loss_grad_value, loss_square_value = sess.run([train_op_adjust, loss_adjust, loss_grad, loss_square], feed_dict={input_decom: batch_images, input_low_i: batch_low_images, input_low_i_ratio: batch_low_images_ratio, input_high_i: batch_high_images})
%					
%			# 打印训练过程中的损失值,`\%2d`表示打印整数，占用两位字符的宽度；`\%.4f`表示打印浮点数，保留四位小数		
%			print("Epoch: [%2d], step: [%2d], loss_adjust: [%.4f], loss_grad: [%.4f],loss_square: [%.4f]" % (ep+1, idx+1, loss_adjust_value, loss_grad_value, loss_square_value))
%			
%			# 保存模型
%			saver.save(sess, "./model/model.ckpt")
%			\end{python}
%			
%		\subsubsection{Evaluation}
%		
%		%		\begin{python}
%			
%			
%			%		\end{python}
	
	
	\section{下周工作计划}
	
	(1) 无监督学习方法上述方法有两个限制。首先，数据集中的成对图片是有限的。其次，在成对数据集上的训练模型会导致过拟合问题。目前已有的解决方案是采用启发性学习方法，另一种是采用深度GAN，结合注意力机制捕获全局/局部的特征。
	
	(2) 目前提升速度的方法，主要是采用有监督的学习方法，在这个领域内，改进前人方法，结合Transformer优点的方法成为主流。先去了解Transformer。
	
	%	\section{Analysis}
	
	%	In this section you will need to show your experimental results. Use tables and
	%	graphs when it is possible. Table~\ref{tbl:bins} is an example.
	
	%	\begin{table}[ht]
		%		\begin{center}
			%			\caption{Every table needs a caption.}
			%			\label{tbl:bins} % spaces are big no-no withing labels
			%			\begin{tabular}{|ccc|} 
				%				\hline
				%				\multicolumn{1}{|c}{$x$ (m)} & \multicolumn{1}{c|}{$V$ (V)} & \multicolumn{1}{c|}{$V$ (V)} \\
				%				\hline
				%				0.0044151 &   0.0030871 &   0.0030871\\
				%				0.0021633 &   0.0021343 &   0.0030871\\
				%				0.0003600 &   0.0018642 &   0.0030871\\
				%				0.0023831 &   0.0013287 &   0.0030871\\
				%				\hline
				%			\end{tabular}
			%		\end{center}
		%	\end{table}
	%	
	%	Analysis of equation~\ref{eq:aperp} shows ...
	%	
	%	Note: this section can be integrated with the previous one as long as you
	%	address the issue. Here explain how you determine uncertainties for different
	%	measured values. Suppose that in the experiment you make a series of
	%	measurements of a resistance of the wire $R$ for different applied voltages
	%	$V$, then you calculate the temperature from the resistance using a known
	%	equation and make a plot  temperature vs. voltage squared. Again suppose that
	%	this dependence is expected to be linear~\cite{Cyr}, and the proportionality coefficient
	%	is extracted from the graph. Then what you need to explain is that for the
	%	resistance and the voltage the uncertainties are instrumental (since each
	%	measurements in done only once), and they are $\dots$. Then give an equation
	%	for calculating the uncertainty of the temperature from the resistance
	%	uncertainty. Finally explain how the uncertainty of the slop of the graph was
	%	found (computer fitting, graphical method, \emph{etc}.)
	%	
	%	If in the process of data analysis you found any noticeable systematic
	%	error(s), you have to explain them in this section of the report.
	%	
	%	It is also recommended to plot the data graphically to efficiently illustrate
	%	any points of discussion. For example, it is easy to conclude that the
	%	experiment and theory match each other rather well if you look at
	%	Fig.~\ref{fig:samplesetup} and Fig.~\ref{fig:exp_plots}.
	%	
	%	\begin{figure}[ht] 
		%		\centering
		%		\includegraphics[width=0.5\columnwidth]{sr_squeezing_vs_detuning}
		%		
		%		% some figures do not need to be too wide
		%		\caption{
			%			\label{fig:exp_plots}  
			%			Every plot must have axes labeled.
			%		}
		%	\end{figure}
	
	
	%	\section{Conclusions}
	%	Here you briefly summarize your findings.
	
	%++++++++++++++++++++++++++++++++++++++++
	% References section will be created automatically 
	% with inclusion of "thebibliography" environment
	% as it shown below. See text starting with line
	% \begin{thebibliography}{99}
		% Note: with this approach it is YOUR responsibility to put them in order
		% of appearance.
		
		\renewcommand{\refname}{References}
		
		
		%	\begin{thebibliography}{00}
			
			%		\bibitem{b1}\label{cite:b1}
			%		W. Wang, C. Wei, W. Yang and J. Liu, "GLADNet: Low-Light Enhancement Network with Global Awareness," 2018 13th IEEE International Conference on Automatic Face \& Gesture Recognition (FG 2018), Xi'an, China, 2018, pp. 751-755, DOI: 10.1109/FG.2018.00118.
			
			%		\bibitem{b2}\label{cite:b2}
			%		A.\ Mahajan, K.\ Somaraj and M. Sameer, "Adopting Artificial Intelligence Powered ConvNet To Detect Epileptic Seizures," 2020 IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES), Langkawi Island, Malaysia, 2021, pp. 427-432, DOI: 10.1109/IECBES48179.2021.9398832.
			
			%		\bibitem{Cyr}
			%		N.\ Cyr, M.\ T$\hat{e}$tu, and M.\ Breton,
			% "All-optical microwave frequency standard: a proposal,"
			%		IEEE Trans.\ Instrum.\ Meas.\ \textbf{42}, 640 (1993).
			
			
			
			%	\end{thebibliography}
		
		\bibliographystyle{unsrt}
		\bibliography{reference}
		
		
	\end{document}
