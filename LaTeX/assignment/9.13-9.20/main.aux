\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Pre-Knowledge}{2}{part.1}\protected@file@percent }
\citation{zhu2020eemefn}
\citation{zhu2020eemefn}
\@writefile{toc}{\contentsline {section}{\numberline {1}方向}{3}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}问题}{3}{subsection.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces   该 LLIE 结构源自\cite  {zhu2020eemefn},如其 Multi-Exposure Fusion部分采用多曝光融合结构,与由 Initial image 生成的边缘图进行 Concat,后续通过一个 U-Net 网络进一步恢复图像。 \relax }}{3}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig: EEMEFN}{{1}{3}{该 LLIE 结构源自\cite {zhu2020eemefn},如其 Multi-Exposure Fusion部分采用多曝光融合结构,与由 Initial image 生成的边缘图进行 Concat,后续通过一个 U-Net 网络进一步恢复图像。 \relax }{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces   该结构的边缘图直接从 $I$ 中通过 StyleGAN 得到边缘图 $I_s$, 而非从 $I_{\alpha }$ 中通过边缘网络获取边缘图。 \relax }}{4}{figure.caption.2}\protected@file@percent }
\newlabel{fig: Overview}{{2}{4}{该结构的边缘图直接从 $I$ 中通过 StyleGAN 得到边缘图 $I_s$, 而非从 $I_{\alpha }$ 中通过边缘网络获取边缘图。 \relax }{figure.caption.2}{}}
\citation{peng2021conformer}
\citation{peng2021conformer}
\citation{peng2021conformer}
\citation{woo2018cbam}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}创新想法}{5}{subsection.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces   Network architecture of the proposed Conformer. (a) Up-sampling and down-sampling for spatial alignment of feature maps and patch embeddings. (b) Implementation details of the CNN block, the transformer block, and the Feature Coupling Unit (FCU). (c) Thumbnail of Conformer. \relax }}{5}{figure.caption.3}\protected@file@percent }
\newlabel{fig: Conformer architecture}{{3}{5}{Network architecture of the proposed Conformer. (a) Up-sampling and down-sampling for spatial alignment of feature maps and patch embeddings. (b) Implementation details of the CNN block, the transformer block, and the Feature Coupling Unit (FCU). (c) Thumbnail of Conformer. \relax }{figure.caption.3}{}}
\citation{ramachandran2019stand}
\citation{woo2018cbam}
\citation{li2023scconv}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces   Low-Lighting 图像恢复网络。结构受 Conformer\cite  {peng2021conformer} （见 Fig. \ref {fig: Conformer architecture}）启发，将原来结构中 CNN 分支的 ResNet 结构修改为 U-Net 结构。其采用一个 U-Net 和 ViT 的并行架构，通过 U-Net 结构得到一个弱恢复的弱特征图 $\mathcal  {F}_2$，通过 ViT 融合的特征可以初步增强弱特征图 $\mathcal  {F}_2$，ViT 的输出经过 Patch Expanding 的特征图 $\mathcal  {F}_3$ 经过与 U-Net 输出的特征图 $\mathcal  {F}_2$ 融合之后得到一个初步恢复的图片 output image，用以后续参与图片的进一步恢复。 \relax }}{6}{figure.caption.4}\protected@file@percent }
\newlabel{fig: First Architecture}{{4}{6}{Low-Lighting 图像恢复网络。结构受 Conformer\cite {peng2021conformer} （见 Fig. \ref {fig: Conformer architecture}）启发，将原来结构中 CNN 分支的 ResNet 结构修改为 U-Net 结构。其采用一个 U-Net 和 ViT 的并行架构，通过 U-Net 结构得到一个弱恢复的弱特征图 $\mathcal {F}_2$，通过 ViT 融合的特征可以初步增强弱特征图 $\mathcal {F}_2$，ViT 的输出经过 Patch Expanding 的特征图 $\mathcal {F}_3$ 经过与 U-Net 输出的特征图 $\mathcal {F}_2$ 融合之后得到一个初步恢复的图片 output image，用以后续参与图片的进一步恢复。 \relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}创新想法的调研支撑}{6}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Attention in CV}{6}{subsection.1.3}\protected@file@percent }
\citation{chen2018learning}
\citation{chen2018learning,zamir2021learning}
\citation{chen2018learning}
\citation{chen2018learning,zamir2021learning}
\citation{meng2020gia}
\citation{chen2018learning,zamir2021learning}
\citation{meng2020gia}
\citation{chen2018learning,meng2020gia,zamir2021learning}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces   U-Net 图像初步恢复网络及其所属的模块。 \relax }}{7}{figure.caption.5}\protected@file@percent }
\newlabel{fig: U-Net and AM}{{5}{7}{U-Net 图像初步恢复网络及其所属的模块。 \relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {paragraph}{U-Net for LLIE}{7}{subsection.1.3}\protected@file@percent }
\citation{zhou2018unet++,zhou2019unet++}
\citation{yang2021locally,zhang2020attention}
\citation{li2018multi,zamir2020learning}
\citation{li2020visual}
\citation{xu2020learning}
\@writefile{toc}{\contentsline {paragraph}{CNN for LLIE}{8}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {part}{II\hspace  {1em}Paper Reading}{8}{part.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}LLIE}{8}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}(2023.1)LAE-Net: A locally-adaptive embedding network for low-light image enhancement}{8}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{LAE-Net：一种用于弱光图像增强的局部自适应嵌入网络}{8}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(Pattern Recognition 2023 1区) doi: 10.1016/j.patcog.2022.109039}{8}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Research Background}{8}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Contribution}{9}{subsubsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Approach}{9}{subsubsection.2.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4}Future}{9}{subsubsection.2.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}(2023.1)LACN: A lightweight attention-guided ConvNeXt network for low-light image enhancement}{9}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{LACN：采用一种轻量级的注意力引导的对流网络来增强弱光图像}{9}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(Engineering Applications of Artificial Intelligence 2区) doi: 10.1016/j.engappai.2022.105632}{9}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Research Background}{9}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Contribution}{9}{subsubsection.2.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces   Model structure of LACN. The brief process is to first perform shallow feature extraction on the image, then send it to ACM to extract features, enter ACM again after passing SKAM, moreover send the features extracted twice by ACM and shallow features into FFM, and finally carry out feature reconstruction to get an enhanced image. \relax }}{10}{figure.caption.6}\protected@file@percent }
\newlabel{fig: LACN model structure}{{6}{10}{Model structure of LACN. The brief process is to first perform shallow feature extraction on the image, then send it to ACM to extract features, enter ACM again after passing SKAM, moreover send the features extracted twice by ACM and shallow features into FFM, and finally carry out feature reconstruction to get an enhanced image. \relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces   Structure of feature fusion module. \relax }}{10}{figure.caption.7}\protected@file@percent }
\newlabel{fig: FFM structure}{{7}{10}{Structure of feature fusion module. \relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Approach}{10}{subsubsection.2.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4}Future}{11}{subsubsection.2.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}(2019.4)Low-Light Image Enhancement via a Deep Hybrid Network}{11}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{基于深度混合网络的弱光图像增强}{11}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(IEEE Transactions on Image Processing 1区) doi: 10.1109/TIP.2019.2910412}{11}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Research Background}{11}{subsubsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Contribution}{11}{subsubsection.2.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces   Proposed low-light image enhancement architecture. Our model consists of two streams, the top content stream is a residual encoder-decoder which aims to restore most of the scene. The bottom edge stream focuses on the salient edge prediction via spatially variant RNNs. To construct communications between two streams, we bridge two networks during the up-sampling stage. In addition to the MSE loss function, we also adopt the perceptual and adversarial losses to further improve the visual quality. \relax }}{11}{figure.caption.8}\protected@file@percent }
\newlabel{fig: DHN Architecture}{{8}{11}{Proposed low-light image enhancement architecture. Our model consists of two streams, the top content stream is a residual encoder-decoder which aims to restore most of the scene. The bottom edge stream focuses on the salient edge prediction via spatially variant RNNs. To construct communications between two streams, we bridge two networks during the up-sampling stage. In addition to the MSE loss function, we also adopt the perceptual and adversarial losses to further improve the visual quality. \relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}Approach}{12}{subsubsection.2.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Edge Stream}{12}{subsubsection.2.3.3}\protected@file@percent }
\newlabel{eq: max and avg}{{1}{12}{Edge Stream}{equation.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.4}Future}{12}{subsubsection.2.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}(2023.5)A Fusion-Based and Multi-Layer Method for Low Light Image Enhancement}{13}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{基于深度混合网络的弱光图像增强}{13}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(ICASSP 2023) doi: 10.1109/ICASSP49357.2023.10096454}{13}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}Research Background}{13}{subsubsection.2.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2}Contribution}{13}{subsubsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.3}Approach}{13}{subsubsection.2.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.4}Future}{13}{subsubsection.2.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces   Image decomposition and enhancement using the multi-layer model. \relax }}{14}{figure.caption.9}\protected@file@percent }
\newlabel{fig: decomposition and enhancement}{{9}{14}{Image decomposition and enhancement using the multi-layer model. \relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}(2023.5)A Fusion-Based and Multi-Layer Method for Low Light Image Enhancement}{14}{subsection.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{基于模拟多曝光融合的弱光图像增强}{14}{subsection.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(Journal of Physics: Conference Series 4区) doi: 10.1088/1742-6596/2478/6/062022}{14}{subsection.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.1}Research Background}{14}{subsubsection.2.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.2}Contribution}{14}{subsubsection.2.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.3}Approach}{14}{subsubsection.2.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces   Schematic diagram of the simulated exposure sequence generation method. For the sake of observation, $I_{subk}$ are the enhanced results of $I_{input-subk}$. \relax }}{15}{figure.caption.10}\protected@file@percent }
\newlabel{fig: Schematic diagram}{{10}{15}{Schematic diagram of the simulated exposure sequence generation method. For the sake of observation, $I_{subk}$ are the enhanced results of $I_{input-subk}$. \relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.4}Future}{15}{subsubsection.2.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}(2023.3)Fusion-Based Low-Light Image Enhancement}{15}{subsection.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{基于模拟多曝光融合的弱光图像增强}{15}{subsection.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(International Conference on Multimedia Modeling C类) doi: 10.1088/1742-6596/2478/6/062022}{15}{subsection.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.1}Research Background}{15}{subsubsection.2.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.2}Contribution}{16}{subsubsection.2.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.3}Approach}{16}{subsubsection.2.6.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces   Parallel fusion frame diagram, where the red box represents the brightness adjustment branch, and the green box represents the inception module. \relax }}{16}{figure.caption.11}\protected@file@percent }
\newlabel{fig: Parallel fusion frame diagram}{{11}{16}{Parallel fusion frame diagram, where the red box represents the brightness adjustment branch, and the green box represents the inception module. \relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}(2022.12)Dual UNet low-light image enhancement network based on attention mechanism}{17}{subsection.2.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{基于模拟多曝光融合的弱光图像增强}{17}{subsection.2.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(Multimedia Tools and Applications 4区)}{17}{subsection.2.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.7.1}Research Background}{17}{subsubsection.2.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.7.2}Contribution}{17}{subsubsection.2.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.7.3}Approach}{17}{subsubsection.2.7.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces   The framework of DUAMNet. The network is divided into T recursive stages \relax }}{18}{figure.caption.12}\protected@file@percent }
\newlabel{fig: DUAMNet framework}{{12}{18}{The framework of DUAMNet. The network is divided into T recursive stages \relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.7.4}Future}{18}{subsubsection.2.7.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8}(2022.10)Effective low-light image enhancement with multiscale and context learning network}{18}{subsection.2.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{基于多尺度和上下文学习网络的有效弱光图像增强}{18}{subsection.2.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(Multimedia Tools and Applications 4区)}{18}{subsection.2.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.8.1}Research Background}{18}{subsubsection.2.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.8.2}Contribution}{18}{subsubsection.2.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.8.3}Approach}{19}{subsubsection.2.8.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces   Overview of Multiscale and Context Learning Network (MCLNet). \relax }}{19}{figure.caption.13}\protected@file@percent }
\newlabel{fig: MCLNet Overview}{{13}{19}{Overview of Multiscale and Context Learning Network (MCLNet). \relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces   The architecture of the Bottleblock of Scale Aggregation module (BSAM) \relax }}{19}{figure.caption.14}\protected@file@percent }
\newlabel{fig: MCLNet BSAM}{{14}{19}{The architecture of the Bottleblock of Scale Aggregation module (BSAM) \relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces   The architecture of the Attentive Residual Multiscale Block (ARMB) \relax }}{20}{figure.caption.15}\protected@file@percent }
\newlabel{fig: MCLNet ARMB}{{15}{20}{The architecture of the Attentive Residual Multiscale Block (ARMB) \relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces   The architecture of the Context Encoding Module (CEM) \relax }}{20}{figure.caption.16}\protected@file@percent }
\newlabel{fig: MCLNet CEM}{{16}{20}{The architecture of the Context Encoding Module (CEM) \relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.8.4}Future}{21}{subsubsection.2.8.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.9}(2018.3)CBAM: Convolutional Block Attention Module}{21}{subsection.2.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{CBAM：卷积块注意模块}{21}{subsection.2.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(ECCV 2018)}{21}{subsection.2.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.9.1}Research Background}{21}{subsubsection.2.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.9.2}Contribution}{21}{subsubsection.2.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.9.3}Approach}{21}{subsubsection.2.9.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces   \textbf  {The overview of CBAM.} The module has two sequential sub-modules: channel and spatial. The intermediate feature map is adaptively refined through our module (CBAM) at every convolutional block of deep networks. \relax }}{22}{figure.caption.17}\protected@file@percent }
\newlabel{fig: CBAM-overview}{{17}{22}{\textbf {The overview of CBAM.} The module has two sequential sub-modules: channel and spatial. The intermediate feature map is adaptively refined through our module (CBAM) at every convolutional block of deep networks. \relax }{figure.caption.17}{}}
\newlabel{eq: CBAM}{{2}{22}{Approach}{equation.2.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Channel Attention Module}{22}{equation.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces   \textbf  {Diagram of each attention sub-module.} As illustrated, the channel sub-module utilizes both max-pooling outputs and average-pooling outputs with a shared network; the spatial sub-module utilizes similar two outputs that are pooled along the channel axis and forward them to a convolution layer. \relax }}{22}{figure.caption.18}\protected@file@percent }
\newlabel{fig: Attention Module}{{18}{22}{\textbf {Diagram of each attention sub-module.} As illustrated, the channel sub-module utilizes both max-pooling outputs and average-pooling outputs with a shared network; the spatial sub-module utilizes similar two outputs that are pooled along the channel axis and forward them to a convolution layer. \relax }{figure.caption.18}{}}
\newlabel{eq: Channel Attention Module}{{3}{23}{Channel Attention Module}{equation.2.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Spatial Attention Module}{23}{equation.2.3}\protected@file@percent }
\newlabel{eq: Spatial Attention Module}{{4}{23}{Spatial Attention Module}{equation.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.9.4}Future}{23}{subsubsection.2.9.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.10}(2023)SCConv: Spatial and Channel Reconstruction Convolution for Feature Redundancy}{23}{subsection.2.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{SCConv：特征冗余的空间和信道重建卷积 }{23}{subsection.2.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(CVPR 2023)}{23}{subsection.2.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.10.1}Research Background}{23}{subsubsection.2.10.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.10.2}Contribution}{23}{subsubsection.2.10.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.10.3}Approach}{24}{subsubsection.2.10.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces   The architecture of SCConv integrated with Spatial Reconstruction Unit (SRU) and Channel Reconstruction Unit (CRU). This figure shows the exact position of our SCConv module within a ResBlock. \relax }}{24}{figure.caption.19}\protected@file@percent }
\newlabel{fig: SCConv}{{19}{24}{The architecture of SCConv integrated with Spatial Reconstruction Unit (SRU) and Channel Reconstruction Unit (CRU). This figure shows the exact position of our SCConv module within a ResBlock. \relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {paragraph}{SRU}{24}{figure.caption.19}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces   The architecture of Spatial Reconstruction Unit. \relax }}{24}{figure.caption.20}\protected@file@percent }
\newlabel{fig: Spatial Reconstruction Unit}{{20}{24}{The architecture of Spatial Reconstruction Unit. \relax }{figure.caption.20}{}}
\newlabel{eq: SRU}{{5}{25}{SRU}{equation.2.5}{}}
\@writefile{toc}{\contentsline {paragraph}{CRU}{25}{equation.2.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces   The architecture of Chanel Reconstruction Unit. \relax }}{25}{figure.caption.21}\protected@file@percent }
\newlabel{fig: Chanel Reconstruction Unit}{{21}{25}{The architecture of Chanel Reconstruction Unit. \relax }{figure.caption.21}{}}
\bibstyle{unsrt}
\bibdata{reference}
\bibcite{zhu2020eemefn}{1}
\bibcite{peng2021conformer}{2}
\bibcite{woo2018cbam}{3}
\bibcite{ramachandran2019stand}{4}
\bibcite{li2023scconv}{5}
\bibcite{chen2018learning}{6}
\bibcite{zamir2021learning}{7}
\bibcite{meng2020gia}{8}
\bibcite{zhou2018unet++}{9}
\bibcite{zhou2019unet++}{10}
\bibcite{yang2021locally}{11}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.10.4}Future}{26}{subsubsection.2.10.4}\protected@file@percent }
\bibcite{zhang2020attention}{12}
\bibcite{li2018multi}{13}
\bibcite{zamir2020learning}{14}
\bibcite{li2020visual}{15}
\bibcite{xu2020learning}{16}
\newlabel{LastPage}{{}{27}{}{page.27}{}}
\xdef\lastpage@lastpage{27}
\xdef\lastpage@lastpageHy{27}
\gdef \@abspage@last{27}
