\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}文献阅读}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Pre-knowledge}{1}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}loss function for CV}{2}{subsubsection.1.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{$\mathcal  {L}_1$-loss}{2}{subsubsection.1.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{$\mathcal  {L}_2$-loss}{2}{equation.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Regularization of the $\mathcal  {L}_1$ and $\mathcal  {L}_2$ loss functions}{2}{equation.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Smooth $\mathcal  {L}_1$ loss function}{3}{lstnumber.-1.1}\protected@file@percent }
\citation{johnson2016perceptual}
\citation{johnson2016perceptual}
\@writefile{toc}{\contentsline {paragraph}{Huber loss function}{4}{equation.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{log-MSE}{4}{equation.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Perceptual loss function}{4}{equation.1.7}\protected@file@percent }
\newlabel{eq: perceptual loss}{{8}{5}{Perceptual loss function}{equation.1.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces   System overview. We train an image transformation network to transform input images into output images. We use a loss network pretrained for image classification to define perceptual loss functions that measure perceptual differences in content and style between images. The loss network remains fixed during the training process. \relax }}{5}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig: perceptual loss}{{1}{5}{System overview. We train an image transformation network to transform input images into output images. We use a loss network pretrained for image classification to define perceptual loss functions that measure perceptual differences in content and style between images. The loss network remains fixed during the training process. \relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {paragraph}{SSIM loss function}{6}{figure.caption.1}\protected@file@percent }
\newlabel{eq: SSIM}{{9}{6}{SSIM loss function}{equation.1.9}{}}
\newlabel{eq: SSIM loss}{{10}{6}{SSIM loss function}{equation.1.10}{}}
\newlabel{eq: revised_SSIM loss}{{11}{6}{SSIM loss function}{equation.1.11}{}}
\@writefile{toc}{\contentsline {paragraph}{MS-SSIM loss function}{6}{equation.1.11}\protected@file@percent }
\newlabel{eq: MS-SSIM}{{12}{6}{MS-SSIM loss function}{equation.1.12}{}}
\newlabel{eq: MS-SSIM loss}{{13}{6}{MS-SSIM loss function}{equation.1.13}{}}
\@writefile{toc}{\contentsline {paragraph}{Cross-entropy loss function}{7}{equation.1.13}\protected@file@percent }
\newlabel{eq: Cross-entropy loss}{{14}{7}{Cross-entropy loss function}{equation.1.14}{}}
\newlabel{eq: revised_Cross-entropy loss}{{15}{7}{Cross-entropy loss function}{equation.1.15}{}}
\@writefile{toc}{\contentsline {paragraph}{Adversarial loss function}{7}{equation.1.15}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces   Computational flow and structure of the GAN. \relax }}{7}{figure.caption.2}\protected@file@percent }
\newlabel{fig: GAN_architecture}{{2}{7}{Computational flow and structure of the GAN. \relax }{figure.caption.2}{}}
\newlabel{eq: Adversarial loss_another}{{16}{8}{Adversarial loss function}{equation.1.16}{}}
\newlabel{eq: Adversarial loss}{{17}{8}{Adversarial loss function}{equation.1.17}{}}
\newlabel{fig: D_optimization}{{3a}{8}{D的优化过程\relax }{figure.caption.3}{}}
\newlabel{sub@fig: D_optimization}{{a}{8}{D的优化过程\relax }{figure.caption.3}{}}
\newlabel{fig: G_optimization}{{3b}{8}{G的优化过程\relax }{figure.caption.3}{}}
\newlabel{sub@fig: G_optimization}{{b}{8}{G的优化过程\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces   The optimization of the GAN parameters \relax }}{8}{figure.caption.3}\protected@file@percent }
\newlabel{fig: Alternate Optimization}{{3}{8}{The optimization of the GAN parameters \relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Region loss function}{8}{figure.caption.3}\protected@file@percent }
\newlabel{eq: Region loss}{{18}{9}{Region loss function}{equation.1.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces   Data flow for training. The proposed loss function consists of three parts. \relax }}{9}{figure.caption.4}\protected@file@percent }
\newlabel{fig: proposed_loss_function}{{4}{9}{Data flow for training. The proposed loss function consists of three parts. \relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Reflectance loss function}{9}{figure.caption.4}\protected@file@percent }
\citation{lv2018mbllen}
\citation{10.1145/3343031.3350926}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}细节}{10}{subsubsection.1.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}个人工作进展}{10}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}梳理损失函数}{10}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}复现KinD代码}{10}{subsection.2.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces  Summary of essential characteristics of representative deep learning-based methods, including learning strategies, network structures, loss functions, training datasets, testing datasets, evaluation metrics, data formats of input, and whether the models are Retinex-based or not. "simulated" means the testing data are simulated by the same approach as the synthetic training data. "self-selected" stands for the real-world images selected by the authors. "\#P" represents the number of trainable parameters. "-" means this item is not available or not indicated in the paper.\relax }}{11}{table.caption.5}\protected@file@percent }
\newlabel{tab: Summary}{{1}{11}{Summary of essential characteristics of representative deep learning-based methods, including learning strategies, network structures, loss functions, training datasets, testing datasets, evaluation metrics, data formats of input, and whether the models are Retinex-based or not. "simulated" means the testing data are simulated by the same approach as the synthetic training data. "self-selected" stands for the real-world images selected by the authors. "\#P" represents the number of trainable parameters. "-" means this item is not available or not indicated in the paper.\relax }{table.caption.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Requirement}{12}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Train}{12}{subsubsection.2.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces   The architecture of our KinD network. Two branches correspond to the reflectance and illumination, respectively. From the perspective of functionality, it also can be divided into three modules, including layer decomposition, reflectance restoration, and illumination adjustment. \relax }}{12}{figure.caption.6}\protected@file@percent }
\newlabel{fig: network}{{5}{12}{The architecture of our KinD network. Two branches correspond to the reflectance and illumination, respectively. From the perspective of functionality, it also can be divided into three modules, including layer decomposition, reflectance restoration, and illumination adjustment. \relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Layer Decomposition Net}{12}{figure.caption.6}\protected@file@percent }
